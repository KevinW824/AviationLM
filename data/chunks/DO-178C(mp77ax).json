{
  "source": "DO-178C(mp77ax).md",
  "chunks": [
    "1150 18th Street, NW, Suite 910 \nWashington, DC 20036-3816 USA \n\n# Software Considerations In Airborne Systems And Equipment Certification\n\n \nPrepared by: SC-205 \nCopies of this document may be obtained from RTCA, Inc. \n\nTelephone: 202-833-9339 \nFacsimile: 202-833-9434 \nInternet: www.rtca.org Please visit the RTCA Online Store for document pricing and ordering information. \n\n \n\n## Foreword",
    "This report was prepared by RTCA Special Committee 205 (SC-205) and EUROCAE Working Group 71",
    "(WG-71) and approved by the RTCA Program Management Committee (PMC) on December 13, 2011. RTCA, Incorporated is a not-for-profit corporation formed to advance the art and science of aviation and aviation electronic systems for the benefit of the public. The organization functions as a Federal Advisory Committee and develops consensus-based recommendations on contemporary aviation issues. RTCA's objectives include but are not limited to:",
    "- \ncoalescing aviation system user and provider technical requirements in a manner that helps government and industry meet their mutual objectives and responsibilities; \n- \nanalyzing and recommending solutions to the system technical issues that aviation faces as it continues to pursue increased safety, system capacity, and efficiency; \n- \ndeveloping consensus on the application of pertinent technology to fulfill user and provider",
    "- \ndeveloping consensus on the application of pertinent technology to fulfill user and provider \nrequirements, including development of minimum operational performance standards for electronic systems and equipment that support aviation; and \n- \nassisting in developing the appropriate technical material upon which positions for the International Civil Aviation Organization and the International Telecommunication Union and other appropriate international organizations can be based.",
    "The organization's recommendations are often used as the basis for government and private sector decisions as well as the foundation for many Federal Aviation Administration Technical Standard Orders.",
    "Since the RTCA is not an official agency of the United States Government, its recommendations may not be regarded as statements of official government policy unless so enunciated by the U.S. government organization or agency having statutory jurisdiction over any matters to which the recommendations relate. \n\nIllustration provided by Pat Neilan, UK CAA \nCONSENSUS n. Collective opinion or concord; general agreement or accord. [Latin, from consentire, to agree] \n\n## List Of Figures",
    "Figure 1-1 Document Overview\n \n ................................................................................................................... 5\nFigure 2-1 Information Flow Between System and Software Life Cycle Processes\n \n .................................... 9\nFigure 2-2 Sequence of Events for Software Error Leading to a Failure Condition\n \n \n................................... 12\nFigure 3-1 Example of a Software Project Using Four Different Development Sequences",
    "Figure 3-1 Example of a Software Project Using Four Different Development Sequences\n \n \n....................... 22\nFigure 6-1 Software Testing Activities\n \n \n....................................................................................................... 45",
    "## List Of Tables",
    "Table 2-1 Failure Condition Category Descriptions ................................................................................... 13 \nTable 7-1 SCM Process Activities Associated with CC1 and CC2 Data.................................................... 58 \nTable 12-1 Tool Qualification Level Determination .................................................................................. 85 \nTable A-1 Software Planning Process",
    "Table A-1 Software Planning Process\n \n ........................................................................................................ 96\nTable A-2 Software Development Processes\n \n \n.............................................................................................. 97\nTable A-3 Verification of Outputs of Software Requirements Process\n \n ...................................................... 98\nTable A-4 Verification of Outputs of Software Design Process",
    "Table A-4 Verification of Outputs of Software Design Process\n \n \n................................................................. 99\nTable A-5 Verification of Outputs of Software Coding & Integration Processes\n \n .................................... 100\nTable A-6 Testing of Outputs of Integration Process\n \n ............................................................................... 101\nTable A-7 Verification of Verification Process Results",
    "Table A-7 Verification of Verification Process Results\n \n ........................................................................... 102\nTable A-8 Software Configuration Management Process\n \n ........................................................................ 103\nTable A-9 Software Quality Assurance Process\n \n ....................................................................................... 104\nTable A-10 Certification Liaison Process",
    "Table A-10 Certification Liaison Process\n \n \n................................................................................................. 105",
    "This Page Intentionally Left Blank \n\n## 1.0 Introduction\n\nThe rapid increase in the use of software in airborne systems and equipment used on aircraft and engines in the early 1980s resulted in a need for industry-accepted guidance for satisfying airworthiness requirements. DO-178, \"Software Considerations in Airborne Systems and Equipment Certification\", was written to satisfy this need.",
    "This document, now revised in the light of experience, provides the aviation community with guidance for determining, in a consistent manner and with an acceptable level of confidence, that the software aspects of airborne systems and equipment comply with airworthiness requirements. As software use increases, technology evolves, and experience is gained in the application of this document, this document will be reviewed and revised. Appendix A provides the background of this document.",
    "## 1.1 Purpose\n\nThe purpose of this document is to provide guidance for the production of software for airborne systems and equipment that performs its intended function with a level of confidence in safety that complies with airworthiness requirements. This guidance includes:",
    "- \nObjectives for software life cycle processes. \n- \nActivities that provide a means for satisfying those objectives. \n- \nDescriptions of the evidence in the form of software life cycle data that indicate that the objectives have been satisfied. \n- \nVariations in the objectives, independence, software life cycle data, and control categories by software level. \n- \nAdditional considerations (for example, previously developed software) that are \napplicable to certain applications. \n-",
    "applicable to certain applications. \n- \nDefinition of terms provided in the glossary. \nIn addition to guidance, supporting information is provided to assist the reader's understanding.",
    "## 1.2 Scope",
    "This document discusses those aspects of certification that pertain to the production of software for airborne systems and equipment used on aircraft, engines, propellers and, by region, auxiliary power units. In discussing those aspects, the system life cycle and its relationship with the software life cycle is described to aid in the understanding of the certification process. A complete description of the system life cycle processes, including the system safety assessment and validation",
    "of the system life cycle processes, including the system safety assessment and validation processes, or the certification process is not intended. The guidance contained in this document does not define or imply the level of involvement of a certification authority in a certification process. To understand certification authority involvement, the applicant should refer to applicable regulations and guidance material issued by the relevant certification authority.",
    "Since certification issues are discussed only in relation to the software life cycle, the operational aspects of the resulting software are not discussed. For example, the certification aspects of user-modifiable data are beyond the scope of this document.",
    "This document does not attempt to define firmware. Firmware should be classified as hardware or software and addressed by the applicable processes. This document assumes that during the system definition, functions have been allocated to either software or hardware. Other documents exist that provide guidance for development assurance for functions that are allocated to implementation in hardware. This document provides guidance for functions that are allocated to software.",
    "Note\n: This allows an efficient method of implementation and development assurance to \nbe determined at the time the system is specified and functions allocated. All parties should agree with this system decision at the time the allocation is made. \nMatters concerning the structure of the applicant's organization, the commercial relationships between the applicant and its suppliers, and personnel qualification criteria are beyond the scope of this document.",
    "## 1.3 Relationship To Other Documents\n\nIn addition to the airworthiness requirements, various national and international standards for software are available. In some communities, compliance with these standards may be required. However, it is outside the scope of this document to invoke specific national or international standards, or to propose a means by which these standards might be used as an alternative or in addition to this document.",
    "It is recognized that projects may be obliged, through contract or other means, to comply with additional standards as applied by, for example, the engine or aircraft manufacturer. Such standards may be derived from general standards produced or adopted by the manufacturer for its activities. Such standards should be considered by the planning process and considered, as appropriate, when applying supplier oversight. \n\n## 1.4 How To Use This Document",
    "## 1.4 How To Use This Document\n\nThe following points should be noted when using this document:",
    "a. This document is intended to be used by the international aviation community. To aid \nsuch use, references to specific national regulations and procedures are minimized. Instead, generic terms are used. For example, the term \"certification authority\" is used to denote the organization or person granting approval on behalf of the country \nresponsible for certification of the product (for example, an aircraft, engine). Where a",
    "responsible for certification of the product (for example, an aircraft, engine). Where a \nsecond country or group of countries validates or participates in this certification, this document may be used with due recognition given to bilateral agreements or memoranda of understanding between the countries involved. \nb. This document recognizes that the guidance herein is not mandated by law, but \nrepresents a consensus of the aviation community. It also recognizes that alternative",
    "represents a consensus of the aviation community. It also recognizes that alternative \nmethods to the methods described herein may be available to the applicant. For these reasons, the use of words such as \"shall\" and \"must\" is avoided. \nc. If an applicant adopts this document as a means of compliance, the applicant should \nsatisfy all applicable objectives. This document should apply to the applicant and any",
    "satisfy all applicable objectives. This document should apply to the applicant and any \nof its suppliers, who are involved with any of the software life cycle processes or the \noutputs of those processes described herein. The applicant is responsible for oversight of all of its suppliers.",
    "d. The applicant should plan a set of activities that satisfy the objectives. This document \ndescribes activities for achieving those objectives. The applicant may plan and, \nsubject to the approval of the certification authority, adopt alternative activities to \nthose described in this document. The applicant may also plan and conduct additional activities that are determined to be necessary. \ne. The applicant should address any additional considerations in its software plans and \nstandards.",
    "standards. \nf. The applicant should perform the planned activities and provide evidence as \nindicated in section 11 to substantiate that the objectives have been satisfied. \ng. Explanatory text is included to aid the reader in understanding the topic under",
    "g. Explanatory text is included to aid the reader in understanding the topic under \ndiscussion. For example, section 2 provides information necessary to understand the interaction between the system life cycle and software life cycle. Similarly, section 3 provides a description of the software life cycle and section 10 an overview of the \ncertification process. \nh. Section 11 contains the data generally produced to aid the software aspects of the",
    "h. Section 11 contains the data generally produced to aid the software aspects of the \ncertification process. The names of the data are denoted in the text by capitalization of the first letter of each word in the name (for example, Source Code). \ni. \nSection 12 discusses additional considerations including guidance for the use of previously developed software, tool qualification, and the use of alternative methods to those described in sections 2 through 11. Section 12 may not apply to every",
    "project. \nj. \nAnnex A specifies the applicability of the objectives, activities, and software life cycle data for each software level as well as the variation in the independence and control categories for each software level. In order to fully understand the guidance, \nthe full body of this document should be considered. \nk. In cases where examples are used to indicate how the guidance might be applied, \neither graphically or through narrative, the examples are not to be interpreted as the",
    "either graphically or through narrative, the examples are not to be interpreted as the \npreferred method. In these cases, the examples are considered supporting \ninformation. \nl. \nA list of items does not imply the list is all-inclusive. \nm. Notes in this document are supporting information used to provide explanatory \nmaterial, emphasize a point, or draw attention to related items which are not entirely within context.",
    "n. Major sections are numbered as X.0 throughout this document. It should be noted \nthat references to an entire section are identified as \"section X\"; whereas, references to the content between section headers X.0 and X.1 are referenced as \"section X.0\". \no. One or more supplements to this document exist and extend the guidance in this",
    "o. One or more supplements to this document exist and extend the guidance in this \ndocument to a specific technique. Supplements are used in conjunction with this document and may be used in conjunction with one another. Unless alternatives are used (see 1.4.i), if a supplement exists for a specific technique, the supplement should",
    "be used to add, delete, or otherwise modify objectives, activities, explanatory text, and software life cycle data in this document to address that technique, as defined appropriately in each supplement. It is the responsibility of the applicant to ensure that the supplement's use is acceptable to the appropriate certification authority. As part of the software planning process, the applicant should review all potentially relevant supplements and identify those that will be used. The",
    "should review all potentially relevant supplements and identify those that will be used. The information in supplements should be used with and in the same way as this document. Annex A of each supplement identifies how the objectives of this document are revised relative to the specific technique addressed by the supplement.",
    "p. Compliance is achieved when all applicable objectives have been satisfied by \nperforming all planned activities and capturing the related evidence. \n\n## 1.5 Document Overview\n\nFigure 1-1\n is a pictorial overview of this document's sections and their relationship to each other. \n\n## System Aspects Relating To Software Development - Section 2",
    "SOFTWARE LIFE CYCLE - SECTION 3\nSOFTWARE LIFE CYCLE PROCESSES\nSOFTWARE PLANNING PROCESS - SECTION 4\nSOFTWARE DEVELOPMENT PROCESSES - SECTION 5\nSOFTWARE REQUIREMENTS PROCESS\nSOFTWARE DESIGN PROCESS\nSOFTWARE CODING PROCESS\nINTEGRATION PROCESS\nINTEGRAL PROCESSES\nSOFTWARE VERIFICATION PROCESS - SECTION 6\nSOFTWARE CONFIGURATION MANAGEMENT PROCESS - SECTION 7\nSOFTWARE QUALITY ASSURANCE PROCESS - SECTION 8\nCERTIFICATION LIAISON PROCESS - SECTION 9\nOVERVIEW OF CERTIFICATION PROCESS - SECTION 10",
    "CERTIFICATION LIAISON PROCESS - SECTION 9\nOVERVIEW OF CERTIFICATION PROCESS - SECTION 10\nSOFTWARE LIFE CYCLE DATA - SECTION 11\nADDITIONAL CONSIDERATIONS - SECTION 12 \nThis Page Intentionally Left Blank",
    "## 2.0 System Aspects Relating To Software Development\n\nThis section discusses those aspects of the system life cycle processes necessary to understand the software life cycle processes. System life cycle processes can be found in other industry documents (for example, SAE ARP4754A). \n\nDiscussed in this section are:",
    "- \nSystem requirements allocation to software (see 2.1). \n- \nThe information flow between the system and software life cycle processes and between the software and hardware life cycle processes (see 2.2). \n- \nThe system safety assessment process, failure conditions, software level definitions, \nand software level determination (see 2.3). \n- \nArchitectural considerations (see 2.4). \n- \nSoftware considerations in system life cycle processes (see 2.5). \n-",
    "- \nSoftware considerations in system life cycle processes (see 2.5). \n- \nSystem considerations in software life cycle processes (see 2.6). \nThe term \"system\" in the context of this document refers to the airborne system and equipment only, not to the wider definition of a system that might include operators, operational procedures, etc.",
    "## 2.1 System Requirements Allocation To Software\n\nAs part of the system life cycle processes, system requirements are developed from the system operational requirements and other considerations such as safety-related, security, and performance requirements. The safety-related requirements result from the system safety assessment process, and may include functional, integrity, and reliability requirements, as well as design constraints.",
    "The system safety assessment process determines and categorizes the failure conditions of the system. Within the safety assessment process, safety-related requirements are defined to ensure the integrity of the system by specifying the desired immunity from, and system responses to, these failure conditions. These requirements are identified for hardware and software to preclude or limit the effects of faults, and may provide fault detection, fault tolerance, fault removal, and fault avoidance.",
    "of faults, and may provide fault detection, fault tolerance, fault removal, and fault avoidance. The system processes are responsible for the refinement and allocation of system requirements to hardware and/or software as determined by the system architecture.",
    "System requirements allocated to software, including safety-related requirements, are developed and refined into software requirements that are verified by the software verification process activities. These requirements and the associated verification should establish that the software performs its intended functions under any foreseeable operating condition. System requirements allocated to software may include:",
    "a. Functional and operational requirements. \nb. Interface requirements. \nc. Performance requirements. \nd. Safety-related requirements, including safety strategies, design constraints and design \nmethods, such as, partitioning, dissimilarity, redundancy, or safety monitoring. In cases where the system is a component of another system, the requirements and \nfailure conditions for that other system may also form part of the system requirements allocated to software. \ne. Security requirements.",
    "e. Security requirements. \nf. Maintenance requirements. g. Certification requirements, including any applicable certification authority \nregulations, issue papers, etc. \nh. Additional requirements needed to aid the system life cycle processes.",
    "## 2.2 Information Flow Between System And Software Life Cycle Processes\n\nFigure 2-1\n is an overview of the information flow between system life cycle processes and the software life cycle processes. This information flow includes the system safety aspects. Due to interdependence of the system safety assessment process and the system design process, the flow of information described in these sections is iterative. \n\n## 2.2.1 Information Flow From System Processes To Software Processes",
    "## 2.2.1 Information Flow From System Processes To Software Processes\n\nThe following data is passed to the software life cycle processes by the system processes either as part of the requirements allocation or during the development life cycle:",
    "a. System requirements allocated to software. \nb. System safety objectives. \nc. Software level for software components and a description of associated failure \ncondition(s), if applicable. \nd. System description and hardware definition. \ne. Design constraints, including external interfaces, partitioning requirements, etc. \nf. Details of any system activities proposed to be performed as part of the software life",
    "f. Details of any system activities proposed to be performed as part of the software life \ncycle. Note that system requirement validation is not usually part of the software life \ncycle processes. The system life cycle processes are responsible for assuring any \nsystem activities proposed to be performed as part of the software life cycle. \ng. Evidence of the acceptability, or otherwise, of any data provided by the software",
    "g. Evidence of the acceptability, or otherwise, of any data provided by the software \nprocesses to the system processes on which any activity has been conducted by the system processes. Examples of such activity are the system processes' evaluations of: \n1. Derived requirements provided by the software processes to determine if there is \nany impact on the system safety assessment and system requirements. \n2. Issues raised by the software processes with respect to the clarification or",
    "2. Issues raised by the software processes with respect to the clarification or \ncorrection of system requirements allocated to software. \nh. Evidence of software verification activities performed by the system life cycle \nprocesses, if any. \nAny evidence provided by the system processes (see 2.2.1.f and 2.2.1.g) should be considered by the software processes to be Software Verification Results (see 11.14).",
    "## 2.2.2 Information Flow From Software Processes To System Processes",
    "The software life cycle processes analyze the system requirements allocated to software as part of the software requirements process. If such an analysis identifies any system requirements as inadequate or incorrect, the software life cycle processes should capture the issues and refer them to the system processes for resolution. Furthermore, as the software design and implementation evolves, details are added and modifications made that may affect system safety assessment and system",
    "details are added and modifications made that may affect system safety assessment and system requirements.",
    "To aid the evaluation of the evolving design and changes to the design, the software life cycle processes should make data available to the system processes including the system safety assessment process. This data will facilitate analyses and evaluations to establish the effect on the system safety assessment and system requirements. It may be advantageous for such analyses and evaluations to be performed jointly by the systems and software processes. Such data includes:",
    "a. Details of derived requirements created during the software life cycle processes. \nb. A description of the software architecture, including software partitioning. c. Evidence of system activities performed by the software life cycle processes, if any. d. Problem or change documentation, including problems identified in the system \nrequirements allocated to software and identified incompatibilities between the \nhardware and the software.",
    "hardware and the software. \ne. Any limitations of use. f. Configuration identification and any configuration status constraints. \ng. Performance, timing, and accuracy characteristics. \nh. Data to facilitate integration of the software into the system. \ni. \nDetails of software verification activities proposed to be performed during system verification, if any.",
    "## 2.2.3 Information Flow Between Software Processes And Hardware Processes\n\nData is passed between the software life cycle process and the hardware life cycle process either as part of the system requirements allocation or during the development life cycles. Such data includes:",
    "a. All requirements, including derived requirements, needed for hardware/software \nintegration, such as definition of protocols, timing constraints, and addressing \nschemes for the interface between hardware and software. \nb. Instances where hardware and software verification activities require coordination. \nc. Identified incompatibilities between the hardware and the software. \n\n## 2.3 System Safety Assessment Process And Software Level",
    "This section provides a brief introduction to how the software level for software components is determined and how architectural considerations may influence the allocation of a software level. It is not the intent of this document to prescribe how these activities are performed; this has to be established and performed as part of the system life cycle processes. The software level of a software component is based upon the contribution of software to potential failure conditions as determined",
    "component is based upon the contribution of software to potential failure conditions as determined by the system safety assessment process by establishing how an error in a software component relates to the system failure condition(s) and the severity of that failure condition(s). The software level establishes the rigor necessary to demonstrate compliance with this document. Development of software to a software level does not imply the assignment of a failure rate for that software. Thus,",
    "to a software level does not imply the assignment of a failure rate for that software. Thus, software reliability rates based on software levels cannot be used by the system safety assessment process in the same way as hardware failure rates.",
    "Only partitioned software components (see 2.4.1) can be assigned individual software levels by the system safety assessment process. If partitioning between software components cannot be demonstrated, the software components should be viewed as a single component when assigning software levels (that is, all components are assigned the software level associated with the most severe failure condition to which the software can contribute). The applicant should establish the system safety",
    "condition to which the software can contribute). The applicant should establish the system safety assessment process to be used based on certification authority guidance. The software level for each software component of the system should then be assigned based on this process and agreed with the certification authorities.",
    "## 2.3.1 Relationship Between Software Errors And Failure Conditions\n\n.",
    "Figure 2-2 shows a sequence of events in which a software error leads to the failure condition at aircraft level. A software error may be latent and, thus, not immediately produce a failure. This model is intentionally a simple, linear representation. In actual operation, the sequence of events that leads from a software error to a failure condition may be complex and not easily represented by a sequence of events, as shown in Figure",
    "2-2 It is important to realize that the likelihood that the software contains an error cannot be quantified in the same way as for random hardware failures.",
    "Architectural considerations (see 2.4) and/or system external factors may also be considered as part of the system safety assessment process when identifying the failure condition categories and assigning the software level to each software component. \n\n## 2.3.2 Failure Condition Categorization",
    "## 2.3.2 Failure Condition Categorization\n\nFor a complete definition of failure condition categories, the applicant should refer to applicable regulations and guidance material issued by the relevant certification authority. The failure condition categories listed in Table 2-1 are valid for large transport aircraft based on established advisory material for the system safety assessment process, and are included to assist in the use of this document. \n\n## Failure Condition Category Descriptions",
    "| Category                                                            | Description                                                    |\n|---------------------------------------------------------------------|----------------------------------------------------------------|\n| Catastrophic                                                        | Failure Conditions, which would result in multiple fatalities, |",
    "| usually with the loss of the airplane.                              |                                                                |\n|                                                                     |                                                                |\n| Hazardous                                                           |                                                                |",
    "|                                                                     |                                                                |\n| Failure Conditions, which would reduce the capability of the        |                                                                |\n| airplane or the ability of the flight crew to cope with adverse     |                                                                |",
    "| operating conditions to the extent that there would be:             |                                                                |\n| -                                                                   |                                                                |\n|                                                                     |                                                                |",
    "| A large reduction in safety margins or functional capabilities;     |                                                                |\n| -                                                                   |                                                                |\n|                                                                     |                                                                |",
    "| Physical distress or excessive workload such that the flight        |                                                                |\n| crew cannot be relied upon to perform their tasks accurately        |                                                                |\n| or completely, or                                                   |                                                                |",
    "| -                                                                   |                                                                |\n|                                                                     |                                                                |\n| Serious or fatal injury to a relatively small number of the         |                                                                |",
    "| occupants other than the flight crew.                               |                                                                |\n| Major                                                               |                                                                |\n|                                                                     |                                                                |",
    "| Failure Conditions which would reduce the capability of the         |                                                                |\n| airplane or the ability of the crew to cope with adverse operating  |                                                                |\n| conditions to the extent that there would be, for example, a        |                                                                |",
    "| significant reduction in safety margins or functional capabilities, |                                                                |\n| a significant increase in crew workload or in conditions            |                                                                |\n| impairing crew efficiency, or discomfort to the flight crew, or     |                                                                |",
    "| physical distress to passengers or cabin crew, possibly including   |                                                                |\n| injuries.                                                           |                                                                |\n| Minor                                                               |                                                                |",
    "|                                                                     |                                                                |\n| Failure Conditions which would not significantly reduce             |                                                                |\n| airplane safety, and which involve crew actions that are well       |                                                                |",
    "| within their capabilities. Minor Failure Conditions may include,    |                                                                |\n| for example, a slight reduction in safety margins or functional     |                                                                |\n| capabilities, a slight increase in crew workload, such as routine   |                                                                |",
    "| flight plan changes, or some physical discomfort to passengers      |                                                                |\n| or cabin crew.                                                      |                                                                |\n| No Safety Effect                                                    |                                                                |",
    "|                                                                     |                                                                |\n| Failure Conditions that would have no effect on safety; for         |                                                                |\n| example, Failure Conditions that would not affect the               |                                                                |",
    "| operational capability of the airplane or increase crew workload.   |                                                                |",
    "## 2.3.3 Software Level Definition\n\nThis document recognizes five software levels, Level A to Level E. For the example failure condition categories listed in section 2.3.2, the relationships between these software levels and failure conditions are:",
    "a. Level A:  Software whose anomalous behavior, as shown by the system safety \nassessment process, would cause or contribute to a failure of system function resulting in a catastrophic failure condition for the aircraft. \nb. Level B:  Software whose anomalous behavior, as shown by the system safety \nassessment process, would cause or contribute to a failure of system function \nresulting in a hazardous failure condition for the aircraft.",
    "resulting in a hazardous failure condition for the aircraft. \nc. Level C:  Software whose anomalous behavior, as shown by the system safety \nassessment process, would cause or contribute to a failure of system function resulting in a major failure condition for the aircraft. \nd. Level D:  Software whose anomalous behavior, as shown by the system safety \nassessment process, would cause or contribute to a failure of system function resulting in a minor failure condition for the aircraft.",
    "e. Level E:  Software whose anomalous behavior, as shown by the system safety \nassessment process, would cause or contribute to a failure of system function with no effect on aircraft operational capability or pilot workload. If a software component is \ndetermined to be Level E and this is confirmed by the certification authority, no further guidance contained in this document applies.",
    "The applicant should always consider the appropriate cert ification guidance and system development considerations for categorizing the failure condition severity and the software level.",
    "## 2.3.4 Software Level Determination",
    "The system safety assessment process determines the software level(s) appropriate to the software components of a particular system based upon the failure condition which may result from anomalous behavior of the software. The impact of, both loss of function and malfunction should be analyzed. External factors such as adverse environmental conditions as well as architectural strategies (as described in section 2.4) may be considered when identifying the failure condition categories and",
    "described in section 2.4) may be considered when identifying the failure condition categories and determining the software level.",
    "Note 1: The applicant may want to consider planned functionality to be added during",
    "future developments, as well as potential changes in system requirements allocated to software that may result in a more severe failure condition category and higher software level. It may be desirable to develop the software to a level higher than that determined by the system safety assessment process of the original application, since later development of software life cycle data for substantiating a higher software level application may be difficult.",
    "Note 2: For airborne systems and equipment mandated by operating regulations, but which do not affect the airworthiness of the aircraft, for example, a flight data recorder, the software level needs to be commensurate with the intended function. In some cases, the software level may be specified in equipment minimum performance standards.",
    "If the anomalous behavior of a software component contributes to more than one failure condition, then the software component should be assigned the software level associated with the most severe failure condition to which the software can contribute, including combined failure conditions. \n\n## 2.4 Architectural Considerations",
    "This section provides information on several architectural strategies that may limit the impact of failures, or detect failures and provide acceptable system responses to contain them. These architectural techniques are typically identified during system design and should not be interpreted as the preferred or required solutions. A serial implementation is one in which multiple software components are used for a system function such that anomalous behavior of any of the components could produce",
    "are used for a system function such that anomalous behavior of any of the components could produce the failure condition. In this implementation, the software components will have the software level associated with the most severe failure condition category of the system function. In implementations where anomalous behavior of two or more partitioned software components is needed in order to cause the failure condition, this may be taken into consideration by the system safety assessment",
    "cause the failure condition, this may be taken into consideration by the system safety assessment process when assigning the software level for these software components.",
    "The system safety assessment process needs to establish that sufficient independence exists between software components with respect to both function (that is, high-level requirements) and design (for example, common design elements, languages, and tools).",
    "If partitioning and independence between software components cannot be demonstrated, the software components should be viewed as a single software component when assigning software levels (that is, all components are assigned the software level associated with the most severe failure condition to which the software can contribute). \n\n## 2.4.1 Partitioning",
    "Partitioning is a technique for providing isolation between software components to contain and/or isolate faults and potentially reduce the effort of the software verification process. Partitioning between software components may be achieved by allocating unique hardware resources to each component (that is, only one software component is executed on each hardware platform in a system). Alternatively, partitioning provisions may be made to allow multiple software components to run on the same",
    "partitioning provisions may be made to allow multiple software components to run on the same hardware platform. Regardless of the method, the following should be ensured for partitioned software components:",
    "a. A partitioned software component should not be allowed to contaminate another \npartitioned software component's code, input/output (I/O), or data storage areas. \nb. A partitioned software component should be allowed to consume shared processor \nresources only during its scheduled period of execution. \nc. Failures of hardware unique to a partitioned software component should not cause \nadverse effects on other partitioned software components.",
    "adverse effects on other partitioned software components. \nd. Any software providing partitioning should have the same or higher software level as \nthe highest level assigned to any of the partitioned software components. \ne. Any hardware providing partitioning should be assessed by the system safety \nassessment process to ensure that it does not adversely affect safety.",
    "assessment process to ensure that it does not adversely affect safety. \nThe software life cycle processes should address the partitioning design considerations. These include the extent and scope of interactions permitted between the partitioned components and whether the protection is implemented by hardware or by a combination of hardware and software.",
    "## 2.4.2 Multiple-Version Dissimilar Software\n\nMultiple-version dissimilar software is a system design technique that involves producing two or more components of software that provide the same function in a way that may avoid some sources of common errors between the components. Multiple-version dissimilar software is also referred to as multi-version software, multi-version independent software, dissimilar software, N-version programming, or software diversity.",
    "Software life cycle processes completed or activated before dissimilarity is introduced into a development, remain potential error sources. System requirements may specify a hardware configuration that provides for the execution of multiple-version dissimilar software. The degree of dissimilarity, and hence the degree of protection, is not usually measurable. Probability of loss of system function will increase to the extent that the safety monitoring associated with dissimilar software",
    "function will increase to the extent that the safety monitoring associated with dissimilar software versions detects actual errors using comparator differences greater than threshold limits. Dissimilar software versions are usually used, therefore, as a means of providing additional protection after the software verification process objectives for the software level, as described in section 6, have been satisfied. Dissimilar software verification methods may be reduced from those used to verify",
    "been satisfied. Dissimilar software verification methods may be reduced from those used to verify single version software if it can be shown that the resulting potential loss of system function is acceptable as determined by the system safety assessment process.",
    "Verification of multiple-version dissimilar software is discussed in section 12.3.2. \n\n## 2.4.3 Safety Monitoring",
    "Safety monitoring is a means of protecting against specific failure conditions by directly monitoring a function for failures that would result in a failure condition. Monitoring functions may be implemented in hardware, software, or a combination of hardware and software. Through the use of monitoring techniques, the software level of the monitored software may be assigned a software level associated with the loss of its related system function. To allow this assignment, there are three",
    "associated with the loss of its related system function. To allow this assignment, there are three important attributes of the monitor that should be determined:",
    "a. Software level:  Safety monitoring software is assigned the software level associated \nwith the most severe failure condition category for the monitored function. \nb. System fault coverage\n:  Assessment of the system fault coverage of a monitor ensures \nthat the monitor's design and implementation are such that the faults which it is intended to detect will be detected under all necessary conditions. \nIndependence of function and monitor\nc. \n:  The monitor and protective mechanism are",
    "Independence of function and monitor\nc. \n:  The monitor and protective mechanism are \nnot rendered inoperative by the same failure that causes the failure condition.",
    "## 2.5 Software Considerations In System Life Cycle Processes\n\nThis section provides an overview of those software-related issues (not necessarily mutually exclusive) that should be considered, as appropriate, by the system life cycle processes: \n\na. Parameter data items. \nb. User-modifiable software. \nc. Commercial-Off-The-Shelf (COTS) software. \nd. Option-selectable software. \ne. Field-loadable software. \nf. Software considerations in system verification. \n\n## 2.5.1 Parameter Data Items",
    "Software consists of Executable Object Code and/or data, and can comprise one or more configuration items. A data set that influences the behavior of the software without modifying the Executable Object Code and is managed as a separate configuration item is called a parameter data item. That is, when discussing parameter data items and Executable Object Code, it is implied that the parameter data items are not part of the Executable Object Code. A parameter data item comprises a structure of",
    "items are not part of the Executable Object Code. A parameter data item comprises a structure of individual elements where each element can be assigned a single value. Each element has attributes such as type, range, or set of allowed values. Examples of parameter data items include configuration tables and databases but not aeronautical data as they are beyond the scope of this document. Parameter data items may contain data that can:",
    "a. Influence paths executed through the Executable Object Code. b. Activate or deactivate software components and functions. c. Adapt the software computations to the system configuration. d. Be used as computational data. e. Establish time and memory partitioning allotments. f. Provide initial values to the software component. \nDepending on how the parameter data item is to be used in the airborne system, the following should be addressed:",
    "- \nUser-modifiable software guidance. \n- \nOption-selectable software guidance. In cases where the parameter data item activates or deactivates functions, the guidance for deactivated code should be addressed as well. \n- \nField-loadable software guidance. Of particular concern is detection of corrupted \nparameter data items, as well as incompatibility between the Executable Object Code \nand parameter data items.",
    "and parameter data items. \nThe parameter data item should be assigned the same software level as the software component using it.",
    "For more information regarding verification of parameter data items, see 6.6. \n\n## 2.5.2 User-Modifiable Software",
    "A user-modifiable component is that part of the software that may be changed by the user within the modification constraints without certification authority review, if the system requirements provide for user modification. A non-modifiable component is that which is not intended to be changed by the user. The potential effects of user modification are determined by the system safety assessment process and used to develop the software requirements, and then, the software verification process",
    "process and used to develop the software requirements, and then, the software verification process activities. Designing for usermodifiable software is discussed further in section 5.2.3. A change that affects the nonmodifiable software, its protection, or the modifiable software boundaries is a software modification and is discussed in section 12.1.1.",
    "Guidance for user-modifiable software includes:",
    "a. The user-modifiable software should not adversely affect safety, operational \ncapabilities, flight crew workload, any non-modifiable software components, or any \nsoftware protection mechanism used. Unless this can be established, the software may not be classified as user-modifiable. The safety impact of displaying information based on user-modifiable software should also be considered. \nb. When the system requirements provide for user modification, then users may modify",
    "b. When the system requirements provide for user modification, then users may modify \nsoftware within the modification constraints without certification authority review. \nc. The system requirements should specify the mechanisms that prevent the user \nmodification from affecting system safety whether or not they are correctly implemented. The software that provides the protection for user modification should",
    "be at the same software level as the function it is protecting from errors in the modifiable component. \nd. If the system requirements do not include provision for user modification, the \nsoftware should not be modified by the user unless compliance with this document is demonstrated for the modification. \ne. At the time of the user modification, the user should take responsibility for all aspects \nof the user-modifiable software, for example, software configuration management,",
    "of the user-modifiable software, for example, software configuration management, \nsoftware quality assurance, and software verification. \nf. The applicant should provide the necessary information to enable the user to manage \nthe software in such a way that the safety of the aircraft is not compromised.",
    "## 2.5.3 Commercial-Off-The-Shelf Software\n\nCOTS software included in airborne systems or equipment should satisfy the objectives of this document. If deficiencies exist in the software life cycle data of COTS software, the data should be augmented to satisfy the objectives of this document. The guidance in section 12.1.4, Upgrading a Development Baseline, and section 12.3.4, Product Service History, may be relevant in this instance. \n\n## 2.5.4 Option-Selectable Software",
    "## 2.5.4 Option-Selectable Software\n\nSome airborne systems and equipment may include optional functions that may be selected by software-programmed options rather than by hardware connector pins. The option-selectable software functions are used to select a particular configuration within the target computer. See 4.2.h, 5.2.4, and 6.4.4.3.d.2 for guidance on deactivated code.",
    "When software programmed options are included, a means should be provided to ensure that inadvertent selections involving non-approved configurations for the target computer within the installation environment cannot be made. \n\n## 2.5.5 Field-Loadable Software",
    "Field-loadable airborne software refers to software that can be loaded without removing the system or equipment from its installation. The safety-related requirements associated with the software loading function are part of the system requirements. If the inadvertent enabling of the software loading function could induce a system failure condition, then a safety-related requirement for the software loading function is specified in the system requirements. System safety considerations relating",
    "loading function is specified in the system requirements. System safety considerations relating to field-loadable software include:",
    "- \nDetection of corrupted or partially loaded software. \n- \nDetermination of the effects of loading the inappropriate software. \n- \nHardware/software compatibility. \n- \nSoftware/software compatibility. \n- \nAircraft/software compatibility. \n- \nInadvertent enabling of the field-loading function. \n- \nLoss or corruption of the software configuration identification display. \nGuidance for field-loadable software includes:",
    "a. Unless otherwise justified by the system safety assessment process, the detection \nmechanism for partial or corrupted software loads should be assigned the same \nfailure condition or software level as the most severe failure condition or software level associated with the function that uses the software load.",
    "b. If a system recovers to a default mode or safe state upon detection of a corrupted or \ninappropriate software load, then each partitioned component of the system should \nhave safety-related requirements specified for recovery to and operation in this mode. \nInterfacing systems may also need to be reviewed for proper operation with the default mode. \nc. The software loading function, including support systems and procedures, should",
    "c. The software loading function, including support systems and procedures, should \ninclude a means to detect incorrect software and/or hardware and/or aircraft \ncombinations and should provide protection appropriate to the failure condition of the function. If the software consists of multiple configuration items, their compatibility should be ensured. \nd. If software is part of an airborne display mechanism that is the means for ensuring",
    "d. If software is part of an airborne display mechanism that is the means for ensuring \nthat the aircraft conforms to a certified configuration, then that software should either be developed to the highest software level of the software to be loaded, or the system safety assessment process should justify the integrity of an end-to-end check of the \nsoftware configuration identification.",
    "## 2.5.6 Software Considerations In System Verification\n\nGuidance for system verification is beyond the scope of this document. However, the software life cycle processes aid and interact with the system verification process and may be able to satisfy some system verification process objectives. Software design details that relate to the system functionality need to be made available to aid system verification. \n\n## 2.6 System Considerations In Software Life Cycle Processes",
    "## 2.6 System Considerations In Software Life Cycle Processes\n\nCredit may be taken from system life cycle processes for the satisfaction, or partial satisfaction, of the software objectives as defined in this document. In such cases, the system activities for which credit is being sought should be shown to meet the applicable objectives of this document with evidence of the completion of planned activities and their outputs identified as part of the software life cycle data.",
    "## 3.0 Software Life Cycle\n\nThis section discusses the software life cycle processes, software life cycle definition, and transition criteria between software life cycle processes. This document does not prescribe preferred software life cycles and interactions between them. The separation of the processes is not intended to imply a structure for the organization(s) that perform them. For each software product, the software life cycle(s) is constructed that includes these processes.",
    "## 3.1 Software Life Cycle Processes\n\nThe software life cycle processes are:",
    "a. The software planning process that defines and coordinates the activities of the \nsoftware development and integral processes for a project. Section 4 describes the software planning process. \nb. The software development processes that produce the software product. These \nprocesses are the software requirements process, the software design process, the software coding process, and the integration process. Section 5 describes the software development processes.",
    "c. The integral processes that ensure the correctness and control of, and confidence in",
    "the software life cycle processes and their outputs. The integral processes are the software verification process, the software configuration management process, the software quality assurance process, and the certification liaison process. It is important to understand that the integral processes are performed concurrently with the software planning and development processes throughout the software life cycle. \nSections 6 through 9 describe the integral processes.",
    "## 3.2 Software Life Cycle Definition",
    "A project defines one or more software life cycle(s) by choosing the activities for each process, specifying a sequence for the activities, and assigning responsibilities for the activities. For a specific project, the sequencing of these processes is determined by attributes of the project, such as system functionality and complexity, software size and complexity, requirements stability, use of previously developed software, development strategies, and hardware availability. The usual sequence",
    "developed software, development strategies, and hardware availability. The usual sequence through the software development processes is requirements, design, coding, and integration.",
    "Figure 3-1 illustrates the sequence of software development processes for several components of a single software product with different software life cycles. Component W implements a set of system requirements by developing the software requirements, using those requirements to define a software design, implementing that design into Source Code, and then integrating the component into the hardware. Component X illustrates the use of previously developed software used in a certified product.",
    "Component X illustrates the use of previously developed software used in a certified product. Component Y illustrates the use of a simple, partitioned function that can be coded directly from the software requirements. Component Z illustrates the use of a prototyping strategy. Usually, the goals of prototyping are to better understand the software requirements and to mitigate development and technical risks. The initial requirements are used as the basis to implement a prototype. This prototype",
    "risks. The initial requirements are used as the basis to implement a prototype. This prototype is evaluated in an environment representative of the intended use of the system under development. Results of the evaluation are used to refine the requirements. The processes of a software life cycle may be iterative, that is, entered and re-entered. The timing and degree of iteration varies due to the incremental development of system functions, complexity, requirements development, hardware",
    "to the incremental development of system functions, complexity, requirements development, hardware availability, feedback to previous processes, and other attributes of the project.",
    "The various parts of the selected software life cycle are tied together with a combination of incremental integration process and software verification process activities. \n\nNote: For simplicity, the software planning and integral processes are not shown.\n\n## 3.3 Transition Criteria Between Processes\n\nTransition criteria are used to determine whether a process may be entered or re-entered.",
    "Each software life cycle process performs activities on inputs to produce outputs. A process may produce feedback to other processes and receive feedback from others. The definition of feedback includes how information is recognized, controlled, and resolved by the receiving process. An example of a feedback is problem reporting. The transition criteria will depend on the planned sequence of software development processes and integral processes, and may be affected by the software level.",
    "software development processes and integral processes, and may be affected by the software level. Examples of transition criteria that may be chosen are: the software verification process reviews have been performed, the input is an identified configuration item, and a traceability analysis has been completed for the input.",
    "Not every input to a process need be complete before that process can be initiated, if the transition criteria established for the process are satisfied. \n\nIf a process acts on partial inputs, the inputs to the process should be examined to ensure that they meet transition criteria. Also, subsequent inputs to the process should be examined to determine that the previous outputs of the software development and software verification processes are still valid.",
    "This Page Intentionally Left Blank \n\n## 4.0 Software Planning Process\n\nThis section discusses the objectives and activities of the software planning process. This process produces the software plans and standards that direct the software development processes and the integral processes. Table A-1\n of Annex A is a summary of the objectives and outputs of the software planning process by software level. \n\n## 4.1 Software Planning Process Objectives",
    "## 4.1 Software Planning Process Objectives\n\nThe purpose of the software planning process is to define the means of producing software that will satisfy its requirements and provide the level of confidence that is consistent with the software level. The objectives of the software planning process are:",
    "a. The activities of the software development processes and integral processes of the \nsoftware life cycle that will address the system requirements and software level(s) are \ndefined (see 4.2). \nb. The software life cycle(s), including the inter-relationships between the processes, \ntheir sequencing, feedback mechanisms, and transition criteria are determined (see 3). \nc. The software life cycle environment, including the methods and tools to be used for",
    "c. The software life cycle environment, including the methods and tools to be used for \nthe activities of each software life cycle process has been selected and defined (see 4.4). \nd. Additional considerations, such as those discussed in section 12, have been \naddressed, if necessary. \ne. Software development standards consistent with the system safety objectives for the \nsoftware to be produced are defined (see 4.5). \nf. Software plans that comply with sections 4.3 and 11 have been produced.",
    "f. Software plans that comply with sections 4.3 and 11 have been produced. \ng. Development and revision of the software plans are coordinated (see 4.3).",
    "## 4.2 Software Planning Process Activities\n\nEffective planning is a determining factor in producing software that satisfies the guidance of this document. Activities for the software planning process include:",
    "a. The software plans should be developed that provide direction to the personnel \nperforming the software life cycle processes. See also 9.1. \nb. The software development standards to be used for the project should be defined or \nselected. \nc. Methods and tools should be chosen that aid error prevention and provide defect \ndetection in the software development processes. \nd. The software planning process should provide coordination between the software",
    "d. The software planning process should provide coordination between the software \ndevelopment and integral processes to provide consistency among strategies in the software plans. \ne. The means should be specified to revise the software plans as a project progresses. \nf. When multiple-version dissimilar software is used in a system, the software planning \nprocess should choose the methods and tools to achieve the dissimilarity necessary to \nsatisfy the system safety objectives.",
    "satisfy the system safety objectives. \ng. For the software planning process to be complete, the software plans and software \ndevelopment standards should be under change control and reviews of them completed (see 4.6). \nh. If deactivated code is planned, the software planning process should describe how the \ndeactivation mechanism and deactivated code will be defined and verified to satisfy system safety objectives. \ni.",
    "i. \nIf user-modifiable software is planned, related processes, tools, environment, and \ndata items substantiating the design (see 5.2.3) should be specified in the software plans and standards. \nj. \nWhen parameter data items are planned, the following should be addressed: \n1. The way that parameter data items are used. \n2. The software level of the parameter data items. \n3. The processes to develop, verify, and modify parameter data items, and any \nassociated tool qualification.",
    "associated tool qualification. \n4. Software load control and compatibility. \nk. The software planning process should address any additional considerations that are \napplicable. \nl. \nIf software development activities will be performed by a supplier, planning should address supplier oversight. \nOther software life cycle processes may begin before completion of the software planning process if transition criteria for the specific process activity are satisfied.",
    "## 4.3 Software Plans\n\nThe software plans define the means of satisfying the objectives of this document. They specify the organizations that will perform those activities. The software plans are:",
    "- \nThe Plan for Software Aspects of Certification (see 11.1) serves as the primary means for communicating the proposed development methods to the certification authority for agreement, and defines the means of compliance with this document. \n- \nThe Software Development Plan (see 11.2) defines the software life cycle(s), \nsoftware development environment, and the means by which the software \ndevelopment process objectives will be satisfied. \n-",
    "development process objectives will be satisfied. \n- \nThe Software Verification Plan (see 11.3) defines the means by which the software verification process objectives will be satisfied. \n- \nThe Software Configuration Management Plan (see 11.4) defines the means by which the software configuration management process objectives will be satisfied. \n- \nThe Software Quality Assurance Plan (see 11.5) defines the means by which the \nsoftware quality assurance process objectives will be satisfied.",
    "software quality assurance process objectives will be satisfied. \nActivities for the software plans include:",
    "a. The software plans should comply with this document. b. The software plans should define the transition criteria for software life cycle \nprocesses by specifying: \n1. The inputs to the process, including feedback from other processes. 2. Any integral process activities that may be required to act on these inputs. \n3. Availability of tools, methods, plans, and procedures. \nc. The software plans should state the procedures to be used to implement software",
    "c. The software plans should state the procedures to be used to implement software \nchanges prior to use on a certified product. Such changes may be as a result of \nfeedback from other processes and may cause a change to the software plans.",
    "## 4.4 Software Life Cycle Environment Planning",
    "Planning for the software life cycle environment defines the methods, tools, procedures, programming languages, and hardware that will be used to develop, verify, control, and produce the software life cycle data (see 11) and software product. Examples of how the software environment chosen can have a beneficial effect on the software include enforcing standards, detecting errors, and implementing error prevention and fault tolerance methods. The software life cycle environment is a potential",
    "error prevention and fault tolerance methods. The software life cycle environment is a potential error source that can contribute to failure conditions. Composition of this software life cycle environment may be influenced by the safety-related requirements determined by the system safety assessment process, for example, the use of dissimilar, redundant components. The goal of error prevention methods is to avoid errors during the software development processes that might contribute to a",
    "methods is to avoid errors during the software development processes that might contribute to a failure condition. The basic principle is to choose requirements development and design methods, tools, and programming languages that limit the opportunity for introducing errors, and verification methods that ensure that errors introduced are detected. The goal of fault tolerance methods is to include safety features in the software design or Source Code to ensure that the software will respond",
    "safety features in the software design or Source Code to ensure that the software will respond correctly to input data errors and prevent output and control errors. The need for error prevention or fault tolerance methods is determined by the system requirements and the system safety assessment process. The considerations presented above may affect:",
    "a. The methods and notations used in the software requirements process and software \ndesign process. \nb. The programming language(s) and methods used in the software coding process. \nc. The software development environment tools. \nd. The software verification and software configuration management tools. \ne. The need for tool qualification (see 12.2). \n\n## 4.4.1 Software Development Environment",
    "The software development environment is a significant factor in the production of high quality software. The software development environment can also adversely affect the production of software in several ways. For example, a compiler could introduce errors by producing a corrupted output or a linker could fail to reveal a memory allocation error that is present. Activities for the selection of software development environment methods and tools include:",
    "a. During the software planning process, the software development environment should \nbe chosen to reduce its potential risk to the software being developed. \nb. The use of tools or combinations of tools and parts of the software development \nenvironment should be chosen to achieve the necessary level of confidence that an error introduced by one part would be detected by another. An acceptable environment is produced when both parts are consistently used together. This",
    "selection includes the assessment of the need for tool qualification. \nc. The software verification process activities or software development standards, \nwhich include consideration of the software level, should be defined to reduce \npotential software development environment-related errors. \nd. If certification credit is sought for use of the tools in combination, the sequence of \noperation of the tools should be specified in the appropriate plan.",
    "operation of the tools should be specified in the appropriate plan. \ne. If optional features of software tools are chosen for use in a project, the effects of the \noptions should be examined and specified in the appropriate plan. This is especially important for compilers and autocode generators. \nf. Known tool problems and limitations should be assessed and those issues which can \nadversely affect airborne software should be addressed.",
    "## 4.4.2 Language And Compiler Considerations\n\nUpon successful completion of verification of the software product, the compiler is considered acceptable for that product. For this to be valid, the software verification process needs to consider particular features of the programming language and compiler. The software planning process considers these features when choosing a programming language and planning for verification. Activities include:",
    "a. Some compilers have features intended to optimize performance of the object code. If \nthe test cases give coverage consistent with the software level, the correctness of the optimization need not be verified. Otherwise, the impact of these features on structural coverage analysis should be determined. Additional information can be found in section 6.4.4.2. \nb. To implement certain features, compilers for some languages may produce object",
    "b. To implement certain features, compilers for some languages may produce object \ncode that is not directly traceable to the Source Code, for example, initialization, built-in error detection, or exception handling (see 6.4.4.2.b). The software planning \nprocess should provide a means to detect this object code and to ensure verification \ncoverage, and should define the means in the appropriate plan. \nc. If a new compiler, linkage editor, or loader version is introduced, or compiler options",
    "c. If a new compiler, linkage editor, or loader version is introduced, or compiler options \nare changed during the software life cycle, previous tests and coverage analyses may \nno longer be valid. The verification planning should provide a means of reverification that is consistent with sections 6 and 12.1.3.",
    "Note\n: Although the compiler is considered acceptable once all of the verification \nobjectives are satisfied, the compiler is only considered acceptable for that \nproduct and not necessarily for other products. \n\n## 4.4.3 Software Test Environment",
    "## 4.4.3 Software Test Environment\n\nSoftware test environment planning defines the methods, tools, procedures, and hardware that will be used to test the outputs of the integration process. Testing may be performed using the target computer, a target computer emulator, or a host computer simulator. Activities include:",
    "a. The emulator or simulator may need to be qualified as described in section 12.2. \nb. The differences between the target computer and the emulator or simulator, and the \neffects of these differences on the ability to detect errors and verify functionality, \nshould be considered. Detection of those errors should be provided by the software verification process and specified in the Software Verification Plan. \n\n## 4.5 Software Development Standards",
    "Software development standards define the rules and constraints for the software development processes. The software development standards include the Software Requirements Standards, the Software Design Standards, and the Software Code Standards. The software verification process uses these standards as a basis for evaluating the compliance of actual outputs of a process with intended outputs. Activities for development of the software standards include:",
    "a. The software development standards should comply with section 11. \nb. The software development standards should enable software components of a given \nsoftware product or related set of products to be uniformly designed and implemented. \nc. The software development standards should disallow the use of constructs or \nmethods that produce outputs that cannot be verified or that are not compatible with safety-related requirements.",
    "d. Robustness should be considered in the software development standards. \nNote 1: In developing standards, consideration can be given to previous experience. \nConstraints and rules on development, design, and coding methods can be included to control complexity. Defensive programming practices may be considered to improve robustness. \nNote 2:\n If allocated to software by system requirements, practices to detect and control",
    "Note 2:\n If allocated to software by system requirements, practices to detect and control \nerrors in stored data, and refresh and monitor hardware status and configuration may be used to mitigate single event upsets.",
    "## 4.6 Review Of The Software Planning Process\n\nReviews of the software planning process are conducted to ensure that the software plans and software development standards comply with the guidance of this document and means are provided to execute them. Activities include:",
    "a. Methods are chosen that enable the objectives of this document to be satisfied. \nb. Software life cycle processes can be applied consistently. \nc. Each process produces evidence that its outputs can be traced to their activity and \ninputs, showing the degree of independence of the activity, the environment, and the methods to be used. \nd. The outputs of the software planning process are consistent and comply with section \n11. \n\n## 5.0 Software Development Processes",
    "## 5.0 Software Development Processes\n\nThis section discusses the objectives and activities of the software development processes. The software development processes are applied as defined by the software planning process (see 4) and the Software Development Plan (see 11.2). Table A-2\n of Annex A is a summary of the objectives and outputs of the software development processes by software level. The software development processes are:",
    "- \nSoftware requirements process. \n- \nSoftware design process. \n- \nSoftware coding process. \n- \nIntegration process.",
    "Software development processes produce one or more levels of software requirements. High-level requirements are produced directly through analysis of system requirements and system architecture. Usually, these high-level requirements are further developed during the software design process, thus producing one or more successive, lower levels of requirements. However, if Source Code is generated directly from high-level requirements, then the high-level requirements are also considered low-level",
    "from high-level requirements, then the high-level requirements are also considered low-level requirements and the guidance for low-level requirements also apply.",
    "Note\n: The applicant may be required to justify software development processes that \nproduce a single level of requirements. \nThe development of software architecture involves decisions made about the structure of the software. During the software design process, the software architecture is defined and low-level requirements are developed. Low-level requirements are software requirements from which Source Code can be directly implemented without further information.",
    "Each software development process may produce derived requirements. Some examples of requirements that might be determined to be derived requirements are:",
    "- \nThe need for interrupt handling software to be developed for the chosen target \ncomputer. \n- \nThe specification of a periodic monitor's iteration rate when not specified by the system requirements allocated to software. \n- \nThe addition of scaling limits when using fixed point arithmetic.",
    "- \nThe addition of scaling limits when using fixed point arithmetic. \nHigh-level requirements may include derived requirements, and low-level requirements may include derived requirements. In order to determine the effects of derived requirements on the system safety assessment and system requirements, all derived requirements should be made available to the system processes including the system safety assessment process.",
    "## 5.1 Software Requirements Process\n\nThe software requirements process uses the outputs of the system life cycle processes to develop the high-level requirements. These high-level requirements include functional, performance, interface, and safety-related requirements. \n\n## 5.1.1 Software Requirements Process Objectives\n\nThe objectives of the software requirements process are:",
    "The objectives of the software requirements process are: \n\na. High-level requirements are developed. \nb. Derived high-level requirements are defined and provided to the system processes, \nincluding the system safety assessment process. \n\n## 5.1.2 Software Requirements Process Activities",
    "Inputs to the software requirements process include the system requirements, the hardware interface and system architecture (if not included in the requirements) from the system life cycle processes, and the Software Development Plan and the Software Requirements Standards from the software planning process. When the planned transition criteria have been satisfied, these inputs are used to develop the high-level requirements. The primary output of this process is the Software Requirements Data",
    "the high-level requirements. The primary output of this process is the Software Requirements Data (see 11.9). The software requirements process is complete when its objectives and the objectives of the integral processes associated with it are satisfied. Activities for this process include:",
    "a. The system functional and interface requirements that are allocated to software \nshould be analyzed for ambiguities, inconsistencies, and undefined conditions. \nb. Inputs to the software requirements process detected as inadequate or incorrect \nshould be reported as feedback to the input source processes for clarification or correction. \nc. Each system requirement that is allocated to software should be specified in the highlevel requirements.",
    "d. High-level requirements that address system requirements allocated to software to \npreclude system hazards should be defined. \ne. The high-level requirements should conform to the Software Requirements \nStandards, and be verifiable and consistent. \nf. The high-level requirements should be stated in quantitative terms with tolerances \nwhere applicable. \ng. The high-level requirements should not describe design or verification detail except \nfor specified and justified design constraints.",
    "for specified and justified design constraints. \nh. Derived high-level requirements and the reason for their existence should be defined. \ni. \nDerived high-level requirements should be provided to the system processes, \nincluding the system safety assessment process. \nj. \nIf parameter data items are planned, the high-level requirements should describe how",
    "j. \nIf parameter data items are planned, the high-level requirements should describe how \nany parameter data item is used by the software. The high-level requirements should also specify their structure, the attributes for each of their data elements, and, when applicable, the value of each element. The values of the parameter data item elements should be consistent with the structure of the parameter data item and the attributes of \nits data elements.",
    "## 5.2 Software Design Process\n\nThe high-level requirements are refined through one or more iterations in the software design process to develop the software architecture and the low-level requirements that can be used to implement Source Code. \n\n## 5.2.1 Software Design Process Objectives\n\nThe objectives of the software design process are:",
    "## 5.2.1 Software Design Process Objectives\n\nThe objectives of the software design process are: \n\na. The software architecture and low-level requirements are developed from the highlevel requirements. \nb. Derived low-level requirements are defined and provided to the system processes, \nincluding the system safety assessment process. \n\n## 5.2.2 Software Design Process Activities",
    "## 5.2.2 Software Design Process Activities\n\nThe software design process inputs are the Software Requirements Data, the Software Development Plan, and the Software Design Standards. When the planned transition criteria have been satisfied, the high-level requirements are used in the design process to develop software architecture and low-level requirements. This may involve one or more lower levels of requirements.",
    "The primary output of the process is the Design Description (see 11.10) which includes the software architecture and the low-level requirements. \n\nThe software design process is complete when its objectives and the objectives of the integral processes associated with it are satisfied. Activities for this process include:",
    "a. Low-level requirements and software architecture developed during the software \ndesign process should conform to the Software Design Standards and be traceable, verifiable, and consistent. \nb. Derived low-level requirements and the reason for their existence should be defined \nand analyzed to ensure that the higher level requirements are not compromised. \nc. Software design process activities could introduce possible modes of failure into the",
    "c. Software design process activities could introduce possible modes of failure into the \nsoftware or, conversely, preclude others. The use of partitioning or other architectural means in the software design may alter the software level assignment for some components of the software. In such cases, additional data should be defined as derived requirements and provided to the system processes, including the system safety assessment process.",
    "d. Interfaces between software components, in the form of data flow and control flow, \nshould be defined to be consistent between the components. \ne. Control flow and data flow should be monitored when safety-related requirements \ndictate, for example, watchdog timers, reasonableness-checks, and cross-channel \ncomparisons. \nf. Responses to failure conditions should be consistent with the safety-related \nrequirements.",
    "f. Responses to failure conditions should be consistent with the safety-related \nrequirements. \ng. Inadequate or incorrect inputs detected during the software design process should be \nprovided to the system life cycle processes, the software requirements process, or the software planning process as feedback for clarification or correction. \nNote\n: The current state of software engineering does not permit a quantitative",
    "Note\n: The current state of software engineering does not permit a quantitative \ncorrelation between complexity and the attainment of system safety objectives. \nWhile no objective guidance can be provided, the software design process should avoid introducing complexity because as the complexity of software increases, it \nbecomes more difficult to verify the design and to show that the safety-related \nrequirements are satisfied.",
    "## 5.2.3 Designing For User-Modifiable Software",
    "User-modifiable software is designed to be modified by its users. A modifiable component is that part of the software that is intended to be changed by the user and a non-modifiable component is that which is not intended to be changed by the user. Usermodifiable software may vary in complexity. Examples include a single memory bit used to select one of two equipment options, a table of messages, or a memory area that can be programmed, compiled, and linked for maintenance functions. Software",
    "or a memory area that can be programmed, compiled, and linked for maintenance functions. Software of any level can include a modifiable component. The activities for user-modifiable software include:",
    "a. The non-modifiable component should be protected from the modifiable component \nto prevent interference in the safe operation of the non-modifiable component. This",
    "to prevent interference in the safe operation of the non-modifiable component. This \nprotection can be enforced by hardware, by software, by the tools used to make the change, or by a combination of the three. If the protection is provided by software, it should be designed and verified at the same software level as the non-modifiable software. If the protection is provided by a tool, the tool should be categorized and qualified as defined in section 12.2.",
    "b. The means provided to change the modifiable component should be shown to be the \nonly means by which the modifiable component can be changed.",
    "## 5.2.4 Designing For Deactivated Code\n\nSystems or equipment may be designed to include several configurations, not all of which are intended to be used in every application. This can lead to deactivated code that cannot be executed, such as unselected functionality or unused library functions, or data that is not used. Deactivated code differs from dead code. The activities for deactivated code include:",
    "a. A mechanism should be designed and implemented to assure that deactivated \nfunctions or components have no adverse impact on active functions or components. \nb. Evidence should be available that the deactivated code is disabled for the \nenvironments where its use is not intended. Unintended execution of deactivated \ncode due to abnormal system conditions is the same as unintended execution of activated code.",
    "c. The development of deactivated code, like the development of the active code, \nshould comply with the objectives of this document. \n\n## 5.3 Software Coding Process\n\nIn the software coding process, the Source Code is implemented from the software architecture and the low-level requirements. \n\nNote\n: For the purpose of this document, compiling, linking, and loading are dealt with \nunder the Integration Process (see 5.4). \n\n## 5.3.1 Software Coding Process Objectives",
    "## 5.3.1 Software Coding Process Objectives\n\nThe objective of the software coding process is: \n\na. Source Code is developed from the low-level requirements. \n\n## 5.3.2 Software Coding Process Activities",
    "## 5.3.2 Software Coding Process Activities\n\nThe coding process inputs are the low-level requirements and software architecture from the software design process, the Software Development Plan, and the Software Code Standards. The software coding process may be entered or re-entered when the planned transition criteria are satisfied. The Source Code is produced by this process based upon the software architecture and the low-level requirements.",
    "The primary output of this process is Source Code (see 11.11). \n\nThe software coding process is complete when its objectives and the objectives of the integral processes associated with it are satisfied. Activities for this process include:",
    "a. The Source Code should implement the low-level requirements and conform to the \nsoftware architecture. \nb. The Source Code should conform to the Software Code Standards. \nc. Inadequate or incorrect inputs detected during the software coding process should be \nprovided to the software requirements process, software design process, and/or \nsoftware planning process as feedback for clarification or correction.",
    "software planning process as feedback for clarification or correction. \nd. Use of autocode generators should conform to the constraints defined in the planning \nprocess.",
    "## 5.4 Integration Process\n\nThe target computer and the Source Code from the software coding process are used with the compiling, linking, and loading data (see 11.16) in the integration process to develop the integrated system or equipment. \n\n## 5.4.1 Integration Process Objectives\n\nThe objective of the integration process is:",
    "## 5.4.1 Integration Process Objectives\n\nThe objective of the integration process is: \n\na. The Executable Object Code and its associated Parameter Data Item Files, if any, are \nproduced and loaded into the target hardware for hardware/software integration. \n\n## 5.4.2 Integration Process Activities",
    "The integration process consists of software integration and hardware/software integration. The integration process may be entered or re-entered when the planned transition criteria have been satisfied. The integration process inputs are the software architecture from the software design process, and the Source Code from the software coding process. The outputs of the integration process are the object code; Executable Object Code (see 11.12); Parameter Data Item File (see 11.22); and the",
    "the object code; Executable Object Code (see 11.12); Parameter Data Item File (see 11.22); and the compiling, linking, and loading data. The integration process is complete when its objectives and the objectives of the integral processes associated with it are satisfied. Activities for this process include:",
    "a. The object code and Executable Object Code should be generated from the Source \nCode and compiling, linking, and loading data. Any Parameter Data Item File should \nbe generated. \nb. Software integration should be performed on a host computer, a target computer \nemulator, or the target computer. \nc. The software should be loaded into the target computer for hardware/software \nintegration. \nd. Inadequate or incorrect inputs detected during the integration process should be",
    "integration. \nd. Inadequate or incorrect inputs detected during the integration process should be \nprovided to the software requirements process, the software design process, the software coding process, or the software planning process as feedback for clarification or correction. \ne. Patches should not be used in software submitted for use in a certified product to",
    "e. Patches should not be used in software submitted for use in a certified product to \nimplement changes in requirements or architecture, or changes found necessary as a result of the software verification process activities. Patches may be used on a limited, case-by-case basis, for example, to resolve known deficiencies in the \nsoftware development environment such as a known compiler problem. \nf. When a patch is used, these should be available:",
    "f. When a patch is used, these should be available: \n1. Confirmation that the software configuration management process can effectively \ntrack the patch. \n2. An analysis to provide evidence that the patched software satisfies all the \napplicable objectives. \n3. Justification in the Software Accomplishment Summary for use of a patch.",
    "## 5.5 Software Development Process Traceability\n\nSoftware development process traceability activities include:",
    "a. Trace Data, showing the bi-directional association between system requirements \nallocated to software and high-level requirements, is developed. The purpose of this \nTrace Data is to: \n1. Enable verification of the complete implementation of the system requirements \nallocated to software. \n2. Give visibility to those derived high-level requirements that are not directly \ntraceable to system requirements. \nb. Trace Data, showing the bi-directional association between the high-level",
    "b. Trace Data, showing the bi-directional association between the high-level \nrequirements and low-level requirements, is developed. The purpose of this Trace \nData is to: \n1. Enable verification of the complete implementation of the high-level \nrequirements. \n2. Give visibility to those derived low-level requirements that are not directly \ntraceable to high-level requirements and to the architectural design decisions made during the software design process.",
    "c. Trace Data, showing the bi-directional association between low-level requirements \nand Source Code, is developed. The purpose of this Trace Data is to: \n1. Enable verification that no Source Code implements an undocumented function. \n2. Enable verification of the complete implementation of the low-level \nrequirements. \n \nThis Page Intentionally Left Blank",
    "## 6.0 Software Verification Process",
    "This section discusses the objectives and activities of the software verification process. Verification is a technical assessment of the outputs of the software planning process, software development processes, and the software verification process. The software verification process is applied as defined by the software planning process (see 4) and the Software Verification Plan (see 11.3). See 4.6 for the verification of the outputs of the planning process. Verification is not simply testing.",
    "for the verification of the outputs of the planning process. Verification is not simply testing. Testing, in general, cannot show the absence of errors. As a result, the following sections use the term \"verify\" instead of \"test\" to discuss the software verification process activities, which are typically a combination of reviews, analyses, and tests.",
    "Tables A-3 through A-7 of Annex A contain a summary of the objectives and outputs of the software verification process, by software level. \n\nNote\n: For lower software levels, less emphasis is on:",
    "- Verification of Source Code. \n- Verification of low-level requirements. \n- Verification of the software architecture. \n- Degree of test coverage. - Control of verification procedures. \n- Independence of software verification process activities. \n- Overlapping software verification process activities, that is, multiple \nverification activities, each of which may be capable of detecting the same \nclass of error. \n- Robustness testing.",
    "class of error. \n- Robustness testing. \n- Verification activities with an indirect effect on error prevention or detection, \nfor example, conformance to software development standards.",
    "## 6.1 Purpose Of Software Verification\n\nThe purpose of the software verification process is to detect and report errors that may have been introduced during the software development processes. Removal of the errors is an activity of the software development processes. The software verification process verifies that:",
    "a. The system requirements allocated to software have been developed into high-level \nrequirements that satisfy those system requirements. \nb. The high-level requirements have been developed into software architecture and lowlevel requirements that satisfy the high-level requirements. If one or more levels of \nsoftware requirements are developed between high-level requirements and low-level",
    "software requirements are developed between high-level requirements and low-level \nrequirements, the successive levels of requirements are developed such that each successively lower level satisfies its higher level requirements. If code is generated directly from high-level requirements, this does not apply.",
    "c. The software architecture and low-level requirements have been developed into \nSource Code that satisfies the low-level requirements and software architecture. \nd. The Executable Object Code satisfies the software requirements (that is, intended \nfunction), and provides confidence in the absence of unintended functionality. \ne. The Executable Object Code is robust with respect to the software requirements such \nthat it can respond correctly to abnormal inputs and conditions.",
    "that it can respond correctly to abnormal inputs and conditions. \nf. The means used to perform this verification are technically correct and complete for \nthe software level.",
    "## 6.2 Overview Of Software Verification Process Activities",
    "Software verification process objectives are satisfied through a combination of reviews, analyses, the development of test cases and procedures, and the subsequent execution of those test procedures. Reviews and analyses provide an assessment of the accuracy, completeness, and verifiability of the software requirements, software architecture, and Source Code. The development of test cases and procedures may provide further assessment of the internal consistency and completeness of the",
    "and procedures may provide further assessment of the internal consistency and completeness of the requirements. The execution of the test procedures provides a demonstration of compliance with the requirements. The inputs to the software verification process include the system requirements, the software requirements, software architecture, Trace Data, Source Code, Executable Object Code, and the Software Verification Plan. The outputs of the software verification process are recorded in the",
    "Software Verification Plan. The outputs of the software verification process are recorded in the Software Verification Cases and Procedures (see 11.13), the Software Verification Results (see 11.14), and the associated Trace Data (see 11.21).",
    "The need for the requirements to be verifiable once they have been implemented in the software may itself impose additional requirements or constraints on the software development processes. \n\nSoftware verification considerations include:",
    "a. If the code tested is not identical to the airborne software, those differences should be \nspecified and justified. \nb. When it is not possible to verify specific software requirements by exercising the \nsoftware in a realistic test environment, other means should be provided and their justification for satisfying the software verification process objectives defined in the Software Verification Plan or Software Verification Results.",
    "c. Deficiencies and errors discovered during the software verification process should be \nreported to other software life cycle processes for clarification and correction as \napplicable. \nd. Reverification should be conducted following corrective actions and/or changes that \ncould impact the previously verified functionality. Reverification should ensure that \nthe modification has been correctly implemented.",
    "the modification has been correctly implemented. \ne. Verification independence is achieved when the verification activity is performed by \na person(s) other than the developer of the item being verified. A tool may be used to achieve equivalence to the human verification activity. For independence, the person \nwho created a set of low-level requirements-based test cases should not be the same person who developed the associated Source Code from those low-level \nrequirements.",
    "## 6.3 Software Reviews And Analyses",
    "Reviews and analyses are applied to the outputs of the software development processes. One distinction between reviews and analyses is that analyses provide repeatable evidence of correctness and reviews provide a qualitative assessment of correctness. A review may consist of an inspection of an output of a process guided by a checklist or similar aid. An analysis may examine in detail the functionality, performance, traceability, and safety implications of a software component, and its",
    "functionality, performance, traceability, and safety implications of a software component, and its relationship to other components within the system or equipment.",
    "There may be cases where the verification objectives described in this section cannot be completely satisfied via reviews and analyses alone. In such cases, those verification objectives may be satisfied with additional testing of the software product. For example, a combination of reviews, analyses, and tests may be developed to establish the worstcase execution time or verification of the stack usage. \n\n## 6.3.1 Reviews And Analyses Of High-Level Requirements",
    "## 6.3.1 Reviews And Analyses Of High-Level Requirements\n\nThese review and analysis activities detect and report requirements errors that may have been introduced during the software requirements process. These review and analysis activities confirm that the high-level requirements satisfy these objectives:",
    "a. Compliance with system requirements\n:  The objective is to ensure that the system \nfunctions to be performed by the software are defined, that the functional, performance, and safety-related requirements of the system are satisfied by the highlevel requirements, and that derived requirements and the reason for their existence are correctly defined. \nAccuracy and consistency\nb. \n:  The objective is to ensure that each high-level",
    "Accuracy and consistency\nb. \n:  The objective is to ensure that each high-level \nrequirement is accurate, unambiguous, and sufficiently detailed, and that the requirements do not conflict with each other. \nCompatibility with the target computer\nc. \n:  The objective is to ensure that no conflicts \nexist between the high-level requirements and the hardware/software features of the \ntarget computer, especially system response times and input/output hardware. \nVerifiability\nd.",
    "target computer, especially system response times and input/output hardware. \nVerifiability\nd. \n:  The objective is to ensure that each high-level requirement can be \nverified. \ne. \nConformance to standards:  The objective is to ensure that the Software Requirements Standards were followed during the software requirements process and that deviations from the standards are justified. \nf. Traceability",
    "f. Traceability\n:  The objective is to ensure that the functional, performance, and safetyrelated requirements of the system that are allocated to software were developed into the high-level requirements. \nAlgorithm aspects\ng. \n:  The objective is to ensure the accuracy and behavior of the \nproposed algorithms, especially in the area of discontinuities.",
    "## 6.3.2 Reviews And Analyses Of Low-Level Requirements\n\nThese review and analysis activities detect and report requirements errors that may have been introduced during the software design process. These review and analysis activities confirm that the low-level requirements satisfy these objectives:",
    "a. Compliance with high-level requirements\n:  The objective is to ensure that the lowlevel requirements satisfy the high-level requirements and that derived requirements \nand the design basis for their existence are correctly defined. \nAccuracy and consistency\nb. \n:  The objective is to ensure that each low-level \nrequirement is accurate and unambiguous, and that the low-level requirements do not conflict with each other. \nCompatibility with the target computer\nc.",
    "Compatibility with the target computer\nc. \n:  The objective is to ensure that no conflicts \nexist between the low-level requirements and the hardware/software features of the \ntarget computer, especially the use of resources such as bus loading, system response times, and input/output hardware. \nVerifiability\nd. \n:  The objective is to ensure that each low-level requirement can be \nverified. \nConformance to standards\ne. \n:  The objective is to ensure that the Software Design",
    "verified. \nConformance to standards\ne. \n:  The objective is to ensure that the Software Design \nStandards were followed during the software design process and that deviations from the standards are justified. \nTraceability\nf. \n:  The objective is to ensure that the high-level requirements and derived \nrequirements were developed into the low-level requirements. \nAlgorithm aspects\ng. \n:  The objective is to ensure the accuracy and behavior of the",
    "Algorithm aspects\ng. \n:  The objective is to ensure the accuracy and behavior of the \nproposed algorithms, especially in the area of discontinuities.",
    "## 6.3.3 Reviews And Analyses Of Software Architecture\n\nThese review and analysis activities detect and report errors that may have been introduced during the development of the software architecture. These review and analysis activities confirm that the software architecture satisfies these objectives:",
    "a. Compatibility with the high-level requirements\n:  The objective is to ensure that the \nsoftware architecture does not conflict with the high-level requirements, especially \nfunctions that ensure system integrity, for example, partitioning schemes. \nb. \nConsistency:  The objective is to ensure that a correct relationship exists between the",
    "b. \nConsistency:  The objective is to ensure that a correct relationship exists between the \ncomponents of the software architecture. This relationship exists via data flow and control flow. If the interface is to a component of a lower software level, it should also be confirmed that the higher software level component has appropriate \nprotection mechanisms in place to protect itself from potential erroneous inputs from the lower software level component.",
    "c. Compatibility with the target computer\n:  The objective is to ensure that no conflicts \nexist, especially initialization, asynchronous operation, synchronization, and \ninterrupts, between the software architecture and the hardware/software features of the target computer. \nVerifiability\nd. \n:  The objective is to ensure that the software architecture can be verified, \nfor example, there are no unbounded recursive algorithms. \nConformance to standards\ne.",
    "for example, there are no unbounded recursive algorithms. \nConformance to standards\ne. \n:  The objective is to ensure that the Software Design \nStandards were followed during the software design process and that deviations to the standards are justified, for example, deviations to complexity restriction and design construct rules. \nPartitioning integrity\nf. \n:  The objective is to ensure that partitioning breaches are \nprevented.",
    "## 6.3.4 Reviews And Analyses Of Source Code\n\nThese review and analysis activities detect and report errors that may have been introduced during the software coding process. Primary concerns include correctness of the code with respect to the software requirements and the software architecture, and conformance to the Software Code Standards. These review and analysis activities are usually confined to the Source Code and confirm that the Source Code satisfies these objectives:",
    "a. Compliance with the low-level requirements\n:  The objective is to ensure that the \nSource Code is accurate and complete with respect to the low-level requirements and \nthat no Source Code implements an undocumented function. \nCompliance with the software architecture\nb. \n:  The objective is to ensure that the Source \nCode matches the data flow and control flow defined in the software architecture. \nVerifiability\nc. \n:  The objective is to ensure the Source Code does not contain statements",
    "Verifiability\nc. \n:  The objective is to ensure the Source Code does not contain statements \nand structures that cannot be verified and that the code does not have to be altered to test it. \nConformance to standards\nd. \n:  The objective is to ensure that the Software Code \nStandards were followed during the development of the code, for example, \ncomplexity restrictions and code constraints. Complexity includes the degree of",
    "complexity restrictions and code constraints. Complexity includes the degree of \ncoupling between software components, the nesting levels for control structures, and the complexity of logical or numeric expressions. This analysis also ensures that \ndeviations to the standards are justified. \nTraceability\ne. \n:  The objective is to ensure that the low-level requirements were \ndeveloped into Source Code. \nf. \nAccuracy and consistency:  The objective is to determine the correctness and",
    "f. \nAccuracy and consistency:  The objective is to determine the correctness and \nconsistency of the Source Code, including stack usage, memory usage, fixed point \narithmetic overflow and resolution, floating-point arithmetic, resource contention and \nlimitations, worst-case execution timing, exception handling, use of uninitialized \nvariables, cache management, unused variables, and data corruption due to task or",
    "variables, cache management, unused variables, and data corruption due to task or \ninterrupt conflicts. The compiler (including its options), the linker (including its options), and some hardware features may have an impact on the worst-case execution timing and this impact should be assessed.",
    "## 6.3.5 Reviews And Analyses Of The Outputs Of The Integration Process\n\nThese review and analysis activities detect and report errors that may have been introduced during the integration process. The objective is to: \n\na. Ensure that the outputs of the integration process are complete and correct. \nActivities include conducting a detailed examination of the compiling, linking and loading data, and memory map. Typical examples of potential errors include:",
    "- \nCompiler warnings. \n- \nIncorrect hardware addresses. \n- \nMemory overlaps. \n- \nMissing software components. \n\n## 6.4 Software Testing\n\nSoftware testing is used to demonstrate that the software satisfies its requirements and to demonstrate with a high degree of confidence that errors that could lead to unacceptable failure conditions, as determined by the system safety assessment process, have been removed. The objectives of software testing are to execute the software to confirm that:",
    "a. The Executable Object Code complies with the high-level requirements. b. The Executable Object Code is robust with the high-level requirements. c. The Executable Object Code complies with the low-level requirements. \nd. The Executable Object Code is robust with the low-level requirements. e. The Executable Object Code is compatible with the target computer. \nFigure 6-1",
    "Figure 6-1\n is a diagram of the software testing activities that may be used to achieve the software testing objectives. The diagram shows three types of testing, which are:",
    "Hardware/software integration testing\n- \n:  To verify correct operation of the software in \nthe target computer environment. \nSoftware integration testing\n- \n:  To verify the interrelationships between software \nrequirements and components and to verify the implementation of the software requirements and software components within the software architecture. \nLow-level testing:  To verify the implementation of low-level requirements. \n- \nNote",
    "Low-level testing:  To verify the implementation of low-level requirements. \n- \nNote\n: If a test case and its corresponding test procedure are developed and executed for \nhardware/software integration testing or software integration testing, and satisfy \nthe requirements-based coverage and structural coverage, it is not necessary to",
    "the requirements-based coverage and structural coverage, it is not necessary to \nduplicate the test for low-level testing. Substituting nominally equivalent low-level tests for high-level tests may be less effective due to the reduced amount of overall \nfunctionality tested.",
    "## 6.4.1 Test Environment\n\nMore than one test environment may be needed to satisfy the objectives for software testing. A preferred test environment includes the software loaded into the target computer and tested in an environment that closely resembles the behavior of the target computer environment.",
    "Note\n: In many cases, the requirements-based coverage and structural coverage \nnecessary can be achieved only with more precise control and monitoring of the test inputs and code execution than generally possible in a fully integrated environment. Such testing may need to be performed on a small software \ncomponent that is functionally isolated from other software components.",
    "component that is functionally isolated from other software components. \nCertification credit may be given for testing done using a target computer emulator or a host computer simulator. Activities related to the test environment include:",
    "a. Selected tests should be performed in the integrated target computer environment, \nsince some errors are only detected in this environment. \n\n## 6.4.2 Requirements-Based Test Selection\n\nRequirements-based testing is emphasized because this strategy has been found to be the most effective at revealing errors. Activities for requirements-based test selection include:",
    "1. Specific test cases should be developed to include normal range test cases and \nrobustness (abnormal range) test cases. \n2. The specific test cases should be developed from the software requirements and the \nerror sources inherent in the software development processes. \nNote\n: Robustness test cases are requirements-based. The robustness testing criteria",
    "Note\n: Robustness test cases are requirements-based. The robustness testing criteria \ncannot be fully satisfied if the software requirements do not specify the correct software response to abnormal conditions and inputs. The test cases may \nreveal inadequacies in the software requirements, in which case the software \nrequirements should be modified. Conversely, if a complete set of requirements exists that covers all abnormal conditions and inputs, the",
    "robustness test cases will follow from those software requirements. \n3. Test procedures are generated from the test cases.",
    "## 6.4.2.1 Normal Range Test Cases\n\nNormal range test cases demonstrate the ability of the software to respond to normal inputs and conditions. Activities include:",
    "a. Real and integer input variables should be exercised using valid equivalence classes \nand boundary values. \nb. For time-related functions, such as filters, integrators, and delays, multiple iterations \nof the code should be performed to check the characteristics of the function in context. \nc. For state transitions, test cases should be developed to exercise the transitions \npossible during normal operation.",
    "possible during normal operation. \nd. For software requirements expressed by logic equations, the normal range test cases \nshould verify the variable usage and the Boolean operators.",
    "## 6.4.2.2 Robustness Test Cases\n\nRobustness test cases demonstrate the ability of the software to respond to abnormal inputs and conditions. Activities include:",
    "a. Real and integer variables should be exercised using equivalence class selection of \ninvalid values. \nb. System initialization should be exercised during abnormal conditions. \nc. The possible failure modes of the incoming data should be determined, especially \ncomplex, digital data strings from an external system. \nd. For loops where the loop count is a computed value, test cases should be developed \nto attempt to compute out-of-range loop count values, and thus demonstrate the",
    "to attempt to compute out-of-range loop count values, and thus demonstrate the \nrobustness of the loop-related code. \ne. A check should be made to ensure that protection mechanisms for exceeded frame \ntimes respond correctly. \nf. For time-related functions, such as filters, integrators, and delays, test cases should be \ndeveloped for arithmetic overflow protection mechanisms. \ng. For state transitions, test cases should be developed to provoke transitions that are not",
    "g. For state transitions, test cases should be developed to provoke transitions that are not \nallowed by the software requirements.",
    "## 6.4.3 Requirements-Based Testing Methods\n\nThe requirements-based testing methods discussed in this document are requirementsbased hardware/software integration testing, requirements-based software integration testing, and requirements-based low-level testing. \n\nWith the exception of hardware/software integration testing, these methods do not prescribe a specific test environment or strategy. Activities include:",
    "a. Requirements-Based Hardware/Software Integration Testing\n:  This testing method \nshould concentrate on error sources associated with the software operating within the target computer environment, and on the high-level functionality. Requirementsbased hardware/software integration testing ensures that the software in the target \ncomputer will satisfy the high-level requirements. Typical errors revealed by this \ntesting method include: \n- \nIncorrect interrupt handling. \n-",
    "testing method include: \n- \nIncorrect interrupt handling. \n- \nFailure to satisfy execution time requirements. \n- \nIncorrect software response to hardware transients or hardware failures, for example, start-up sequencing, transient input loads, and input power transients. \n- \nData bus and other resource contention problems, for example, memory \nmapping. \n- \nInability of built-in test to detect failures. \n- \nErrors in hardware/software interfaces. \n- \nIncorrect behavior of control loops. \n-",
    "- \nErrors in hardware/software interfaces. \n- \nIncorrect behavior of control loops. \n- \nIncorrect control of memory management hardware or other hardware devices under software control. \n- \nStack overflow. \n- \nIncorrect operation of mechanism(s) used to confirm the correctness and compatibility of field-loadable software. \n- \nViolations of software partitioning. \nb. Requirements-Based Software Integration Testing\n:  This testing method should",
    "b. Requirements-Based Software Integration Testing\n:  This testing method should \nconcentrate on the inter-relationships between the software requirements, and on the \nimplementation of requirements by the software architecture. Requirements-based \nsoftware integration testing ensures that the software components interact correctly",
    "software integration testing ensures that the software components interact correctly \nwith each other and satisfy the software requirements and software architecture. This method may be performed by expanding the scope of requirements through successive integration of code components with a corresponding expansion of the scope of the test cases. Typical errors revealed by this testing method include: \n- \nIncorrect initialization of variables and constants. \n- \nParameter passing errors. \n-",
    "- \nIncorrect initialization of variables and constants. \n- \nParameter passing errors. \n- \nData corruption, especially global data. \n- \nInadequate end-to-end numerical resolution. \n- \nIncorrect sequencing of events and operations. \nc. Requirements-Based Low-Level Testing\n:  This testing method should concentrate on \ndemonstrating that each software component complies with its low-level \nrequirements. Requirements-based low-level testing ensures that the software",
    "requirements. Requirements-based low-level testing ensures that the software \ncomponents satisfy their low-level requirements. Typical errors revealed by this \ntesting method include: \n- \nFailure of an algorithm to satisfy a software requirement. \n- \nIncorrect loop operations. \n- \nIncorrect logic decisions. \n- \nFailure to process correctly legitimate combinations of input conditions. \n- \nIncorrect responses to missing or corrupted input data. \n-",
    "- \nIncorrect responses to missing or corrupted input data. \n- \nIncorrect handling of exceptions, such as arithmetic faults or violations of array \nlimits. \n- \nIncorrect computation sequence. \n- \nInadequate algorithm precision, accuracy, or performance.",
    "## 6.4.4 Test Coverage Analysis",
    "Test coverage analysis is a two step process involving requirements-based coverage analysis and structural coverage analysis. The first step analyzes the test cases in relation to the software requirements to confirm that the selected test cases satisfy the specified criteria. The second step confirms that the requirements-based test procedures exercised the code structure to the applicable coverage criteria. If the structural coverage analysis showed the applicable coverage was not met,",
    "coverage criteria. If the structural coverage analysis showed the applicable coverage was not met, additional activities are identified for resolution of such situations as dead code (see 6.4.4.3)",
    "The objectives for test coverage analysis are:",
    "a. Test coverage of high-level requirements is achieved. \nb. Test coverage of low-level requirements is achieved. \nc. Test coverage of software structure to the appropriate coverage criteria is achieved. \nd. Test coverage of software structure, both data coupling and control coupling, is \nachieved. \n\n## 6.4.4.1 Requirements-Based Test Coverage Analysis",
    "## 6.4.4.1 Requirements-Based Test Coverage Analysis\n\nThis analysis is to determine how well the requirements-based testing verified the implementation of the software requirements. This analysis may reveal the need for additional requirements-based test cases. Activities include:",
    "a. Analysis, using the associated Trace Data, to confirm that test cases exist for each \nsoftware requirement. \nb. Analysis to confirm that test cases satisfy the criteria of normal and robustness testing \nas defined in section 6.4.2. \nc. Resolution of any deficiencies identified in the analysis. Possible solutions are adding \nor enhancing test cases. \nd. Analysis to confirm that all the test cases, and thus all the test procedures, used to",
    "d. Analysis to confirm that all the test cases, and thus all the test procedures, used to \nachieve structural coverage, are traceable to requirements.",
    "## 6.4.4.2 Structural Coverage Analysis\n\nThis analysis determines which code structure, including interfaces between components, was not exercised by the requirements-based test procedures. The requirements-based test cases may not have completely exercised the code structure, including interfaces, so structural coverage analysis is performed and additional verification produced to provide structural coverage. Activities include:",
    "a. Analysis of the structural coverage information collected during requirements-based \ntesting to confirm that the degree of structural coverage is appropriate to the software level. \nb. Structural coverage analysis may be performed on the Source Code, object code, or",
    "b. Structural coverage analysis may be performed on the Source Code, object code, or \nExecutable Object Code. Independent of the code form on which the structural coverage analysis is performed, if the software level is A and a compiler, linker, or other means generates additional code that is not directly traceable to Source Code statements, then additional verification should be performed to establish the correctness of such generated code sequences. \nNote",
    "Note\n: \"Additional code that is not directly traceable to Source Code statements\" is \ncode that introduces branches or side effects that are not immediately apparent at the Source Code level. This means that compiler-generated arraybound checks, for example, are not considered to be directly traceable to \nSource Code statements for the purposes of structural coverage analysis, and should be subjected to additional verification.",
    "c. Analysis to confirm that the requirements-based testing has exercised the data and \ncontrol coupling between code components. \nd. Structural coverage analysis resolution (see 6.4.4.3).",
    "## 6.4.4.3 Structural Coverage Analysis Resolution\n\nStructural coverage analysis may reveal code structure including interfaces that were not exercised during testing. Resolution will require additional software verification process activity. Causes of unexecuted code structure including interfaces, and associated activities to resolve them include:",
    "a. Shortcomings in requirements-based test cases or procedures\n:  The test cases should \nbe augmented or test procedures changed to provide the missing coverage. The method(s) used to perform the requirements-based coverage analysis may need to be reviewed. \nInadequacies in software requirements\nb. \n:  The software requirements should be \nmodified and additional test cases developed and test procedures executed. \nExtraneous code, including dead code\nc.",
    "Extraneous code, including dead code\nc. \n:  The code should be removed and an analysis \nperformed to assess the effect and the need for reverification. If extraneous code is found at the Source Code or object code level, it may be allowed to remain only if \nanalysis shows it does not exist in the Executable Object Code (for example, due to \nsmart compiling, linking, or some other mechanism), and procedures are in place to \nprevent inclusion in future builds. \nDeactivated code\nd.",
    "prevent inclusion in future builds. \nDeactivated code\nd. \n:  Deactivated code should be handled in one of two ways, \ndepending upon its defined category: \n1. \nCategory One:  Deactivated code that is not intended to be executed in any \ncurrent configuration used within any certified product. For this category, a",
    "current configuration used within any certified product. For this category, a \ncombination of analysis and testing should show that the means by which the deactivated code could be inadvertently executed are prevented, isolated, or eliminated. Any reassignment of the software level for Category One deactivated code should be justified by the system safety assessment process and documented",
    "in the Plan for Software Aspects of Certification. Similarly, any alleviation of the software verification process for Category One deactivated code should be justified by the software development processes and documented in the Plan for Software Aspects of Certification.",
    "2. Category Two\n:  Deactivated code that is only executed in certain approved \nconfigurations of the target computer environment. The operational configuration needed for normal execution of this code should be established and additional test cases and test procedures developed to satisfy the required coverage objectives. \n\n## 6.4.5 Reviews And Analyses Of Test Cases, Procedures, And Results\n\nThese review and analysis activities confirm that the software testing satisfies these objectives:",
    "a. Test cases\n:  The objectives related to verification of test cases are presented in \nsections 6.4.4.a and 6.4.4.b. \nTest procedures\nb. \n:  The objective is to verify that the test cases, including expected \nresults, were correctly developed into test procedures. \nTest results\nc. \n:  The objective is to ensure that the test results are correct and that \ndiscrepancies between actual and expected results are explained. \n\n## 6.5 Software Verification Process Traceability",
    "## 6.5 Software Verification Process Traceability\n\nSoftware verification process traceability activities include:",
    "a. Trace Data, showing the bi-directional association between software requirements \nand test cases, is developed. This Trace Data supports the requirements-based test \ncoverage analysis. \nb. Trace Data, showing the bi-directional association between test cases and test \nprocedures, is developed. This Trace Data enables verification that the complete set \nof test cases has been developed into test procedures.",
    "of test cases has been developed into test procedures. \nc. Trace Data, showing the bi-directional association between test procedures and test \nresults, is developed. This Trace Data enables verification that the complete set of \ntest procedures has been executed.",
    "## 6.6 Verification Of Parameter Data Items\n\nVerification of a parameter data item can be conducted separately from the verification of the Executable Object Code if all of the following apply:",
    "- \nThe Executable Object Code has been developed and verified by normal range testing to correctly handle all Parameter Data Item Files that comply with their defined structure and attributes. \n- \nThe Executable Object Code is robust with respect to Parameter Data Item Files structures and attributes. \n- \nAll behavior of the Executable Object Code resulting from the contents of the \nParameter Data Item File can be verified. \n-",
    "Parameter Data Item File can be verified. \n- \nThe structure of the life cycle data allows the parameter data item to be managed separately. \nUnless all conditions above are met, parameter data items should not be verified separately from the rest of the software. In this case, the Executable Object Code and the Parameter Data Item Files should be verified together.",
    "For parameter data items that can be verified separately from verification of the Executable Object Code, the objectives identified below apply. The objectives below may be achieved by a combination of tests, analyses, and reviews.",
    "a. The Parameter Data Item File should be verified to comply with its structure as \ndefined by the high-level requirements; this verification includes ensuring that the \nParameter Data Item File does not contain any elements not defined by the high-level \nrequirements. Each data element in the Parameter Data Item File should also be shown to have the correct value, to be consistent with other data elements, and to \ncomply with its attributes as defined by the high-level requirements. \nNote",
    "comply with its attributes as defined by the high-level requirements. \nNote\n: For certain data elements, their attributes may be the only aspect that needs \nto be verified. In other cases, the value of the data element may need to be verified. \nb. All elements of the Parameter Data Item File have been covered during verification.",
    "b. All elements of the Parameter Data Item File have been covered during verification. \nIf changes are made to the structure or attributes of the parameter data item, then the need for modification and reverification of the Executable Object Code should be analyzed.",
    "## 7.0 Software Configuration Management Process\n\nThis section discusses the objectives and activities of the software configuration management (SCM) process. The SCM process is applied as defined by the software planning process (see 4) and the Software Configuration Management Plan (see 11.4).",
    "Outputs of the SCM process are recorded in Software Configuration Management Records (see 11.18) or in other software life cycle data. The SCM process, working in cooperation with the other software life cycle processes, assists in:",
    "a. Providing a defined and controlled configuration of the software throughout the \nsoftware life cycle. \nb. Providing the ability to consistently replicate the Executable Object Code and \nParameter Data Item Files, if any, for software manufacture or to regenerate it in case \nof a need for investigation or modification. \nc. Providing control of process inputs and outputs during the software life cycle that \nensures consistency and repeatability of process activities.",
    "ensures consistency and repeatability of process activities. \nd. Providing a known point for review, assessing status, and change control by control \nof configuration items and the establishment of baselines. \ne. Providing controls that ensure problems receive attention and changes are recorded, \napproved, and implemented. \nf. Providing evidence of approval of the software by control of the outputs of the \nsoftware life cycle processes.",
    "software life cycle processes. \ng. Assessing the software product compliance with requirements. \nh. Ensuring that secure physical archiving, recovery, and control are maintained for the \nconfiguration items.",
    "## 7.1 Software Configuration Management Process Objectives\n\nThe SCM process objectives are:",
    "a. Each configuration item and its successive versions are labeled unambiguously so \nthat a basis is established for the control and reference of configuration items. \nb. Baselines are defined for further software life cycle process activity and allow \nreference to, control of, and traceability between, configuration items. \nc. The problem reporting process records process non-compliance with software plans \nand standards, records deficiencies of outputs of software life cycle processes,",
    "and standards, records deficiencies of outputs of software life cycle processes, \nrecords anomalous behavior of software products, and ensures resolution of these problems. \nd. Change control provides for recording, evaluation, resolution, and approval of \nchanges throughout the software life cycle. \ne. Change review ensures problems and changes are assessed, approved, or",
    "e. Change review ensures problems and changes are assessed, approved, or \ndisapproved, approved changes are implemented, and feedback is provided to affected processes through problem reporting and change control methods defined during the software planning process. \nf. Status accounting provides data for the configuration management of software life \ncycle processes with respect to configuration identification, baselines, Problem Reports, and change control.",
    "g. Archival and retrieval ensures that the software life cycle data associated with the \nsoftware product can be retrieved in case of a need to duplicate, regenerate, retest or modify the software product. The objective of the release activity is to ensure that only authorized software is used, especially for software manufacturing, in addition to being archived and retrievable. \nh. Software load control ensures that the Executable Object Code and Parameter Data",
    "h. Software load control ensures that the Executable Object Code and Parameter Data \nItem Files, if any, are loaded into the system or equipment with appropriate \nsafeguards. \ni. \nSoftware life cycle environment control ensures that the tools used to produce the software are identified, controlled, and retrievable. \nThe objectives for SCM are independent of software level. However, two categories of software life cycle data may exist based on the SCM controls applied to the data (see 7.3).",
    "of Annex A is a summary of the objectives and outputs of the SCM process. \n\n## 7.2 Software Configuration Management Process Activities",
    "The SCM process includes the activities of configuration identification, change control, baseline establishment, and archiving of the software product, including the related software life cycle data. The SCM process does not stop when the software product is approved by the certification authority, but continues throughout the service life of the system or equipment. If software life cycle activities will be performed by a supplier, then configuration management activities should be applied to",
    "will be performed by a supplier, then configuration management activities should be applied to the supplier.",
    "## 7.2.1 Configuration Identification\n\nActivities include:",
    "a. Configuration identification should be established for the software life cycle data. \nb. Configuration identification should be established for each configuration item, for \neach separately controlled component of a configuration item, and for combinations of configuration items that comprise a software product. \nc. Configuration items should be configuration identified prior to the implementation of \nchange control and traceability analysis.",
    "change control and traceability analysis. \nd. A configuration item should be configuration identified before that item is used by \nother software life cycle processes, referenced by other software life cycle data, or used for software manufacture or software loading. \ne. If the software product identification cannot be determined by physical examination \n(for example, part number plate examination), then the Executable Object Code and",
    "(for example, part number plate examination), then the Executable Object Code and \nParameter Data Item Files, if any, should contain configuration identification that can be accessed by other parts of the system or equipment. This may be applicable to field-loadable software.",
    "## 7.2.2 Baselines And Traceability\n\nActivities include:",
    "a. Baselines should be established for configuration items used for certification credit. \nIntermediate baselines may be established to aid in controlling software life cycle process activities. \nb. A software product baseline should be established for the software product and \ndefined in the Software Configuration Index (see 11.16). \nNote\n: User-modifiable software is not included in the software product baseline,",
    "Note\n: User-modifiable software is not included in the software product baseline, \nexcept for its associated protection and boundary components. Therefore, modifications may be made to user-modifiable software without affecting the \nconfiguration identification of the software product baseline. \nc. Baselines should be established in controlled software libraries, whether physical,",
    "c. Baselines should be established in controlled software libraries, whether physical, \nelectronic, or other, to ensure their integrity. Once a baseline is established, it should be protected from change. \nd. Change control activities should be followed to develop a derivative baseline from an \nestablished baseline. \ne. A baseline should be traceable to the baseline from which it was derived, if \ncertification credit is sought for software life cycle process activities or data",
    "certification credit is sought for software life cycle process activities or data \nassociated with the development of the previous baseline. \nf. A configuration item should be traceable to the configuration item from which it was \nderived, if certification credit is sought for software life cycle process activities or \ndata associated with the development of the previous configuration item. \ng. A baseline or configuration item should be traceable either to the output it identifies",
    "g. A baseline or configuration item should be traceable either to the output it identifies \nor to the process with which it is associated.",
    "## 7.2.3 Problem Reporting, Tracking, And Corrective Action\n\nActivities include:",
    "a. A Problem Report should be prepared that describes the process non-compliance with \nplans, output deficiency, or software anomalous behavior, and the corrective action taken, as defined in section 11.17. \nNote\n: Software life cycle process and software product problems may be recorded in \nseparate problem reporting systems. \nb. Problem reporting should provide for configuration identification of affected",
    "b. Problem reporting should provide for configuration identification of affected \nconfiguration item(s) or definition of affected process activities, status reporting of Problem Reports, and approval and closure of Problem Reports. \nc. Problem Reports that require corrective action of the software product or outputs of \nsoftware life cycle processes should invoke the change control activity.",
    "## 7.2.4 Change Control\n\nActivities include:",
    "a. Change control should preserve the integrity of the configuration items and baselines \nby providing protection against their change. \nb. Change control should ensure that any change to a configuration item requires a \nchange to its configuration identification. \nc. Changes to baselines and to configuration items under change control to produce",
    "c. Changes to baselines and to configuration items under change control to produce \nderivative baselines should be recorded, approved, and tracked. Problem reporting is related to change control, since resolution of a reported problem may result in changes to configuration items or baselines. \nNote\n: It is generally recognized that early implementation of change control assists the control and management of software life cycle process activities.",
    "d. Software changes should be traced to their origin and the software life cycle \nprocesses repeated from the point at which the change affects their outputs. For example, an error discovered at hardware/software integration, that is shown to result from an incorrect design, should result in design correction, code correction, and \nrepetition of the associated integral process activities. \ne. Throughout the change activity, software life cycle data affected by the change",
    "e. Throughout the change activity, software life cycle data affected by the change \nshould be updated and records should be maintained for the change control activity. \nThe change control activity is aided by the change review activity.",
    "## 7.2.5 Change Review\n\nActivities include:",
    "a. Assessment of the impact of the problem or proposed change on system \nrequirements. Feedback should be provided to the system processes, including the system safety assessment process, and any responses from the system processes should be assessed. \nb. Assessment of the impact of the problem or proposed change on software life cycle \ndata identifying changes to be made and actions to be taken. \nc. Confirmation that affected configuration items are configuration identified.",
    "c. Confirmation that affected configuration items are configuration identified. \nd. Feedback of Problem Report or change impact and decisions to affected processes.",
    "## 7.2.6 Configuration Status Accounting\n\nActivities include: \n\na. Reporting on configuration item identification, baseline identification, Problem \nReport status, change history, and release status. \nb. Definition of the data to be maintained and the means of recording and reporting \nstatus of this data. \n\n## 7.2.7 Archive, Retrieval, And Release\n\nActivities include:",
    "a. Software life cycle data associated with the software product should be retrievable \nfrom the approved source (for example, an archive at the developing organization or company). \nb. Procedures should be established to ensure the integrity of the stored data, regardless \nof medium of storage, by: \n1. Ensuring that no unauthorized changes can be made. \n2. Selecting storage media that minimize regeneration errors or deterioration.",
    "2. Selecting storage media that minimize regeneration errors or deterioration. \n3. Preventing loss or corruption of data over time. Depending on the storage media \nused, this may include periodically exercising the media or refreshing the \narchived data. \n4. Storing duplicate copies in physically separate archives that minimize the risk of \nloss in the event of a disaster. \nc. The duplication process should be verified to produce accurate copies, and",
    "c. The duplication process should be verified to produce accurate copies, and \nprocedures should exist that ensure error-free copying of the Executable Object Code \nand Parameter Data Item Files, if any \nd. Configuration items should be identified and released prior to use for software",
    "d. Configuration items should be identified and released prior to use for software \nmanufacture and the authority for their release should be established. As a minimum, the components of the software product loaded into the airborne system or equipment should be released. This includes the Executable Object Code and Parameter Data \nItem Files, if any, and may also include associated media for software loading. \nNote\n: Release is generally also required for the data that defines the approved",
    "Note\n: Release is generally also required for the data that defines the approved \nsoftware for loading into the airborne system or equipment. Definition of that \ndata is outside the scope of this document, but may include the Software Configuration Index. \ne. Data retention procedures should be established to satisfy airworthiness requirements \nand enable software modifications. \nNote: Additional data retention considerations may include items such as business",
    "Note: Additional data retention considerations may include items such as business \nneeds and future certification authority reviews, which are outside the scope of this document.",
    "## 7.3 Data Control Categories\n\nSoftware life cycle data can be assigned to one of two configuration management control categories: Control Category 1 (CC1) and Control Category 2 (CC2). Table 7-1\n defines the set of SCM process activities associated with each control category, where - indicates the minimum activities that apply for software life cycle data of that category. CC2 \nactivities are a subset of the CC1 activities.",
    "The Annex A tables specify the control category by software level for the software life cycle data items.",
    "|                                          |                                               | Table 7-   | 1 SCM Process Activities Associated with CC1 and CC2 Data    |\n|------------------------------------------|-----------------------------------------------|------------|--------------------------------------------------------------|",
    "| SCM Process Activity                     | Reference                                     | CC1        | CC2                                                          |\n| Configuration Identification             | 7.2.1                                         |            |                                                              |",
    "| -                                        |                                               |            |                                                              |\n|                                          |                                               |            |                                                              |",
    "| -                                        |                                               |            |                                                              |\n|                                          |                                               |            |                                                              |",
    "| -                                        |                                               |            |                                                              |\n|                                          |                                               |            |                                                              |",
    "|                                          |                                               |            |                                                              |\n| Baselines                                | 7.2.2.a                                       |            |                                                              |",
    "| 7.2.2.b                                  |                                               |            |                                                              |\n| 7.2.2.c                                  |                                               |            |                                                              |",
    "| 7.2.2.d                                  |                                               |            |                                                              |\n| 7.2.2.e                                  |                                               |            |                                                              |",
    "| -                                        |                                               |            |                                                              |\n|                                          |                                               |            |                                                              |",
    "| -                                        |                                               |            |                                                              |\n|                                          | Traceability                                  | 7.2.2.f    |                                                              |",
    "| 7.2.2.g                                  |                                               |            |                                                              |\n| Problem Reporting                        | 7.2.3                                         |            |                                                              |",
    "| -                                        |                                               |            |                                                              |\n|                                          |                                               |            |                                                              |",
    "|                                          |                                               |            |                                                              |\n| -                                        |                                               |            |                                                              |",
    "|                                          |                                               |            |                                                              |\n| -                                        |                                               |            |                                                              |",
    "|                                          | Change Control - integrity and identification | 7.2.4.a    |                                                              |\n| 7.2.4.b                                  |                                               |            |                                                              |",
    "| -                                        |                                               |            |                                                              |\n|                                          |                                               |            |                                                              |",
    "|                                          |                                               |            |                                                              |\n| Change Control - tracking                | 7.2.4.c                                       |            |                                                              |",
    "| 7.2.4.d                                  |                                               |            |                                                              |\n| 7.2.4.e                                  |                                               |            |                                                              |",
    "| Change Review                            | 7.2.5                                         |            |                                                              |\n| -                                        |                                               |            |                                                              |",
    "|                                          |                                               |            |                                                              |\n|                                          |                                               |            |                                                              |",
    "| Configuration Status Accounting          | 7.2.6                                         |            |                                                              |\n| -                                        |                                               |            |                                                              |",
    "|                                          |                                               |            |                                                              |\n|                                          |                                               |            |                                                              |",
    "| Retrieval                                | 7.2.7.a                                       |            |                                                              |\n| -                                        |                                               |            |                                                              |",
    "|                                          |                                               |            |                                                              |\n| -                                        |                                               |            |                                                              |",
    "|                                          |                                               |            |                                                              |\n| Protection against Unauthorized Changes  | 7.2.7.b.1                                     |            |                                                              |",
    "| -                                        |                                               |            |                                                              |\n|                                          |                                               |            |                                                              |",
    "| -                                        |                                               |            |                                                              |\n|                                          |                                               |            |                                                              |",
    "| -                                        |                                               |            |                                                              |\n|                                          |                                               |            |                                                              |",
    "|                                          |                                               |            |                                                              |\n| Media Selection, Refreshing, Duplication | 7.2.7.b.2                                     |            |                                                              |",
    "| 7.2.7.b.3                                |                                               |            |                                                              |\n| 7.2.7.b.4                                |                                               |            |                                                              |",
    "| 7.2.7.c                                  |                                               |            |                                                              |\n| Release                                  | 7.2.7.d                                       |            |                                                              |",
    "| -                                        |                                               |            |                                                              |\n|                                          |                                               |            |                                                              |",
    "|                                          |                                               |            |                                                              |\n| Data Retention                           | 7.2.7.e                                       |            |                                                              |",
    "| -                                        |                                               |            |                                                              |\n|                                          |                                               |            |                                                              |",
    "| -                                        |                                               |            |                                                              |\n|                                          |                                               |            |                                                              |",
    "## 7.4 Software Load Control\n\nSoftware load control refers to the process by which programmed instructions and data are transferred from a master memory device into the system or equipment. For example, methods may include (subject to approval by the certification authority) the installation of factory pre-programmed memory devices or \"in situ\" re-programming of the system or equipment using a field loading device. Whichever method is used, software load control should include:",
    "a. Procedures for part numbering and media identification that identify software \nconfigurations that are intended to be approved for loading into the airborne system or equipment. \nb. Whether the software is delivered as an end item or is delivered installed in the \nairborne system or equipment, records should be kept that confirm software \ncompatibility with the airborne system or equipment hardware. \n\n## 7.5 Software Life Cycle Environment Control",
    "## 7.5 Software Life Cycle Environment Control\n\nThe software life cycle environment tools are defined by the software planning process and identified in the Software Life Cycle Environment Configuration Index (see 11.15). \n\nActivities include:",
    "a. Configuration identification should be established for the Executable Object Code, or \nequivalent, of the tools used to develop, control, build, verify, and load the software. \nb. The SCM process for controlling qualified tools should comply with the objectives \nassociated with Control Category 1 or Control Category 2 data (see 7.3), according to \nthe guidance provided by section 12.2.3. \nc. Unless section 7.5b applies, the SCM process for controlling the Executable Object",
    "c. Unless section 7.5b applies, the SCM process for controlling the Executable Object \nCode, or equivalent, for tools used to build and load the software (for example, \ncompilers, assemblers, and linkage editors) should comply with the objectives associated with Control Category 2 data, as a minimum. \n \nThis Page Intentionally Left Blank",
    "## 8.0 Software Quality Assurance Process",
    "This section discusses the objectives and activities of the software quality assurance (SQA) process. The SQA process is applied as defined by the software planning process (see 4) and the Software Quality Assurance Plan (see 11.5). Outputs of the SQA process activities are recorded in Software Quality Assurance Records (see 11.19) or other software life cycle data. The SQA process assesses the software life cycle processes and their outputs to obtain assurance that objectives are satisfied,",
    "software life cycle processes and their outputs to obtain assurance that objectives are satisfied, deficiencies are detected, evaluated, tracked, and resolved, and software product and software life cycle data conform to certification requirements.",
    "## 8.1 Software Quality Assurance Process Objectives\n\nThe SQA process objectives provide confidence that the software life cycle processes produce software that conforms to its requirements by assuring that these processes are performed in compliance with the approved software plans and standards. The objectives of the SQA process are to obtain assurance that:",
    "a. Software plans and standards are developed and reviewed for compliance with this \ndocument and for consistency. \nb. Software life cycle processes, including those of suppliers, comply with approved \nsoftware plans and standards. \nc. The transition criteria for the software life cycle processes are satisfied. \nd. A conformity review of the software product is conducted. \n\n## 8.2 Software Quality Assurance Process Activities\n\nActivities for satisfying the SQA process objectives include:",
    "a. The SQA process should take an active role in the activities of the software life cycle \nprocesses, and have those performing the SQA process enabled with the authority, responsibility, and independence to ensure that the SQA process objectives are satisfied. \nb. The SQA process should provide assurance that software plans and standards are \ndeveloped and reviewed for compliance with this document and for consistency.",
    "developed and reviewed for compliance with this document and for consistency. \nc. The SQA process should provide assurance that the software life cycle processes \ncomply with the approved software plans and standards. \nd. The SQA process should include audits of the software life cycle processes during \nthe software life cycle to obtain assurance that: \n1. Software plans are available as specified in section 4.2. \n2. Deviations from the software plans and standards are detected, recorded,",
    "2. Deviations from the software plans and standards are detected, recorded, \nevaluated, tracked, and resolved. \nNote\n: It is generally accepted that early detection of process deviations assists \nefficient achievement of software life cycle process objectives. \n3. Approved deviations are recorded. \n4. The software development environment has been provided as specified in the \nsoftware plans. \n5. The problem reporting, tracking, and corrective action process activities comply",
    "5. The problem reporting, tracking, and corrective action process activities comply \nwith the Software Configuration Management Plan. \n6. Inputs provided to the software life cycle processes by the system processes, \nincluding the system safety assessment process, have been addressed. \nNote\n: Monitoring of the activities of software life cycle processes may be \nperformed to provide assurance that the activities are under control.",
    "performed to provide assurance that the activities are under control. \ne. The SQA process should provide assurance that the transition criteria for the software \nlife cycle processes have been satisfied in compliance with the approved software plans. \nf. The SQA process should provide assurance that software life cycle data is controlled \nin accordance with the control categories as defined in section 7.3 and the tables of Annex A.",
    "in accordance with the control categories as defined in section 7.3 and the tables of Annex A. \ng. Prior to the delivery of software products submitted as part of a certification \napplication, a software conformity review should be conducted. \nh. The SQA process should produce records of the SQA process activities (see 11.19), \nincluding audit results and evidence of completion of the software conformity review for each software product submitted as part of certification application. \ni.",
    "i. \nThe SQA process should provide assurance that supplier processes and outputs comply with approved software plans and standards.",
    "## 8.3 Software Conformity Review\n\nThe purpose of the software conformity review is to obtain assurance, for a software product submitted as part of a certification application, that the software life cycle processes are complete, software life cycle data is complete, and the Executable Object Code and Parameter Data Item Files, if any, are controlled and can be regenerated. \n\nThis review should determine that:",
    "a. Planned software life cycle process activities for certification credit, including the \ngeneration of software life cycle data, have been completed and records of their completion are retained. \nb. Software life cycle data developed from specific system requirements, safety-related \nrequirements, or software requirements are traceable to those requirements. \nc. Evidence exists that software life cycle data have been produced in accordance with",
    "c. Evidence exists that software life cycle data have been produced in accordance with \nsoftware plans and standards, and is controlled in accordance with the SCM Plan. \nd. Evidence exists that Problem Reports have been evaluated and have their status \nrecorded. \ne. Software requirement deviations are recorded and approved. f. The Executable Object Code and Parameter Data Item Files, if any, can be \nregenerated from the archived Source Code.",
    "regenerated from the archived Source Code. \ng. The approved software can be loaded successfully through the use of released \ninstructions. \nh. Problem Reports deferred from a previous software conformity review are reevaluated to determine their status. \ni. \nIf certification credit is sought for the use of previously developed software, the current software product baseline is traceable to the previous baseline and the approved changes to that baseline. \n \nThis Page Intentionally Left Blank",
    "## 9.0 Certification Liaison Process\n\nThe objectives of the certification liaison process are to:",
    "a. Establish communication and understanding between the applicant and the \ncertification authority throughout the software life cycle to assist the certification process. \nb. Gain agreement on the means of compliance through approval of the Plan for \nSoftware Aspects of Certification. \nc. Provide compliance substantiation. \nThe certification liaison process is applied as defined by the software planning process \n(see 4) and the Plan for Software Aspects of Certification (see 11.1). Table A-10",
    "(see 4) and the Plan for Software Aspects of Certification (see 11.1). Table A-10\n of Annex A is a summary of the objectives and outputs of this process.",
    "## 9.1 Means Of Compliance And Planning\n\nThe applicant proposes a means of compliance that defines how the development of the airborne system or equipment will satisfy the certification basis. The Plan for Software Aspects of Certification (see 11.1) defines the software aspects of the airborne system or equipment within the context of the proposed means of compliance. This plan also states the software level(s) as determined by the system safety assessment process. Activities include:",
    "a. Submitting the Plan for Software Aspects of Certification and other requested data to \nthe certification authority for review. \nb. Resolving issues identified by the certification authority concerning the planning for \nthe software aspects of certification. \nc. Obtaining agreement with the certification authority on the Plan for Software Aspects \nof Certification. \n\n## 9.2 Compliance Substantiation",
    "The applicant provides evidence that the software life cycle processes satisfy the software plans, by making software life cycle data available to the certification authority for review. Certification authority reviews may take place at various facilities. For example, the reviews may take place at the applicant's facilities, the applicant's supplier's facilities, or at the certification authority's facilities. This may involve discussions with the applicant or its suppliers. The applicant",
    "facilities. This may involve discussions with the applicant or its suppliers. The applicant arranges these reviews of the activities of the software life cycle processes and makes software life cycle data available as needed.",
    "Activities include: \n\na. Resolving issues raised by the certification authority as a result of its reviews. \nb. Submitting the Software Accomplishment Summary (see 11.20) and Software \nConfiguration Index (see 11.16) to the certification authority. \nc. Submitting or making available other data or evidence of compliance requested by \nthe certification authority. \n\n## 9.3 Minimum Software Life Cycle Data Submitted To Certification Authority",
    "## 9.3 Minimum Software Life Cycle Data Submitted To Certification Authority\n\nThe minimum software life cycle data that is submitted to the certification authority is: \n\na. Plan for Software Aspects of Certification. \nb. Software Configuration Index. \nc. Software Accomplishment Summary. \n\n## 9.4 Software Life Cycle Data Related To Type Design",
    "## 9.4 Software Life Cycle Data Related To Type Design\n\nUnless otherwise agreed by the certification authority, the regulations concerning retrieval and approval of software life cycle data related to the type design of the product to be certified applies to: \n\na. Software Requirements Data. \nb. Design Description. \nc. Source Code. \nd. Executable Object Code and Parameter Data Item Files, if any. \ne. Software Configuration Index. \nf. Software Accomplishment Summary.",
    "## 10.0 Overview Of Certification Process\n\nThis section is an overview of the certification process with respect to software aspects of airborne systems and equipment, and is provided for information purposes only. \n\nThe airborne community and certification authorities use several terms related to aircraft approval for flight with its associated equipment. The terms used are \"certification\", \n\"approval\", and, with respect to tools, \"qualification\".",
    "\"Certification\" applies to aircraft, engines, or propellers; and, in respect of some certification authorities, auxiliary power units. The certification authorities consider the software as part of the airborne system or equipment installed on the certified product;",
    "that is, the certification authorities do not certify the software as a unique, stand-alone product. Systems and equipment, including embedded software, should be \"approved\" in order to be accepted as a part of a certification. Approval by the certification authorities is given dependent upon a successful demonstration or by review of the products of the software life cycle. Any such approval currently has significance only within the context of a specific certification. Tool \"qualification\" is",
    "has significance only within the context of a specific certification. Tool \"qualification\" is discussed in section 12.2.",
    "## 10.1 Certification Basis",
    "The certification authority establishes the certification basis for the product to be certified in consultation with the applicant. The certification basis defines the particular regulations together with any special conditions to be applied beyond the published regulations. When certified products are being modified, the certification authority considers the impact the modification has on the original certification basis. In some cases, the certification basis for the modification may not",
    "original certification basis. In some cases, the certification basis for the modification may not change from the original certification basis; however, the original means of compliance may not be applicable for showing that the modification complies with the certification basis and may need to be changed.",
    "## 10.2 Software Aspects Of Certification",
    "The certification authority assesses the Plan for Software Aspects of Certification for completeness and consistency with the means of compliance that was agreed upon to satisfy the certification basis. The certification authority satisfies itself that the software level(s) proposed by the applicant is consistent with the outputs of the system safety assessment process and other system life cycle data. The certification authority informs the applicant of issues with the proposed software plans",
    "data. The certification authority informs the applicant of issues with the proposed software plans that need to be satisfied prior to certification authority agreement.",
    "## 10.3 Compliance Determination\n\nPrior to certification, the certification authority determines that the product to be certified, including the software aspects of its systems or equipment, complies with the certification basis. For the software, this is accomplished by reviewing the Software Accomplishment Summary and evidence of compliance. The certification authority uses the Software Accomplishment Summary as an overview for the software aspects of certification.",
    "The certification authority may review at its discretion the software life cycle processes and their outputs during the software life cycle, as discussed in section 9.2. \n\n## 11.0 Software Life Cycle Data\n\nData is produced during the software life cycle to plan, direct, explain, define, record, or provide evidence of activities. This data enables the software life cycle processes, system or equipment certification, and post-certification modification of the software product.",
    "This section discusses the characteristics, form, configuration management controls, and content of the software life cycle data. \n\n## :  Software Life Cycle Data Should Be: A. Characteristics",
    "Unambiguous\n1. \n:  Information is unambiguous if it is written in terms which only \nallow a single interpretation, aided, if necessary, by a definition. \nComplete\n2. \n:  Information is complete when it includes necessary and relevant \nrequirements and/or descriptive material; responses are defined for the range of valid input data; figures used are labeled; and terms and units of measure are defined. \nVerifiable\n3. \n:  Information is verifiable if it can be checked for correctness by a",
    "Verifiable\n3. \n:  Information is verifiable if it can be checked for correctness by a \nperson or tool. \nConsistent\n:  Information is consistent if there are no conflicts within it. \n4. \nModifiable\n5. \n:  Information is modifiable if it is structured and has a style such that \nchanges can be made completely, consistently, and correctly while retaining the structure. \nTraceable\n6. \n:  Information is traceable if the origin of its components can be \ndetermined. \nb.",
    "Traceable\n6. \n:  Information is traceable if the origin of its components can be \ndetermined. \nb. \nForm:  The form of the software life cycle data should provide for the efficient \nretrieval and review of software life cycle data throughout the service life of the airborne system or equipment. The data and the specific form of the data should be specified in the Plan for Software Aspects of Certification. \nNote 1: The software life cycle data may be held in a number of forms (for example,",
    "Note 1: The software life cycle data may be held in a number of forms (for example, \nheld electronically or in printed form). \nNote 2: The applicant may package software life cycle data items in any manner the \napplicant finds convenient (for example, as individual data items or as a \ncombined data item). \nNote 3: The Plan for Software Aspects of Certification and the Software \nAccomplishment Summary may be required as separate documents by some certification authorities. \nNote 4",
    "Note 4\n: The term \"data\" refers to evidence and other information and does not \nimply the format such data should take. \nc. \nConfiguration management controls:  Software life cycle data can be placed in one of \ntwo categories associated with the software configuration management controls applied: CC1 and CC2 (see 7.3). The minimum control category assigned to each data item, and its variation by software level is specified in the tables of Annex A. If",
    "additional data items than those described herein are produced as evidence to aid the certification process, they should be, as a minimum, under CC2 controls.",
    "d. Content\n:  The software life cycle data descriptions provided in the following sections \nidentify the data that is generally produced by the software life cycle. The descriptions are not intended to describe all data that may be necessary to develop a",
    "software product, and are not intended to imply a particular data packaging method or organization of the data within a package. The descriptions of the content of software life cycle data items provided herein are not all encompassing and should be read in \nconjunction with the body of this document and adapted to the needs of the applicant.",
    "## 11.1 Plan For Software Aspects Of Certification\n\nThe Plan for Software Aspects of Certification (PSAC) is the primary means used by the certification authority for determining whether an applicant is proposing a software life cycle that is commensurate with the rigor required for the level of software being developed. This plan should include:",
    "a. System overview\n:  This section provides an overview of the system, including a \ndescription of its functions and their allocation to the hardware and software, the architecture, processor(s) used, hardware/software interfaces, and safety features. \nSoftware overview\nb. \n:  This section briefly describes the software functions with \nemphasis on the proposed safety and partitioning concepts. Examples include",
    "emphasis on the proposed safety and partitioning concepts. Examples include \nresource sharing, redundancy, fault tolerance, mitigation of single event upset, and timing and scheduling strategies. \nCertification considerations\nc. \n:  This section provides a summary of the certification",
    "Certification considerations\nc. \n:  This section provides a summary of the certification \nbasis, including the means of compliance, as relating to the software aspects of certification. This section also states the proposed software level(s) and summarizes the justification provided by the system safety assessment process, including potential software contributions to failure conditions. \nSoftware life cycle\nd. \n:  This section defines the software life cycle to be used and",
    "Software life cycle\nd. \n:  This section defines the software life cycle to be used and \nincludes a summary of each of the software life cycle processes for which detailed information is defined in their respective software plans. The summary explains how the objectives of each software life cycle process will be satisfied, and specifies the organizations to be involved, the organizational responsibilities, and the system life cycle processes and certification liaison process responsibilities.",
    "Software life cycle data\ne. \n:  This section specifies the software life cycle data that will \nbe produced and controlled by the software life cycle processes. This section also describes the relationship of the data to each other or to other data defining the system, the software life cycle data to be submitted to the certification authority, the \nform of the data, and the means by which the data will be made available to the certification authority. \nSchedule\nf.",
    "Schedule\nf. \n:  This section describes the means the applicant will use to provide the \ncertification authority with visibility of the activities of the software life cycle processes so reviews can be planned. \ng. \nAdditional considerations:  This section describes specific considerations that may \naffect the certification process. Examples include alternative methods of compliance, tool qualification, previously developed software, option-selectable software, user-",
    "modifiable software, deactivated code, COTS software, field-loadable software, parameter data items, multiple-version dissimilar software, and product service history.",
    "h. Supplier oversight\n:  This section describes the means of ensuring that supplier \nprocesses and outputs will comply with approved software plans and standards. \n\n## 11.2 Software Development Plan\n\nThe Software Development Plan (SDP) is a description of the software development procedures and software life cycle(s) to be used to satisfy the software development process objectives. It may be included in the Plan for Software Aspects of Certification. This plan should include:",
    "a. Standards\n:  Identification of the Software Requirements Standards, Software Design \nStandards, and Software Code Standards for the project. Also, references to the standards for previously developed software, including COTS software, if those standards are different. \nSoftware life cycle\nb. \n:  A description of the software life cycle processes to be used to",
    "Software life cycle\nb. \n:  A description of the software life cycle processes to be used to \nform the specific software life cycle(s) to be used on the project, including the transition criteria for the software development processes. This description is distinct from the summary provided in the Plan for Software Aspects of Certification, in that it provides the detail necessary to ensure proper implementation of the software life cycle processes. \nSoftware development environment\nc.",
    "Software development environment\nc. \n:  A statement of the chosen software \ndevelopment environment in terms of hardware and software, including: \n1. The requirements development method(s) and tools to be used. 2. The design method(s) and tools to be used. \n3. The coding method(s), programming language(s), coding tool(s) to be used, and \nwhen applicable, options and constraints of autocode generators.",
    "when applicable, options and constraints of autocode generators. \n4. The compilers, linkage editors, and loaders to be used. 5. The hardware platforms for the tools to be used.",
    "## 11.3 Software Verification Plan\n\nThe Software Verification Plan (SVP) is a description of the verification procedures to be used to satisfy the software verification process objectives. These procedures may vary by software level as defined in the tables of Annex A. This plan should include:",
    "a. Organization\n:  Organizational responsibilities within the software verification process \nand interfaces with the other software life cycle processes. \nIndependence\nb. \n:  A description of the methods for establishing verification \nindependence, when required. \nc. \nVerification methods:  A description of the verification methods to be used for each \nactivity of the software verification process: \n1. Review methods, including checklists or other aids.",
    "1. Review methods, including checklists or other aids. \n2. Analysis methods, including traceability and coverage analysis. 3. Testing methods, including the method for selecting test cases, the test \nprocedures to be used, and the test data to be produced. \nd. Verification environment\n:  A description of the equipment for testing, the testing and \nanalysis tools, and how to apply these tools and hardware test equipment. Section",
    "analysis tools, and how to apply these tools and hardware test equipment. Section \n4.4.3b provides guidance on indicating target computer and simulator or emulator differences. \nTransition criteria\ne. \n:  The transition criteria for entering the software verification \nprocess. \nPartitioning considerations\nf. \n:  If partitioning is used, the methods used to verify the \nintegrity of the partitioning. \nCompiler assumptions\ng. \n:  A description of the assumptions made by the applicant",
    "Compiler assumptions\ng. \n:  A description of the assumptions made by the applicant \nabout the correctness of the compiler, linkage editor, or loader (see 4.4.2). \nReverification method\nh. \n:  For software modification, a description of the methods for \nidentifying, analyzing, and verifying the affected areas of the software and the changed parts of the Executable Object Code. \nPreviously developed software\ni. \n:  For previously developed software, if the initial",
    "Previously developed software\ni. \n:  For previously developed software, if the initial \ncompliance baseline for the verification process does not comply with this document, a description of the methods to satisfy the objectives of this document. \nMultiple-version dissimilar software\nj. \n:  If multiple-version dissimilar software is used, \na description of the software verification process activities (see 12.3.2).",
    "## 11.4 Software Configuration Management Plan\n\nThe Software Configuration Management Plan establishes the methods to be used to achieve the objectives of the SCM process throughout the software life cycle. This plan should include:",
    "a. Environment\n:  A description of the SCM environment to be used, including \nprocedures, tools, methods, standards, organizational responsibilities, and interfaces. \nActivities\n:  A description of the SCM process activities in the software life cycle: \nb. \nConfiguration identification\n1. \n:  Items to be identified, when they will be identified, \nthe identification methods for software life cycle data (for example, part",
    "the identification methods for software life cycle data (for example, part \nnumbering), and the relationship of software identification and the system or equipment identification. \nBaselines and traceability\n2. \n:  The means of establishing baselines, what baselines \nwill be established, when these baselines will be established, the software library controls, and the configuration item and baseline traceability. \n3. \nProblem reporting:  The content and identification of Problem Reports for the",
    "3. \nProblem reporting:  The content and identification of Problem Reports for the \nsoftware product and software life cycle processes, when they will be written, the \nmethod of closing Problem Reports, and the relationship to the change control activity.",
    "4. Change control\n:  Configuration items and baselines to be controlled, when they \nwill be controlled, the problem/change control activities that control them, precertification controls, post-certification controls, and the means of preserving the \nintegrity of baselines and configuration items. \nChange review\n5. \n:  The method of handling feedback from and to the software life",
    "Change review\n5. \n:  The method of handling feedback from and to the software life \ncycle processes; the methods of assessing and prioritizing problems, approving changes, and handling their resolution or change implementation; and the relationship of these methods to the problem reporting and change control activities. \nConfiguration status accounting\n6. \n:  The data to be recorded to enable reporting",
    "Configuration status accounting\n6. \n:  The data to be recorded to enable reporting \nconfiguration management status, definition of where that data will be kept, how it will be retrieved for reporting, and when it will be available. \nArchive, retrieval, and release\n7. \n:  The integrity controls, the release method and \nauthority, and data retention. \nSoftware load control\n8. \n:  A description of the software load control safeguards and \nrecords. \nSoftware life cycle environment controls\n9.",
    "records. \nSoftware life cycle environment controls\n9. \n:  Controls for the tools used to develop, \nbuild, verify, and load the software, addressing sections 11.4.b.1 through \n11.4.b.7. This includes control of tools to be qualified. \nSoftware life cycle data controls\n:  Controls associated with CC1 and CC2 data. \n10. \nTransition criteria\n:  The transition criteria for entering the SCM process. \nc. \nSCM data\nd. \n:  A definition of the software life cycle data produced by the SCM process,",
    "c. \nSCM data\nd. \n:  A definition of the software life cycle data produced by the SCM process, \nincluding SCM Records, the Software Configuration Index, and the Software Life Cycle Environment Configuration Index. \nSupplier control\n:  The means of applying SCM process requirements to suppliers. \ne.",
    "## 11.5 Software Quality Assurance Plan\n\nThe Software Quality Assurance Plan establishes the methods to be used to achieve the objectives of the SQA process. The SQA Plan may include descriptions of process improvement, metrics, and progressive management methods. This plan should include:",
    "a. Environment\n:  A description of the SQA environment, including scope, \norganizational responsibilities and interfaces, standards, procedures, tools, and methods. \nAuthority\nb. \n:  A statement of the SQA authority, responsibility, and independence, \nincluding the SQA approval of software products. \nc. \nActivities:  The SQA activities that are to be performed for each software life cycle \nprocess and throughout the software life cycle including:",
    "process and throughout the software life cycle including: \n1. SQA methods, for example, reviews, audits, reporting, inspections, and \nmonitoring of the software life cycle processes. \n2. Activities related to the problem reporting, tracking, and corrective action \nsystem. \n3. A description of the software conformity review activity. \n:  The transition criteria for entering the SQA process. \nd. Transition criteria\nTiming\ne.",
    ":  The transition criteria for entering the SQA process. \nd. Transition criteria\nTiming\ne. \n:  The timing of the SQA process activities in relation to the activities of the \nsoftware life cycle processes. \nSQA Records\n:  A definition of the records to be produced by the SQA process. \nf. \nSupplier oversight\ng. \n:  A description of the means of ensuring that suppliers' processes \nand outputs will comply with the plans and standards.",
    "## 11.6 Software Requirements Standards\n\nSoftware Requirements Standards define the methods, rules, and tools to be used to develop the high-level requirements. These standards should include:",
    "a. The methods to be used for developing software requirements, such as structured \nmethods. \nb. Notations to be used to express requirements, such as data flow diagrams and formal \nspecification languages. \nc. Constraints on the use of the tools used for requirements development. \nd. The method to be used to provide derived requirements to the system processes. \n\n## 11.7 Software Design Standards",
    "## 11.7 Software Design Standards\n\nSoftware Design Standards define the methods, rules, and tools to be used to develop the software architecture and low-level requirements. These standards should include:",
    "a. Design description method(s) to be used. \nb. Naming conventions to be used. c. Conditions imposed on permitted design methods, for example, scheduling, and the \nuse of interrupts and event-driven architectures, dynamic tasking, re-entry, global \ndata, and exception handling, and rationale for their use. \nd. Constraints on the use of the design tools. \ne. Constraints on design, for example, exclusion of recursion, dynamic objects, data \naliases, and compacted expressions.",
    "aliases, and compacted expressions. \nf. Complexity restrictions, for example, maximum level of nested calls or conditional \nstructures, use of unconditional branches, and number of entry/exit points of code components.",
    "## 11.8 Software Code Standards\n\nSoftware Code Standards define the programming languages, methods, rules, and tools to be used to code the software. These standards should include:",
    "a. Programming language(s) to be used and/or defined subset(s). For a programming \nlanguage, reference the data that unambiguously defines the syntax, the control behavior, the data behavior, and side-effects of the language. This may require \nlimiting the use of some features of a language. \nb. Source Code presentation standards, for example, line length restriction, indentation,",
    "b. Source Code presentation standards, for example, line length restriction, indentation, \nand blank line usage and Source Code documentation standards, for example, name of author, revision history, inputs and outputs, and affected global data. \nc. Naming conventions for components, subprograms, variables, and constants. \nd. Conditions and constraints imposed on permitted coding conventions, such as the",
    "d. Conditions and constraints imposed on permitted coding conventions, such as the \ndegree of coupling between software components and the complexity of logical or numerical expressions and rationale for their use. \ne. Constraints on the use of the coding tools.",
    "## 11.9 Software Requirements Data\n\nSoftware Requirements Data is a definition of the high-level requirements including the derived requirements. This data should include:",
    "a. Description of the allocation of system requirements to software, with attention to \nsafety-related requirements and potential failure conditions. \nb. Functional and operational requirements under each mode of operation. \nc. Performance criteria, for example, precision and accuracy. \nd. Timing requirements and constraints. e. Memory size constraints. \nf. Hardware and software interfaces, for example, protocols, formats, frequency of \ninputs, and frequency of outputs.",
    "inputs, and frequency of outputs. \ng. Failure detection and safety monitoring requirements. \nh. Partitioning requirements allocated to software, how the partitioned software \ncomponents interact with each other, and the software level(s) of each partition.",
    "## 11.10 Design Description\n\nThe Design Description is a definition of the software architecture and the low-level requirements that will satisfy the high-level requirements. This data should include:",
    "a. A detailed description of how the software satisfies the specified high-level \nrequirements, including algorithms, data structures, and how software requirements are allocated to processors and tasks. \nb. The description of the software architecture defining the software structure to \nimplement the requirements. \nc. The input/output description, for example, a data dictionary, both internally and \nexternally throughout the software architecture.",
    "externally throughout the software architecture. \nd. The data flow and control flow of the design. e. Resource limitations, the strategy for managing each resource and its limitations, the \nmargins, and the method for measuring those margins, for example, timing and \nmemory. \nf. Scheduling procedures and inter-processor/inter-task communication mechanisms, \nincluding time-rigid sequencing, preemptive scheduling, Ada rendezvous, and \ninterrupts.",
    "including time-rigid sequencing, preemptive scheduling, Ada rendezvous, and \ninterrupts. \ng. Design methods and details for their implementation, for example, software loading, \nuser-modifiable software, or multiple-version dissimilar software. \nh. Partitioning methods and means of preventing partition breaches. \ni. \nDescriptions of the software components, whether they are new or previously developed, and, if previously developed, reference to the baseline from which they were taken. \nj.",
    "j. \nDerived requirements resulting from the software design process. \nk. If the system contains deactivated code, a description of the means to ensure that the \ncode cannot be enabled in the target computer. \nl. \nRationale for those design decisions that are traceable to safety-related system \nrequirements.",
    "## 11.11 Source Code\n\nThis data consists of code written in source language(s). The Source Code is used with the compiling, linking, and loading data in the integration process to develop the integrated system or equipment. For each Source Code component, this data should include the software identification, including the name and date of revision and/or version, as applicable. \n\n## 11.12 Executable Object Code",
    "## 11.12 Executable Object Code\n\nThe Executable Object Code consists of a form of code that is directly usable by the processing unit of the target computer and is, therefore, the software that is loaded into the hardware or system. \n\n## 11.13 Software Verification Cases And Procedures\n\nSoftware Verification Cases and Procedures detail how the software verification process activities are implemented. This data should include descriptions of the:",
    "a. Review and analysis procedures:  The scope and depth of the review or analysis \nmethods to be used, in addition to the description in the Software Verification Plan. \nb. Test cases\n:  The purpose of each test case, set of inputs, conditions, expected results \nto achieve the required coverage criteria, and the pass/fail criteria. \nTest procedures\nc. \n:  The step-by-step instructions for how each test case is to be set up",
    "Test procedures\nc. \n:  The step-by-step instructions for how each test case is to be set up \nand executed, how the test results are evaluated, and the test environment to be used.",
    "## 11.14 Software Verification Results\n\nThe Software Verification Results are produced by the software verification process activities. Software Verification Results should:",
    "a. For each review, analysis, and test, indicate each procedure that passed or failed \nduring the activities and the final pass/fail results. \nb. Identify the configuration item or software version reviewed, analyzed, or tested. c. Include the results of tests, reviews, and analyses, including coverage analyses and \ntraceability analyses.",
    "traceability analyses. \nAny discrepancies found should be recorded and tracked via problem reporting. Additionally, evidence provided in support of the system processes' assessment of information provided by the software processes (see 2.2.1.f and 2.2.1.g) should be considered to be Software Verification Results.",
    "## 11.15 Software Life Cycle Environment Configuration Index\n\nThe Software Life Cycle Environment Configuration Index (SECI) identifies the configuration of the software life cycle environment. This index is written to aid reproduction of the hardware and software life cycle environment for software regeneration, reverification, or software modification, and should:",
    "a. Identify the software life cycle environment hardware and its operating system \nsoftware. \nb. Identify the tools to be used during the development of the software. Examples \ninclude compilers, linkage editors, loaders, data integrity tools such as tools that calculate and embed checksums or cyclical redundancy checks, and any autocode generator with its associated options. \nc. Identify the test environment used to verify the software product, for example, the",
    "c. Identify the test environment used to verify the software product, for example, the \nsoftware testing and analysis tools. \nd. Identify qualified tools and their associated tool qualification data. \n: This data may be included in the Software Configuration Index.",
    "Note\n\n## 11.16 Software Configuration Index\n\nThe Software Configuration Index (SCI) identifies the configuration of the software product. Specific configuration identifiers and version identifiers should be provided. \n\nNote: The SCI can contain one data item or a set (hierarchy) of data items. The SCI can \ncontain the items listed below or it may reference another SCI or other configuration identified data that specifies the individual items and their versions. \nThe SCI should identify:",
    "a. The software product. \nb. Executable Object Code and Parameter Data Item Files, if any. \nc. Each Source Code component. \nd. Previously developed software in the software product, if used. e. Software life cycle data. \nf. Archive and release media. \ng. Instructions for building the Executable Object Code and Parameter Data Item Files,",
    "g. Instructions for building the Executable Object Code and Parameter Data Item Files, \nif any, including, for example, instructions and data for compiling and linking; and the procedures used to recover the software for regeneration, testing, or modification. \nh. Reference to the Software Life Cycle Environment Configuration Index (see 11.15), \nif it is packaged separately. \ni. \nData integrity checks for the Executable Object Code, if used. \nj.",
    "i. \nData integrity checks for the Executable Object Code, if used. \nj. \nProcedures, methods, and tools for making modifications to the user-modifiable \nsoftware, if any. \nk. Procedures and methods for loading the software into the target hardware. \nNote\n: The SCI may be produced for one software product version or it may be extended \nto contain data for several alternative or successive software product versions.",
    "## 11.17 Problem Reports\n\nProblem Reports are a means to identify and record the resolution to software product anomalous behavior, process non-compliance with software plans and standards, and deficiencies in software life cycle data. Problem Reports should include:",
    "a. Identification of the configuration item and/or the software life cycle process activity \nin which the problem was observed. \nb. Identification of the configuration item(s) to be modified or a description of the \nprocess to be changed. \nc. A problem description that enables the problem to be understood and resolved. The \nproblem description should contain sufficient detail to facilitate the assessment of the potential safety or functional effects of the problem.",
    "d. A description of the corrective action taken to resolve the reported problem.",
    "## 11.18 Software Configuration Management Records\n\nThe results of the SCM process activities are recorded in SCM Records. Examples include configuration identification lists, baseline or software library records, change history reports, archive records, and release records. These examples do not imply that records of these specific types need to be produced. \n\nNote\n: Due to the integral nature of the SCM process, its outputs will often be included \nas parts of other software life cycle data.",
    "## 11.19 Software Quality Assurance Records\n\nThe results of the SQA process activities are recorded in SQA Records. These may include SQA review or audit reports, meeting minutes, records of authorized process deviations, or software conformity review records. \n\n## 11.20 Software Accomplishment Summary\n\nThe Software Accomplishment Summary is the primary data item for showing compliance with the Plan for Software Aspects of Certification. This summary should include:",
    "a. System overview\n:  This section provides an overview of the system, including a \ndescription of its functions and their allocation to hardware and software, the architecture, the processor(s) used, the hardware/software interfaces, and safety features. This section also describes any differences from the system overview in the Plan for Software Aspects of Certification. \nSoftware overview\nb. \n:  This section briefly describes the software functions with",
    "Software overview\nb. \n:  This section briefly describes the software functions with \nemphasis on the safety and partitioning concepts used, and explains differences from the software overview proposed in the Plan for Software Aspects of Certification. \nCertification considerations\nc. \n:  This section restates the certification considerations \ndescribed in the Plan for Software Aspects of Certification and describes any differences. \nSoftware life cycle\nd.",
    "Software life cycle\nd. \n:  This section summarizes the actual software life cycle(s) and \nexplains differences from the software life cycle and software life cycle processes proposed in the Plan for Software Aspects of Certification. \nSoftware life cycle data\ne. \n:  This section describes any differences from the proposals",
    "Software life cycle data\ne. \n:  This section describes any differences from the proposals \nmade in the Plan for Software Aspects of Certification for the software life cycle data produced, the relationship of the data to each other and to other data defining the system, and the means by which the data was made available to the certification authority. This section explicitly references, by configuration identifiers and version,",
    "the applicable Software Configuration Index and Software Life Cycle Environment Configuration Index. Detailed information regarding configuration identifiers and specific versions of software life cycle data is provided in the Software Configuration Index. \nAdditional considerations\nf. \n:  This section summarizes any specific considerations that",
    "Additional considerations\nf. \n:  This section summarizes any specific considerations that \nmay warrant the attention of the certification authority. It explains any differences from the proposals contained in the Plan for Software Aspects of Certification regarding such considerations. Reference should be made to data items applicable to these matters, such as issue papers or special conditions. \nSupplier oversight\ng. \n:  This section describes how supplier processes and outputs",
    "Supplier oversight\ng. \n:  This section describes how supplier processes and outputs \ncomply with plans and standards. \nh. \nSoftware identification:  This section identifies the software configuration by part \nnumber and version. \ni. \nSoftware characteristics\n:  This section states the Executable Object Code size, timing \nmargins including worst-case execution time, memory margins, resource limitations, \nand the means used for measuring each characteristic. \nChange history\nj.",
    "and the means used for measuring each characteristic. \nChange history\nj. \n:  If applicable, this section includes a summary of software changes \nwith attention to changes made due to failures affecting safety, and identifies any \nchanges from and improvements to the software life cycle processes since the previous certification. \nSoftware status\nk. \n:  This section contains a summary of Problem Reports unresolved at",
    "Software status\nk. \n:  This section contains a summary of Problem Reports unresolved at \nthe time of certification. The Problem Report summary includes a description of each problem and any associated errors, functional limitations, operational restrictions, potential adverse effect(s) on safety together with a justification for allowing the Problem Report to remain open, and details of any mitigating action that has been or needs to be carried out. \nCompliance statement\nl.",
    "Compliance statement\nl. \n:  This section includes a statement of compliance with this \ndocument and a summary of the methods used to demonstrate compliance with criteria specified in the software plans. This section also addresses additional rulings made by the certification authority and any deviations from the software plans, standards, and this document not covered elsewhere in the Software Accomplishment Summary.",
    "## 11.21 Trace Data\n\nTrace Data establishes the associations between life cycle data items contents. Trace Data should be provided that demonstrates bi-directional associations between: \n\na. System requirements allocated to software and high-level requirements. b. High-level requirements and low-level requirements. \nc. Low-level requirements and Source Code. \nd. Software Requirements and test cases. \ne. Test cases and test procedures. \nf. Test procedures and test results.",
    "## 11.22 Parameter Data Item File\n\nThe Parameter Data Item File consists of a form of data that is directly usable by the processing unit of the target computer. \n\nSoftware life cycle data should be produced for each instantiation of a Parameter Data Item. If packaged separately, this data should include a reference to the Software Accomplishment Summary of the associated Executable Object Code. \n\n## 12.0 Additional Considerations",
    "The previous sections of this document provide guidance for satisfying certification requirements in which the applicant submits evidence of the software life cycle processes as described in those sections. This section provides guidance on additional considerations where objectives and/or activities may replace, modify, or add to some or all of the objectives and/or activities defined in the rest of this document. The use of additional considerations and the proposed impact on the guidance",
    "rest of this document. The use of additional considerations and the proposed impact on the guidance provided in the other sections of this document should be agreed on a case-by-case basis with the certification authorities.",
    "## 12.1 Use Of Previously Developed Software",
    "The guidance of this section discusses the issues associated with the use of previously developed software, including the assessment of modifications; the effect of changing an aircraft installation, application environment, or development environment; upgrading a development baseline; and SCM and SQA considerations. The intention to use previously developed software is stated in the Plan for Software Aspects of Certification. Unresolved Problem Reports associated with the previously developed",
    "Aspects of Certification. Unresolved Problem Reports associated with the previously developed software should be evaluated for impact.",
    "## 12.1.1 Modifications To Previously Developed Software\n\nThis guidance discusses modifications to previously developed software where the outputs of the previous software life cycle processes comply with this document. \n\nModification may result from requirement changes, the detection of errors, and/or software enhancements. Activities include:",
    "a. The revised outputs of the system safety assessment process should be reviewed \nconsidering the proposed modifications. \nb. If the software level is revised, the guidance of section 12.1.4 should be considered. c. Both the impact of the software requirements changes and the impact of software \narchitecture changes should be analyzed, including the consequences of software requirement changes upon other requirements and the coupling between several",
    "software components that may result in reverification effort involving more than the modified area. \nd. The area affected by a change should be determined. This may be done by data flow \nanalysis, control flow analysis, timing analysis, traceability analysis, or a combination of these analyses. \ne. Areas affected by the change should be reverified in accordance with section 6.",
    "## 12.1.2 Change Of Aircraft Installation\n\nAirborne systems or equipment containing software that has been previously \"approved\" \nat a certain software level and under a specific certification basis may be used in a new aircraft installation. Activities include:",
    "a. The system safety assessment process assesses the new aircraft installation and \ndetermines the software level and the certification basis. No additional effort will be required if these are the same for the new installation as they were in the previous installation. \nb. If functional modifications are required for the new installation, the guidance of \nsection 12.1.1 should be satisfied. \nc. If the previous development activity did not produce outputs required to substantiate",
    "c. If the previous development activity did not produce outputs required to substantiate \nthe system safety objectives of the new installation, the guidance of section 12.1.4 \nshould be satisfied.",
    "## 12.1.3 Change Of Application Or Development Environment\n\nUse and modification of previously developed software may involve a new development environment, a new target processor or other hardware, or integration with other software than that used for the original application.",
    "New development environments may increase or reduce some activities within the software life cycle. New application environments may require activities in addition to software life cycle process activities that address modifications. Changes to an application or development environment should be identified, analyzed, and reverified. Activities include:",
    "a. If a new development environment uses software tools, the guidance of section 12.2 \nmay be applicable. \nb. The rigor of the evaluation of an application change should consider the complexity",
    "b. The rigor of the evaluation of an application change should consider the complexity \nand sophistication of the programming language. For example, the rigor of the evaluation for Ada generics will be greater if the generic parameters are different in the new application. For object-oriented languages, the rigor will be greater if the objects that are inherited are different in the new application. \nc. Using a different autocode generator or a different set of autocode generator options",
    "c. Using a different autocode generator or a different set of autocode generator options \nmay change the Source Code or object code generated. The impact of any changes \nshould be analyzed. \nd. If a different compiler or different set of compiler options are used, resulting in \ndifferent object code, the results from a previous software verification process",
    "different object code, the results from a previous software verification process \nactivity using the object code may not be valid and should not be used for the new application. In this case, previous test results may no longer be valid for the structural coverage criteria of the new application. Similarly, compiler assumptions about optimization may not be valid. \ne. If a different processor is used, then a change impact analysis is performed to \ndetermine:",
    "e. If a different processor is used, then a change impact analysis is performed to \ndetermine: \n1. Software components that are new or will need to be modified as a result of \nchanging the processor, including any modification for hardware/software integration. \n2. The results from a previous software verification process activity directed at the \nhardware/software interface that may be used for the new application.",
    "hardware/software interface that may be used for the new application. \n3. Previous hardware/software integration tests that should be executed for the new \napplication. It is expected that there will always be a minimal set of tests to be \nrun. \n4. Additional hardware/software integration tests and reviews that may be \nnecessary. \nf. If a hardware item, other than the processor, is changed and the design of the \nsoftware isolates the interfacing modules from other modules then a change impact",
    "software isolates the interfacing modules from other modules then a change impact \nanalysis should be performed to: \n1. Determine the software modules or interfaces that are new or will be modified to \naccommodate the changed hardware component. \n2. Determine the extent of reverification required. \ng. Verification of software interfaces should be conducted where previously developed",
    "g. Verification of software interfaces should be conducted where previously developed \nsoftware is used with different interfacing software. A change impact analysis may be used to determine the extent of reverification required.",
    "## 12.1.4 Upgrading A Development Baseline\n\nGuidance follows for software whose software life cycle data from a previous application are determined to be inadequate or do not satisfy the objectives of this document, due to the requirements associated with a new application. This guidance is intended to aid in satisfying the objectives of this document when applied to:",
    "- \nCOTS software. \n- \nAirborne software developed to other guidance. \n- \nAirborne software developed prior to the existence of this document. \n- \nSoftware previously developed to this document at a lower software level. \nActivities for upgrading a development baseline include:",
    "a. The objectives of this document should be satisfied while taking advantage of \nsoftware life cycle data of the previous development that satisfy the objectives for the \nnew application. \nb. Software aspects of certification should be based on the failure conditions and \nsoftware level(s) as determined by the system safety assessment process. Comparison to failure conditions of the previous application will determine areas that may need to \nbe upgraded.",
    "be upgraded. \nc. Software life cycle data from a previous development should be evaluated to ensure \nthat the software verification process objectives of the software level are satisfied for the new application to the necessary level of rigor and independence. \nd. Reverse engineering may be used to regenerate software life cycle data that is",
    "d. Reverse engineering may be used to regenerate software life cycle data that is \ninadequate or missing in satisfying the objectives of this document. In addition to producing the software product, additional activities may need to be performed to \nsatisfy the software verification process objectives. \ne. If use of product service history is planned to satisfy the objectives of this document \nin upgrading a development baseline, section 12.3.4 should be considered.",
    "in upgrading a development baseline, section 12.3.4 should be considered. \nf. The applicant should specify the strategy for accomplishing compliance with this \ndocument in the Plan for Software Aspects of Certification.",
    "## 12.1.5 Software Configuration Management Considerations\n\nIf previously developed software is used, the software configuration management process activities for the new application should include, in addition to the activities of section 7:",
    "a. Providing traceability from the software product and software life cycle data of the \nprevious application to the new application. \nb. Providing change control that enables problem reporting, problem resolution, and \ntracking of changes to software components used in more than one application. \n\n## 12.1.6 Software Quality Assurance Considerations\n\nIf previously developed software is used, the software quality assurance activities should include, in addition to the activities of section 8:",
    "a. Providing assurance that the software components satisfy or exceed the software life \ncycle criteria of the software level for the new application. \nb. Providing assurance that changes to the software life cycle processes are stated in the \nsoftware plans. \n\n## 12.2 Tool Qualification 12.2.1 Determining If Tool Qualification Is Needed",
    "Qualification of a tool is needed when processes of this document are eliminated, reduced, or automated by the use of a software tool without its output being verified as specified in section 6. The purpose of the tool qualification process is to ensure that the tool provides confidence at least equivalent to that of the process(es) eliminated, reduced, or automated. The tool qualification process may be applied to a single tool, a collection of tools, or one or more functions within a tool.",
    "may be applied to a single tool, a collection of tools, or one or more functions within a tool. For a tool with multiple functions, if protection of tool functions can be demonstrated, only those functions that are used to eliminate, reduce or automate software life cycle processes, and whose outputs are not verified, need be qualified. Protection is the use of a mechanism to ensure that a tool function cannot adversely impact another tool function. A tool is qualified only for use on a",
    "tool function cannot adversely impact another tool function. A tool is qualified only for use on a specific system where the intention to use the tool is stated in the Plan for Software Aspects of Certification that supports the system. If a tool previously qualified on one system is proposed for use on another system, it should be requalified within the context of that other system.",
    "## 12.2.2 Determining The Tool Qualification Level\n\nIf tool qualification is needed, the impact of the tool use in the software life cycle processes should be assessed in order to determine its tool qualification level (TQL). The following criteria should be used to determine the impact of the tool:",
    "a. Criteria 1\n: A tool whose output is part of the airborne software and thus could insert \nan error. \nCriteria 2\nb. \n: A tool that automates verification process(es) and thus could fail to detect \nan error, and whose output is used to justify the elimination or reduction of: \n1. Verification process(es) other than that automated by the tool, or \n2. Development process(es) that could have an impact on the airborne software. \nc. Criteria 3",
    "2. Development process(es) that could have an impact on the airborne software. \nc. Criteria 3\n: A tool that, within the scope of its intended use, could fail to detect an \nerror.",
    "If the tool eliminates, reduces, or automates processes in this document and its output is not verified as specified in section 6, the appropriate TQL is as shown in Table 12-1. Five levels of tool qualification, TQL-1 to TQL-5, are identified based on the tool use and its potential impact in the software life cycle processes. TQL-1 is the most rigorous level and TQL-5 is the least rigorous level. When assessing the impact of a given tool, the criteria should be considered sequentially from",
    "When assessing the impact of a given tool, the criteria should be considered sequentially from criteria 1 to criteria 3. The tool qualification level should be coordinated with the certification authority as early as possible.",
    "Criteria \nSoftware Level \n1 \n2 \n3 \nA \nTQL-1 \nTQL-4 \nTQL-5 \nB \nTQL-2 \nTQL-4 \nTQL-5 \nC \nTQL-3 \nTQL-5 \nTQL-5 \nD \nTQL-4 \nTQL-5 \nTQL-5 \n\n## 12.2.3 Tool Qualification Process\n\nThe objectives, activities, guidance, and life cycle data required for each Tool Qualification Level are described in DO-330, \"Software Tool Qualification Considerations.\" \n\n## 12.3 Alternative Methods",
    "Some methods were not discussed in the previous sections of this document because of inadequate maturity at the time this document was written or limited applicability for airborne software. It is not the intention of this document to restrict the implementation of any current or future methods. Any single alternative method discussed in this section may be used in satisfying one or more of the objectives in this document. In addition, alternative methods may be used to support one another. An",
    "in this document. In addition, alternative methods may be used to support one another. An alternative method cannot be considered in isolation from the suite of software development processes. The effort for obtaining certification credit of an alternative method is dependent on the software level and the impact of the alternative method on the software life cycle processes. Guidance for using an alternative method includes:",
    "a. An alternative method should be shown to satisfy the objectives of this document or \nthe applicable supplement. \nb. The applicant should specify in the Plan for Software Aspects of Certification, and \nobtain agreement from the certification authority for: \n1. The impact of the proposed method on the software development processes. \n2. The impact of the proposed method on the software life cycle data. 3. The rationale for use of the alternative method that shows that the system safety",
    "objectives are satisfied. One technique for presenting the rationale for using an alternative method is an assurance case, in which arguments are explicitly given to link the evidence to the claims of compliance with the system safety \nobjectives. \nc. The rationale should be substantiated by software plans, processes, expected results, \nand evidence of the use of the method.",
    "## 12.3.1 Exhaustive Input Testing\n\nThere are situations where the software component of an airborne system or equipment is simple and isolated such that the set of inputs and outputs can be bounded. If so, it may be possible to demonstrate that exhaustive testing of this input space can be substituted for one or more of the software verification process activities identified in section 6. \n\nFor this alternative method, activities include:",
    "For this alternative method, activities include: \n\na. Defining the complete set of valid inputs and outputs of the software. \nb. Performing an analysis that confirms the isolation of the inputs to the software. \nc. Developing rationale for the exhaustive input test cases and procedures. \nd. Developing the test cases, test procedures, and test results. \n\n## 12.3.2 Considerations For Multiple-Version Dissimilar Software Verification",
    "## 12.3.2 Considerations For Multiple-Version Dissimilar Software Verification\n\nGuidance follows concerning the software verification process as it applies to multipleversion dissimilar software. If the software verification process is modified because of the use of multiple-version dissimilar software, evidence should be provided that the software verification process objectives are satisfied and that equivalent error detection is achieved for each software version.",
    "Multiple, dissimilar versions of the software are produced using combinations of these techniques:",
    "- \nThe Source Code is implemented in two or more different programming languages. \n- \nThe object code is generated using two or more different compilers. \n- \nEach software version of Executable Object Code executes on a separate, dissimilar processor, or on a single processor with the means to provide partitioning between the software versions. \n- \nThe software requirements, software design, and/or Source Code are developed by two or more development teams whose interactions are managed. \n-",
    "- \nThe software requirements, software design, and/or Source Code are developed in two or more software development environments, and/or each version is verified using separate test environments. \n- \nThe Executable Object Code is linked and loaded using two or more different linkage editors and two or more different loaders. \n- \nThe software requirements, software design, and/or Source Code are developed in",
    "- \nThe software requirements, software design, and/or Source Code are developed in \nconformance with two or more different Software Requirements Standards, Software Design Standards, and/or Software Code Standards, respectively.",
    "When multiple versions of software are used, the software verification methods may be modified from those used to verify single version software. They will apply to software development process activities that are multi-thread, such as separate, multiple development teams. The software verification process is dependent on the combined hardware and software architectures since this affects the dissimilarity of the multiple software versions. Additional software verification process objectives to",
    "of the multiple software versions. Additional software verification process objectives to be satisfied are to demonstrate that:",
    "a. Inter-version compatibility requirements are satisfied, including compatibility during \nnormal and abnormal operations and state transitions. \nb. Equivalent error detection is achieved. \nOther changes in software verification process activities may be agreed with the certification authority, if the changes are substantiated by rationale that confirms equivalent software verification coverage. \n\n## 12.3.2.1 Independence Of Multiple-Version Dissimilar Software",
    "## 12.3.2.1 Independence Of Multiple-Version Dissimilar Software\n\nWhen multiple-version dissimilar software versions are developed independently using a managed method, the development processes have the potential to reveal certain classes of errors such that verification of each software version is equivalent to independent verification of the software development processes. Activities include:",
    "a. The applicant should demonstrate that different teams with limited interaction \ndeveloped each software version's software requirements, software design, and Source Code. \nb. Independent test coverage analyses should still be performed as with a single version. \nNote\n: Section 12.3.2.1 only addresses the subject of independence. Reduction of \nsoftware levels is not discussed or intended. \n\n## 12.3.2.2 Multiple Processor-Related Verification",
    "When each version of dissimilar software executes on a different type of processor, the verification of some aspects of compatibility of the code with the processor (see 6.4.3.a) \nmay be replaced by verification to ensure that the multiple types of processor produce the correct outputs. This verification consists of integration tests in which the outputs of the multiple versions are cross-compared in requirements-based test cases. The applicant should complete the following activities:",
    "a. Show that equivalent error detection is achieved. b. Show that each processor was designed by a different developer. \nc. Show that the outputs of the multiple versions are equivalent. \n\n## 12.3.2.3 Multiple-Version Source Code Verification",
    "The guidance for structural coverage analysis (see 6.4.4.2) may be modified for systems or equipment using multiple-version dissimilar software. For structural coverage analysis, the accompanying Level A activity (see 6.4.4.2.b) to evaluate any additional code \n(generated by a compiler, linker, or other means) that is not directly traceable to Source Code statements need not be done provided that the applicant completes the following activities:",
    "a. Show that each version of software is coded using a different programming language. b. Show that each compiler used is from a different developer. \n\n## 12.3.2.4 Tool Qualification For Multiple-Version Dissimilar Software",
    "If multiple-version dissimilar software is used, the tool qualification process may be modified, if evidence is available that the multiple software tools used in the software development process are dissimilar. This depends on the demonstration of equivalent software verification process activity in the development of the multiple software versions using dissimilar software tools. The applicant should complete the following activities:",
    "a. Show that each tool was obtained from a different developer. \nb. Show that each tool has a dissimilar design. \n\n## 12.3.2.5 Multiple Simulators And Verification",
    "If separate, dissimilar simulators are used to verify multiple-version dissimilar software versions, then the approach to tool qualification of the simulators may be modified. This depends on the demonstration of equivalent software verification process activity in the simulation of the multiple software versions using multiple simulators. Unless it can be justified as unnecessary for multiple simulators to be dissimilar, the applicant should complete the following activities:",
    "a. Provide evidence that each simulator was developed by a different team. b. Provide evidence that each simulator has different requirements, a different design, \nand a different programming language. \nc. Provide evidence that each simulator executes on a different processor. \nNote\n: When a multiple processor system using multiple, dissimilar versions of software",
    "Note\n: When a multiple processor system using multiple, dissimilar versions of software \nare executing on identical processors, it may be difficult to demonstrate dissimilarity of simulators because of the reliance on information obtained from a common source, for example, the processor manufacturer.",
    "## 12.3.3 Software Reliability Models\n\nMany methods for predicting software reliability based on developmental metrics have been published, for example, software structure, defect detection rate, etc. This document does not provide guidance for those types of methods, because at the time of writing, currently available methods did not provide results in which confidence can be placed. \n\n## 12.3.4 Product Service History",
    "## 12.3.4 Product Service History\n\nIf equivalent safety for the software can be demonstrated by the use of the software's product service history, some certification credit may be granted. The acceptability of this method is dependent on:",
    "- \nConfiguration management of the software. \n- \nEffectiveness of problem reporting activity. \n- \nStability and maturity of the software. \n- \nRelevance of product service history environment. \n- \nLength of the product service history. \n- \nActual error rates in the product service history. \n- \nImpact of modifications.",
    "Use of service history data for certification credit is predicated upon sufficiency, relevance, and types of problems occurring during the service history period. The use, conditions of use, and results of software service history should be defined, assessed by the system processes, including the system safety assessment process, and submitted to the appropriate certification authority. Guidance for determining applicability of service history and the length of service history needed is",
    "for determining applicability of service history and the length of service history needed is presented below.",
    "## 12.3.4.1 Relevance Of Service History\n\nThe following applies for establishing the relevance of service history:",
    "a. Type of service history\n:  Service history to be used should be defined and agreement \nobtained from the certification authority. Service history data should be provided using a measure relevant to the operations of the system. For example, flight hours is an appropriate measure for software that is used continuously during flight, such as flight control software, and the number of demands is an appropriate measure for software that is executed on demand, such as landing gear software.",
    "Known configuration\nb. \n:  The applicant should show that the software and associated \nevidence used to comply with system safety objectives have been under configuration management throughout the product service history. \nOperating time collection process\nc. \n:  The applicant should show that the means of",
    "Operating time collection process\nc. \n:  The applicant should show that the means of \ncollecting and calculating flight hours for software that is used continuously during flight or number of demands for software that is executed on demand is sufficiently accurate and complete. Flight hours or number of demands should account for changes in any factors that are important to the intended application including but not limited to: \n1. Software and system configuration.",
    "1. Software and system configuration. \n2. Operational mode or state. \n3. Operating environment. \nd. Changes to the software\n:  Configuration changes during the product service history \nshould be identified. An analysis should be conducted to determine whether the changes made to the software alter the applicability of the service history data for the \nperiod preceding the changes. \nUsage and Environment\ne. \n:  The intended software usage should be analyzed to show",
    "Usage and Environment\ne. \n:  The intended software usage should be analyzed to show \nthe relevance of the product service history. \n1. It should be assured that software capabilities to be used are exercised in all \noperational modes. \n2. Analysis should also be performed to assure that relevant permutations of input \ndata are executed. \n3. The operating environment used to collect the service history data should be",
    "3. The operating environment used to collect the service history data should be \nassessed to show relevance to the intended use in the proposed application. If the operating environments of the existing and proposed applications differ, additional verification should confirm compliance with the system safety \nobjectives in the target environment. \n4. If credit is being sought for compatibility with the hardware environment, then",
    "4. If credit is being sought for compatibility with the hardware environment, then \nthe relationship between the service history environment and the intended environment should be addressed. The impact of any hardware modifications \nduring the service history period should also be assessed. \nf. Deactivated code\n:  Analysis should be performed to show that any code that was \ndeactivated during the period of service history is not activated in the new",
    "deactivated during the period of service history is not activated in the new \nenvironment. If it is found that previously deactivated code is activated in the new environment, additional verification should be conducted.",
    "## 12.3.4.2 Sufficiency Of Accumulated Service History\n\nThe required amount of service history is determined by: \n\na. The system safety objectives of the software and the software level. \nb. Any differences in service history environment and system operational environment. \nc. The objectives from sections 4 to 9 being addressed by service history. d. Evidence, in addition to service history, addressing those objectives.",
    "## 12.3.4.3 Collection, Reporting, And Analysis Of Problems Found During Service History",
    "a. Problem reporting process\n:  The applicant should show that problem reporting during \nthe product service history period provides assurance that representative data is available and that problems during the service history period were reported and recorded, and are retrievable. \n1. The specific data to be collected should be agreed on with the certification \nauthority and should include the following for each recorded problem, in addition \nto the items in section 11.17: \ni.",
    "to the items in section 11.17: \ni. \nThe hardware/software configuration in effect when the problem occurred. \nii. The operating environment within which the problem occurred. \niii. The operating mode or state within which the problem occurred. iv. Any application-specific information needed for problem assessment. \nv. Classification of the problem with respect to severity, safety significance,",
    "v. Classification of the problem with respect to severity, safety significance, \nand whether the problem was the result of a change in the software configuration since the start of service history data collection. \nvi. Assessment of whether the problem was: \n- \nReproducible. \n- \nRecoverable. \n- \nRelated to other previously reported problems, including, but not limited to, a common cause. \n2. The chronological trend of Problem Reports should be evaluated and any \nincreasing trend explained.",
    "increasing trend explained. \n3. The completeness of the software's error history should also be addressed. This \nincludes: \ni. \nThe ability to detect faults and maintain a fault log. \nii. \nThe means for operators to report problems. \niii. The completeness of the problem reporting records. \niv. The means for the applicant to determine the system safety impact of any \nopen Problem Reports, and to determine whether problems that were not",
    "open Problem Reports, and to determine whether problems that were not \nsafety-related during the service history will be safety-related in the intended \nenvironment. Problems that were not safety-related in the service experience \nenvironment, but which will be safety-related in the intended environment, \nmight indicate the need for additional verification. \nv. The means for the applicant to determine the number of occurrences of a \nspecific problem. \nb. Process-related problems",
    "specific problem. \nb. Process-related problems\n:  Those problems that are indicative of an inadequate \nprocess, such as design or code errors, should be indicated separately from those \nwhose cause are outside the scope of this document, such as hardware or system requirements errors. \nSafety-related problems\nc. \n:  All in-service problems should be evaluated to determine \nwhich problems were safety-related, and to confirm that all safety-related problems \nhave been corrected.",
    "## 12.3.4.4 Service History Information To Be Included In The Plan For Software Aspects Of Certification\n\nThe following items should be specified in the Plan for Software Aspects of Certification and agreed with the certification authority when seeking certification credit for service history:",
    "a. Rationale for claiming relevant service history, addressing the items in section \n12.3.4.1. \nb. Amount of service history needed together with the rationale. This should include the \nitems in section 12.3.4.2, any censoring rules for data used in estimation, and measured parameters, if applicable. This data should be provided using measures relevant to the operations of the system. \nc. Rationale for calculating the total relevant service history period, including factors",
    "c. Rationale for calculating the total relevant service history period, including factors \nsuch as operational modes, the number of independently operating copies in the installation and in service, and the definition of \"normal operation\" and \"normal operation time.\" \nNote\n: If the error rate is greater than that identified in the plan, these errors should be",
    "Note\n: If the error rate is greater than that identified in the plan, these errors should be \nanalyzed and the analyses reviewed with the certification authority. The length of the service history period may need to be extended or service history may be inapplicable as an alternative means of compliance. \nd. Definition of what was counted as an error and rationale for that definition. This \nshould address the items in section 12.3.4.3a.",
    "should address the items in section 12.3.4.3a. \ne. Proposed acceptable error rates and rationale for the product service history period in \nrelation to the system safety and proposed error rates. This should address the items in sections 12.3.4.3b and 12.3.4.3c. \nf. Definition of criteria for problems that would invalidate service history under section \n12.3.4.3 or for other reasons. \ng. Criteria for errors that will be corrected; how they will be corrected and verified; and",
    "g. Criteria for errors that will be corrected; how they will be corrected and verified; and \nrationale for any defects for which no action will be taken. \nh. Objectives in sections 4 to 9 to be addressed through the use of service history. \n \n \nThis Page Intentionally Left Blank",
    "## Annex A Process Objectives And Outputs By Software Level\n\nReferences in these tables point to those sections in the text that define the particular objectives, related activities, and outputs. \n\nThe tables include guidance for:",
    "a. The process objectives applicable for each software level. For level E software, see \n2.3.3. \nb. The independence by software level of the software life cycle process activities \napplicable to satisfy that process's objectives. \nc. The control category by software level for the software life cycle data produced by \nthe software life cycle process activities (see 7.3).",
    "the software life cycle process activities (see 7.3). \nThese tables should not be used as a checklist. These tables do not reflect all aspects of compliance to this document. In order to fully understand the guidance, the full body of this document should be considered.",
    "The following legend applies to \"Applicability by Software Level\" and \"Control Category by Software Level\" for all tables: \n\nLEGEND: \n \nThe objective should be satisfied with independence. \n \n \nThe objective should be satisfied. \n \nBlank \nSatisfaction of objective is at applicant's discretion. \n \n \nData satisfies the objectives of Control Category 1 (CC1). \n \n \nData satisfies the objectives of Control Category 2 (CC2). \n\n## Table A-1 Software Planning Process",
    "Objective \n \nApplicability by \nSoftware Level \nOutput \nControl Category \nby Software Level \nActivity \n \nDescription \nRef \nRef \nA \nB \nC \nD \nData Item \nRef \nA \nB \nC \nD \n1 \n4.1.a \n    \nThe activities of the software life cycle processes are defined. \n4.2.a 4.2.c 4.2.d 4.2.e 4.2.g 4.2.i 4.2.l 4.3.c \n2 \n4.1.b \n4.2i 4.3.b \n    \nThe software life cycle(s), including the inter-relationships \nbetween the processes, their sequencing, feedback mechanisms, and transition criteria, is defined. \n3",
    "3 \n4.1.c \n    \nSoftware life cycle environment is selected and defined. \n4.4.1 4.4.2.a 4.4.2.b 4.4.2.c 4.4.3 \n    \n4 \nAdditional considerations are addressed. \n4.1.d \n4.2.f 4.2.h 4.2.i 4.2.j 4.2.k \n5 \nSoftware development standards are defined. \n4.1.e \n    \n4.2.b 4.2.g 4.5 \n6 \nSoftware plans comply with this document. \n4.1.f \n4.3.a 4.6 \n    \n7 \n4.1.g \n4.2.g 4.6 \n    \nDevelopment and revision of software plans are coordinated. \n| PSAC                  | 11.1   |     |",
    "| PSAC                  | 11.1   |     |\n|-----------------------|--------|-----|\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n| SDP                   | 11.2   |     |\n|                      |        |     |",
    "| SDP                   | 11.2   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n| SVP                   | 11.3   |     |\n|                      |        |     |\n|                       |        |     |",
    "|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n| SCM Plan              | 11.4   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |",
    "|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n| SQA Plan              | 11.5   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |",
    "|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n| PSAC                  | 11.1   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |",
    "|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |\n| SDP                   | 11.2   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |",
    "|                       |        |     |\n|                       |        |     |\n| SVP                   | 11.3   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |\n| SCM Plan              | 11.4   |     |\n|                      |        |     |",
    "| SCM Plan              | 11.4   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |\n| SQA Plan              | 11.5   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |",
    "|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |\n| PSAC                  | 11.1   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |",
    "|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |\n| SDP                   | 11.2   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |",
    "|                       |        |     |\n|                       |        |     |\n| SVP                   | 11.3   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |\n| SCM Plan              | 11.4   |     |\n|                      |        |     |",
    "| SCM Plan              | 11.4   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |\n| SQA Plan              | 11.5   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |",
    "|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |\n| PSAC                  | 11.1   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |",
    "|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n| SDP                   | 11.2   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |",
    "|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n| SVP                   | 11.3   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |",
    "|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n| SCM Plan              | 11.4   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |",
    "|                      |        |     |\n|                       |        |     |\n| SQA Plan              | 11.5   |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n| SW Requirements       |        |     |",
    "|                       |        |     |\n| SW Requirements       |        |     |\n| Standards             |        |     |\n| 11.6                  |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |\n| SW Design Standards   |        |     |",
    "|                       |        |     |\n| SW Design Standards   |        |     |\n| 11.7                  |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |\n| SW Code Standards     |        |     |\n| 11.8                  |        |     |",
    "| SW Code Standards     |        |     |\n| 11.8                  |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |\n| Software Verification |        |     |\n| Results               |        |     |\n| 11.14                 |        |     |",
    "| Results               |        |     |\n| 11.14                 |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |\n| Software Verification |        |     |\n| Results               |        |     |\n| 11.14                 |        |     |",
    "| Results               |        |     |\n| 11.14                 |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                      |        |     |\n|                       |        |     |\n|                       |        |     |",
    "## Table A-2 Software Development Processes",
    "Objective \n \nApplicability by \nSoftware Level \nOutput \nControl Category \nby Software Level \nActivity \n \nDescription \nRef \nRef \nA \nB \nC \nD \nData Item \nRef \nA \nB \nC \nD \n11.9 \n    \nSoftware Requirements Data \n1 \n5.1.1.a \n    \nHigh-level requirements are developed. \nTrace Data \n11.21 \n    \n5.1.2.a 5.1.2.b 5.1.2.c 5.1.2.d 5.1.2.e 5.1.2.f 5.1.2.g 5.1.2.j 5.5.a \n2 \n11.9 \n    \n5.1.1.b \n5.1.2.h \n5.1.2.i \n    \nSoftware Requirements Data",
    "2 \n11.9 \n    \n5.1.1.b \n5.1.2.h \n5.1.2.i \n    \nSoftware Requirements Data \nDerived highlevel requirements are defined and provided to the system processes, including the system safety assessment process. \n3 \n5.2.1.a \n5.2.2.a 5.2.2.d \n    \nDesign Description \n11.10 \n    \nSoftware architecture is developed. \nDesign Description \n11.10 \n   \n \n4 \n5.2.1.a \n    \nLow-level \nrequirements are developed. \nTrace Data \n11.21 \n   \n \n5.2.2.a 5.2.2.e 5.2.2.f \n5.2.2.g",
    "requirements are developed. \nTrace Data \n11.21 \n   \n \n5.2.2.a 5.2.2.e 5.2.2.f \n5.2.2.g \n5.2.3.a 5.2.3.b 5.2.4.a 5.2.4.b 5.2.4.c 5.5.b \n5 \nDesign Description \n11.10 \n   \n \n5.2.1.b \n5.2.2.b 5.2.2.c \n    \nDerived lowlevel requirements are defined and provided to the system processes, including the \nsystem safety assessment process. \nSource Code \n11.11 \n   \n \n    \n6 \nSource Code is developed. \n5.3.1.a \nTrace Data \n11.21 \n   \n \n5.3.2.a 5.3.2.b 5.3.2.c 5.3.2.d 5.5.c",
    "5.3.1.a \nTrace Data \n11.21 \n   \n \n5.3.2.a 5.3.2.b 5.3.2.c 5.3.2.d 5.5.c \nExecutable Object Code \n11.12 \n    \n7 \n5.4.1.a \n    \n5.4.2.a 5.4.2.b 5.4.2.c 5.4.2.d 5.4.2.e 5.4.2.f \nParameter Data Item File \n11.22 \n    \nExecutable Object Code and Parameter Data Item Files, if any, are produced \nand loaded in \nthe target \ncomputer.",
    "## Verification Of Outputs Of Software Requirements Process",
    "Objective \n \nApplicability by \nSoftware Level \nOutput \nControl Category \nby Software Level \nActivity \n \nDescription \nRef \nRef \nA \nB \nC \nD \nData Item \nRef \nA \nB \nC \nD \n1 \n6.3.1.a \n6.3.1 \n    \nHigh-level requirements comply with system \nrequirements. \n2 \nHigh-level requirements are \naccurate and consistent. \n6.3.1.b \n6.3.1 \n    \n3 \n6.3.1.c \n6.3.1 \n   \n \nHigh-level requirements are compatible with target computer. \n4 \nHigh-level requirements are \nverifiable. \n6.3.1.d \n6.3.1 \n    \n5",
    "4 \nHigh-level requirements are \nverifiable. \n6.3.1.d \n6.3.1 \n    \n5 \nHigh-level requirements \nconform to standards. \n6.3.1.e \n6.3.1 \n    \n6 \n6.3.1.f \n6.3.1 \n    \nHigh-level requirements are \ntraceable to system \nrequirements. \n7 \nAlgorithms are accurate. \n6.3.1.g \n6.3.1 \n    \n| 11.14        |     |\n|--------------|-----|\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |",
    "|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |",
    "| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|              |     |\n| Software     |     |",
    "|             |     |\n|              |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |\n|             |     |\n|              |     |\n|             |     |",
    "| 11.14        |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|              |     |\n| Software     |     |",
    "|             |     |\n|              |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |",
    "## Verification Of Outputs Of Software Design Process",
    "Objective \n \nApplicability by \nSoftware Level \nOutput \nControl Category \nby Software Level \nActivity \n \nDescription \nRef \nRef \nA \nB \nC \nD \nData Item \nRef \nA \nB \nC \nD \n1 \n6.3.2.a \n6.3.2 \n    \nSoftware Verification Results \n11.14    \n \nLow-level requirements comply with high-level requirements. \n2 \n6.3.2.b \n6.3.2 \n    \nSoftware Verification Results \n11.14    \n \nLow-level requirements are accurate and consistent. \n3 \n6.3.2.c \n6.3.2 \n   \n \nSoftware Verification Results \n11.14  ",
    "3 \n6.3.2.c \n6.3.2 \n   \n \nSoftware Verification Results \n11.14   \n \n \nLow-level requirements are compatible with target computer. \n4 \nLow-level requirements are verifiable. \n6.3.2.d \n6.3.2 \n   \n \nSoftware Verification Results \n11.14   \n \n \n5 \nLow-level requirements conform to standards. \n6.3.2.e \n6.3.2 \n    \nSoftware Verification Results \n11.14    \n \n6 \n6.3.2.f \n6.3.2 \n    \nSoftware Verification Results \n11.14   ",
    "11.14    \n \n6 \n6.3.2.f \n6.3.2 \n    \nSoftware Verification Results \n11.14    \n \nLow-level requirements are traceable to highlevel requirements. \n7 \nAlgorithms are accurate. \n6.3.2.g \n6.3.2 \n    \nSoftware Verification Results \n11.14    \n \n8 \nSoftware Verification Results \n11.14    \n \n6.3.3.a \n6.3.3 \n-    \nSoftware architecture is compatible with highlevel requirements. \n9 \nSoftware architecture is consistent. \n6.3.3.b \n6.3.3 \n   \n \nSoftware Verification Results",
    "9 \nSoftware architecture is consistent. \n6.3.3.b \n6.3.3 \n   \n \nSoftware Verification Results \n11.14    \n \n10 \n6.3.3.c \n6.3.3 \n  \n \n \nSoftware Verification Results \n11.14   \n \n \nSoftware architecture is compatible with target computer. \n11 \nSoftware architecture is verifiable. \n6.3.3.d \n6.3.3 \n  \n \n \nSoftware Verification Results \n11.14   \n \n \n12 \nSoftware architecture conforms to standards. \n6.3.3.e \n6.3.3 \n   \n \nSoftware Verification Results \n11.14    \n \n13",
    "6.3.3.e \n6.3.3 \n   \n \nSoftware Verification Results \n11.14    \n \n13 \nSoftware partitioning integrity is confirmed. \n6.3.3.f \n6.3.3 \n    \nSoftware Verification Results \n11.14    ",
    "## Verification Of Outputs Of Software Coding & Integration Processes",
    "Objective \n \nApplicability by \nSoftware Level \nOutput \nControl Category \nby Software Level \nActivity \n \nDescription \nRef \nRef \nA \nB \nC \nD \nData Item \nRef \nA \nB \nC \nD \n1 \n6.3.4.a \n6.3.4 \n    \nSoftware Verification Results \n11.14 \n   \n \nSource Code complies with low-level requirements. \n2 \n6.3.4.b \n6.3.4 \n    \nSoftware Verification Results \n11.14 \n   \n \nSource Code complies with software architecture. \nSoftware Verification Results \n11.14 \n  \n \n \n3 \nSource Code is verifiable.",
    "Software Verification Results \n11.14 \n  \n \n \n3 \nSource Code is verifiable. \n6.3.4.c \n6.3.4 \n   \n \n4 \n6.3.4.d \n6.3.4 \n    \nSoftware Verification Results \n11.14 \n   \n \nSource Code conforms to standards. \n5 \n6.3.4.e \n6.3.4 \n    \nSoftware Verification Results \n11.14 \n   \n \nSource Code is traceable to low-level requirements. \n6 \n6.3.4.f \n6.3.4 \n    \nSoftware Verification Results \n11.14 \n   \n \nSource Code is accurate and consistent. \n7 \n6.3.5.a \n6.3.5 \n  ",
    "11.14 \n   \n \nSource Code is accurate and consistent. \n7 \n6.3.5.a \n6.3.5 \n    \nSoftware Verification Results \n11.14 \n   \n \nOutput of software integration process is complete and correct. \n11.13 \n    \n8 \nSoftware Verification Cases and Procedures \n6.6.a \n6.6 \n \n \n \n \nParameter Data Item File is correct and complete \nSoftware Verification Results \n11.14 \n    \n9 \n6.6.b \n6.6 \n \n \n \n \nSoftware Verification Results \n11.14 \n  ",
    "11.14 \n    \n9 \n6.6.b \n6.6 \n \n \n \n \nSoftware Verification Results \n11.14 \n   \n \nVerification of Parameter Data Item File is achieved.",
    "## Testing Of Outputs Of Integration Process",
    "Objective \n \nApplicability by \nSoftware Level \nOutput \nControl Category \nby Software Level \nActivity \n \nDescription \nRef \nRef \nA \nB \nC \nD \nData Item \nRef \nA \nB \nC \nD \nSoftware Verification Cases and Procedures \n11.13 \n    \n1 \n6.4.a \n    \nSoftware Verification Results \n11.14 \n    \n6.4.2 6.4.2.1 6.4.3 6.5 \nExecutable Object Code complies with high-level requirements. \nTrace Data \n11.21 \n    \nSoftware Verification Cases and Procedures \n11.13 \n    \n2 \n6.4.b \n   ",
    "11.21 \n    \nSoftware Verification Cases and Procedures \n11.13 \n    \n2 \n6.4.b \n    \nSoftware Verification Results \n11.14 \n    \n6.4.2 6.4.2.2 6.4.3 6.5 \nExecutable Object Code is robust with high-level requirements. \nTrace Data \n11.21 \n    \nSoftware Verification Cases and Procedures \n11.13 \n   \n \n3 \n6.4.c \n    \nSoftware Verification Results \n11.14 \n   \n \n6.4.2 6.4.2.1 6.4.3 6.5 \nExecutable Object Code complies with low-level requirements. \nTrace Data \n11.21 \n  ",
    "Executable Object Code complies with low-level requirements. \nTrace Data \n11.21 \n   \n \nSoftware Verification Cases and Procedures \n11.13 \n   \n \n4 \n6.4.d \n    \nSoftware Verification Results \n11.14 \n   \n \n6.4.2 6.4.2.2 6.4.3 6.5 \nExecutable Object Code is robust with low-level requirements. \nTrace Data \n11.21 \n   \n \nSoftware Verification Cases and Procedures \n11.13 \n    \n5 \n6.4.e \n6.4.1.a 6.4.3.a \n    \nSoftware Verification Results \n11.14 \n   ",
    "11.13 \n    \n5 \n6.4.e \n6.4.1.a 6.4.3.a \n    \nSoftware Verification Results \n11.14 \n    \nExecutable Object Code is compatible with target computer.",
    "## Verification Of Verification Process Results",
    "Objective \n \nApplicability by \nSoftware Level \nOutput \nControl Category \nby Software Level \nActivity \n \nDescription \nRef \nRef \nA \nB \nC \nD \nData Item \nRef \nA \nB \nC \nD \n1 \nTest procedures are correct. \n6.4.5.b \n6.4.5 \n \n \n \n \n2 \n6.4.5.c \n6.4.5 \n \n \n \n \nTest results are correct and discrepancies explained. \n3 \n6.4.4.a \n6.4.4.1 \n \n \n \n \nTest coverage of high-level requirements is achieved. \n4 \n6.4.4.b \n6.4.4.1 \n \n \n \n \nTest coverage of low-level requirements is achieved. \n5 \n6.4.4.c",
    "4 \n6.4.4.b \n6.4.4.1 \n \n \n \n \nTest coverage of low-level requirements is achieved. \n5 \n6.4.4.c \n",
    "6.4.4.2.a 6.4.4.2.b 6.4.4.2.d 6.4.4.3 \nTest coverage of software structure (modified condition/decision coverage) is achieved. \n6 \n6.4.4.c \n \n \n \n \nTest coverage of software structure (decision coverage) is achieved. \n6.4.4.2.a 6.4.4.2.b 6.4.4.2.d 6.4.4.3 \n7 \n6.4.4.c \n \n \n \n \n6.4.4.2.a 6.4.4.2.b 6.4.4.2.d 6.4.4.3 \nTest coverage of software structure (statement coverage) is achieved. \n8 \n6.4.4.d \n \n \n \n \n6.4.4.2.c 6.4.4.2.d 6.4.4.3",
    "8 \n6.4.4.d \n \n \n \n \n6.4.4.2.c 6.4.4.2.d 6.4.4.3 \nTest coverage of software structure (data coupling and control coupling) is achieved. \n9 \n6.4.4.c \n6.4.4.2.b \n",
    "Verification of additional code, that cannot be traced to Source Code, is achieved. \n| 11.14        |     |\n|--------------|-----|\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |",
    "|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n| Software     |     |\n| Verification |     |",
    "|             |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |\n|             |     |\n|              |     |\n|              |     |\n| Software     |     |",
    "|             |     |\n|              |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |",
    "|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|             |     |\n|              |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |",
    "| Software     |     |\n| Verification |     |\n| Results      |     |\n| 11.14        |     |\n|             |     |\n|              |     |\n|              |     |\n|              |     |\n| Software     |     |\n| Verification |     |\n| Results      |     |",
    "## Table A-8 Software Configuration Management Process",
    "Objective \n \nApplicability by \nSoftware Level \nOutput \nControl Category \nby Software Level \nActivity \n \nDescription \nRef \nRef \nA \nB \nC \nD \nData Item \nRef \nA \nB \nC \nD \n1 \nConfiguration items are identified. \n7.1.a \n7.2.1 \n \n \n \n \nSCM Records \n11.18     \nSoftware Configuration Index \n11.16     \n2 \nBaselines and traceability are established. \n7.1.b \n7.2.2 \n \n \n \n \nSCM Records \n11.18     \nProblem Reports \n11.17     \n3 \n \n \n \n \nSCM Records \n11.18    ",
    "11.18     \nProblem Reports \n11.17     \n3 \n \n \n \n \nSCM Records \n11.18     \nProblem reporting, change control, change review, and configuration status accounting are established. \n7.1.c 7.1.d 7.1.e 7.1.f \n7.2.3 7.2.4 7.2.5 7.2.6 \n4 \nArchive, retrieval, and release are established. \n7.1.g \n7.2.7 \n \n \n \n \nSCM Records \n11.18     \n5 \nSoftware load control is established. \n7.1.h \n7.4 \n \n \n \n \nSCM Records \n11.18     11.15    ",
    "7.1.h \n7.4 \n \n \n \n \nSCM Records \n11.18     11.15     \nSoftware Life Cycle Environment Configuration Index \n6 \nSoftware life cycle environment control is established. \n7.1.i \n7.5 \n \n \n \n \nSCM Records \n11.18    ",
    "## Software Quality Assurance Process",
    "Objective \n \nApplicability by \nSoftware Level \nOutput \nControl Category \nby Software \nLevel \nActivity \n \nDescription \nRef \nRef \nA \nB \nC \nD \nData Item \nRef \nA \nB \nC \nD \n1 \n8.1.a \n    \nSQA Records \n11.19 \n   \n \n8.2.b 8.2.h 8.2.i \nAssurance is obtained that software plans and standards are developed and reviewed for compliance with this document and for consistency. \n2 \n8.1.b \n    SQA Records \n11.19 \n   ",
    "2 \n8.1.b \n    SQA Records \n11.19 \n    \nAssurance is obtained that software life cycle processes comply with approved software plans. \n8.2.a 8.2.c 8.2.d 8.2.f 8.2.h 8.2.i \n3 \n8.1.b \n    \nSQA Records \n11.19 \n   \n \nAssurance is obtained that software life cycle processes comply with approved software standards. \n8.2.a 8.2.c 8.2.d 8.2.f 8.2.h 8.2.i \n4 \n8.1.c \n    \nSQA Records \n11.19 \n   \n \n8.2.e 8.2.h 8.2.i",
    "4 \n8.1.c \n    \nSQA Records \n11.19 \n   \n \n8.2.e 8.2.h 8.2.i \nAssurance is obtained that transition criteria for the software life cycle processes are satisfied. \n5 \n8.1.d \n    SQA Records \n11.19 \n    \n8.2.g 8.2.h 8.3 \nAssurance is obtained that \nsoftware conformity review is conducted.",
    "## Table A-10 Certification Liaison Process",
    "Objective \n \nApplicability by \nSoftware Level \nOutput \nControl Category \nby Software Level \nActivity \n \nDescription \nRef \nRef \nA \nB \nC \nD \nData Item \nRef \nA \nB \nC \nD \n1 \n9.a \n9.1.b 9.1.c \n    \nPlan for Software Aspects of Certification \n11.1 \n    \nCommunication and understanding between \nthe applicant and the certification authority is established. \n2 \n9.b \n    \nPlan for Software Aspects of Certification \n11.1 \n    \n9.1.a 9.1.b 9.1.c",
    "2 \n9.b \n    \nPlan for Software Aspects of Certification \n11.1 \n    \n9.1.a 9.1.b 9.1.c \nThe means of compliance is proposed and agreement with the Plan for Software Aspects of Certification is obtained. \n11.20 \n    \nSoftware Accomplishment Summary \n3 \n9.c \n    \nCompliance substantiation is provided. \n9.2.a 9.2.b 9.2.c \nSoftware Configuration Index \n11.16 \n   ",
    "This Page Intentionally Left Blank \n \n\n## Annex B Acronyms And Glossary Of Terms",
    "| Acronym                                                    | Meaning                                    |\n|------------------------------------------------------------|--------------------------------------------|\n| ARP                                                        | Aerospace Recommended Practice             |\n| ATM                                                        | Air Traffic Management                     |",
    "| CAST                                                       | Certification Authorities Software Team    |\n| CC1                                                        | Control Category 1                         |\n| CC2                                                        | Control Category 2                         |\n| CNS                                                        | Communication, Navigation and Surveillance |",
    "| COTS                                                       | Commercial-Off-The-Shelf                   |\n| CRC                                                        | Cyclic Redundancy Check                    |\n| DO                                                         | Document                                   |\n| EASA                                                       | European Aviation Safety Agency            |",
    "| EUROCAE European Organization for Civil Aviation Equipment |                                            |\n| FAA                                                        | Federal Aviation Administration            |\n| IDAL                                                       | Item Development Assurance Level           |\n| I/O                                                        | Input/Output                               |",
    "| MC/DC                                                      | Modified Condition/Decision Coverage       |\n| PMC                                                        | Program Management Committee               |\n| PSAC                                                       | Plan for Software Aspects of Certification |\n| RTCA                                                       | RTCA, Inc.                                 |",
    "| SAE                                                        | Society of Automotive Engineers            |\n| SC                                                         | Special Committee                          |\n| SCI                                                        | Software Configuration Index               |\n| SCM                                                        | Software Configuration Management          |",
    "| SDP                                                        | Software Development Plan                  |",
    "Acronym \nMeaning \nSECI \nSoftware Life Cycle Environment Configuration Index \nSQA \nSoftware Quality Assurance \nSVP \nSoftware Verification Plan \nSW \nSoftware \nTOR \nTerms of Reference \nTQL \nTool Qualification Level \nU.S.A. \nUnited States of America \nWG \nWorking Group \n \n\n## Glossary\n\nThese definitions are provided for the terms used in this document. If a term is not defined in this annex, it is possible that it is defined instead in the body of this document.",
    "Activity - Tasks that provide a means of meeting the objectives. Aeronautical data - Data used for aeronautical applications such as navigation, flight planning, flight simulators, terrain awareness, and other purposes. \n\nAirborne - A qualifier used to denote software, equipment, or systems onboard an aircraft. \n\nAlgorithm - A finite set of well-defined rules that give a sequence of operations for performing a specific task.",
    "Alternative method - Different approach to satisfy one or more objectives of this document.",
    "Anomalous behavior - Behavior that is inconsistent with specified requirements. Applicant - A person or organization seeking approval from the certification authority. Approval - The act or instance of giving formal recognition or official sanction. Approved source - The location of the software life cycle data to be retrieved is identified in the Software Configuration Index. The \"approved source\" could be a software configuration management library, an electronic archive, or an organization",
    "could be a software configuration management library, an electronic archive, or an organization other than the developing organization.",
    "Assurance - The planned and systematic actions necessary to provide adequate confidence and evidence that a product or process satisfies given requirements. \n\nAudit - An independent examination of the software life cycle processes and their outputs to confirm required attributes. \n\nAutocode generator - A coding tool that automatically produces Source Code or object code from low-level requirements.",
    "Baseline - The approved, recorded configuration of one or more configuration items, that thereafter serves as the basis for further development, and that is changed only through change control procedures.",
    "Boolean expression - An expression that results in a TRUE or FALSE value. Boolean operator - An operator with Boolean operands that yields a Boolean result. Certification - Legal recognition by the certification authority that a product, service, organization, or person complies with the requirements. Such certification comprises the activity of technically checking the product, service, organization, or person and the formal recognition of compliance with the applicable requirements by issue",
    "or person and the formal recognition of compliance with the applicable requirements by issue of a certificate, license, approval, or other documents as required by national laws and procedures. In particular, certification of a product involves: (a) the process of assessing the design of a product to ensure that it complies with a set of requirements applicable to that type of product so as to demonstrate an acceptable level of safety; (b) the process of assessing an individual product to",
    "to demonstrate an acceptable level of safety; (b) the process of assessing an individual product to ensure that it conforms with the certified type design; (c) the issuance of a certificate required by national laws to declare that compliance or conformity has been found with requirements in accordance with items (a) or (b) above.",
    "Certification authority –Organization or person responsible within the state, country, or other relevant body, concerned with the certification or approval of a product in accordance with requirements.",
    "Note 1: A matter concerned with aircraft, engine, propeller or, by region, auxiliary power unit type certification or with equipment approval would usually be addressed by the certification authority; matters concerned with continuing airworthiness might be addressed by what would be referred to as the airworthiness authority.",
    "Note 2: The certification authority may use a system under which the actual determinations of the compliance can be made by an approved design organization or by authorized individuals. \n\nCertification credit - Acceptance by the certification authority that a process, product, or demonstration satisfies a certification requirement. \n\nCertification liaison process - The process that establishes communication, understanding, and agreements between the applicant and the certification authority.",
    "Change control - (1) The process of recording, evaluating, approving or disapproving, and coordinating changes to configuration items after formal establishment of their configuration identification or to baselines after their establishment. (2) The systematic evaluation, coordination, approval or disapproval, and implementation of approved changes in the configuration of a configuration item after formal establishment of its configuration identification or to baselines after their",
    "item after formal establishment of its configuration identification or to baselines after their establishment.",
    "Note: This term may be called \"configuration control\" in other industry documents. \n\nCode - The implementation of particular data or a particular computer program in a symbolic form, such as Source Code, object code, or machine code.",
    "Commercial-Off-The-Shelf (COTS) software - Commercially available applications sold by vendors through public catalog listings. COTS software is not intended to be customized or enhanced. Contract-negotiated software developed for a specific application is not COTS software. \n\nCompacted expressions - Representations of Source Code where many constructs are combined into a single expression.",
    "Compiler - Program that translates Source Code statements of a high level language, such as FORTRAN or Pascal, into object code. \n\nComponent - A self-contained part, combination of parts, subassemblies, or units that performs a distinct function of a system. \n\nCondition - A Boolean expression containing no Boolean operators except for the unary operator (NOT).",
    "Configuration identification - (1) The process of designating the configuration items in a system and recording their characteristics. (2) The approved documentation that defines a configuration item. \n\nConfiguration item - (1) One or more hardware or software components treated as a unit for configuration management purposes. (2) Software life cycle data treated as a unit for configuration management purposes.",
    "Configuration management - (1) The process of (a) identifying and defining the configuration items of a system; (b) controlling the release and change of these items throughout the software life cycle; (c) recording and reporting the status of configuration items and Problem Reports; and (d) verifying the completeness and correctness of configuration items. (2) A discipline applying technical and administrative direction and surveillance to (a) identify and record the functional and physical",
    "administrative direction and surveillance to (a) identify and record the functional and physical characteristics of a configuration item; (b) control changes to those characteristics; and (c) record and report change control processing and implementation status.",
    "Configuration status accounting - The recording and reporting of the information necessary to manage a configuration effectively, including a listing of the approved configuration identification, the status of proposed changes to the configuration, and the implementation status of approved changes.",
    "Control category - Configuration management controls placed on software life cycle data. The two categories, CC1 and CC2, define the software configuration management processes and activities applied to control software life cycle data. \n\nControl coupling - The manner or degree by which one software component influences the execution of another software component. \n\nControl program - A computer program designed to schedule and to supervise the execution of programs in a computer system.",
    "Coverage analysis - The process of determining the degree to which a proposed software verification process activity satisfies its objective. \n\nData coupling - The dependence of a software component on data not exclusively under the control of that software component. \n\nData dictionary - The detailed description of data, parameters, variables, and constants used by the system.",
    "Database - A set of data, part or the whole of another set of data, consisting of at least one file that is sufficient for a given purpose or for a given data processing system.",
    "Deactivated code - Executable Object Code (or data) that is traceable to a requirement and, by design, is either (a) not intended to be executed (code) or used (data), for example, a part of a previously developed software component such as unused legacy code, unused library functions, or future growth code; or (b) is only executed (code) or used (data) in certain configurations of the target computer environment, for example, code that is enabled by a hardware pin selection or software",
    "computer environment, for example, code that is enabled by a hardware pin selection or software programmed options. The following examples are often mistakenly categorized as deactivated code but should be identified as required for implementation of the design/requirements: defensive programming structures inserted for robustness, including compiler-inserted object code for range and array index checks, error or exception handling routines, bounds and reasonableness checking, queuing controls,",
    "checks, error or exception handling routines, bounds and reasonableness checking, queuing controls, and time stamps.",
    "Dead code - Executable Object Code (or data) which exists as a result of a software development error but cannot be executed (code) or used (data) in any operational configuration of the target computer environment. It is not traceable to a system or software requirement. The following exceptions are often mistakenly categorized as dead code but are necessary for implementation of the requirements/design: embedded identifiers, defensive programming structures to improve robustness, and",
    "embedded identifiers, defensive programming structures to improve robustness, and deactivated code such as unused library functions.",
    "Decision - A Boolean expression composed of conditions and zero or more Boolean operators. If a condition appears more than once in a decision, each occurrence is a distinct condition. \n\nDecision coverage - Every point of entry and exit in the program has been invoked at least once and every decision in the program has taken on all possible outcomes at least once.",
    "Derived requirements - Requirements produced by the software development processes which (a) are not directly traceable to higher level requirements, and/or (b) specify behavior beyond that specified by the system requirements or the higher level software requirements.",
    "Embedded identifier - Identification attributes of the software, for example, creation date, part number, linker integrity verification checksum or cyclic redundancy check (CRC), or version identification, included in the target Executable Object Code. \n\nEmulator - A device, computer program, or system that accepts the same inputs and produces the same output as a given system using the same object code.",
    "End-to-end numerical resolution - Measure of the numerical precision resulting from computations through the integrated system. \n\nEquivalence class - The partition of the input domain of a program such that a test of a representative value of the class is equivalent to a test of other values of the class. \n\nEquivalent safety - Level of safety achieved using an alternative method to satisfy both the objectives of this document and the system safety objectives.",
    "Error - With respect to software, a mistake in requirements, design, or code. Executable Object Code - A form of code that is directly usable by the processing unit of the target computer and is, therefore, a compiled, assembled, and linked binary image that is loaded into the target computing hardware.",
    "Extraneous code - Code (or data) that is not traceable to any system or software requirement. An example of extraneous code is legacy code that was incorrectly retained although its requirements and test cases were removed. Another example of extraneous code is dead code. \n\nFailure - The inability of a system or system component to perform a required function within specified limits. A failure may be produced when a fault is encountered.",
    "Failure condition - The effect on the aircraft and its occupants both direct and consequential caused or contributed to by one or more failures, considering relevant adverse operational and environmental conditions. A failure condition is classified according to the severity of its effect as defined in advisory material issued by the certification authority.",
    "Fault - A manifestation of an error in software. A fault, if it occurs, may cause a failure. Fault tolerance - The built-in capability of a system to provide continued correct execution in the presence of a limited number of hardware or software faults. \n\nFormal methods - Descriptive notations and analytical methods used to construct, develop, and reason about mathematical models of system behavior. A formal method is a formal analysis carried out on a formal model.",
    "Hardware/software integration - The process of combining the software into the target computer. \n\nHigh-level requirements - Software requirements developed from analysis of system requirements, safety-related requirements, and system architecture. \n\nHost computer - The computer on which the software is developed.",
    "Independence - Separation of responsibilities which ensures the accomplishment of objective evaluation. (1) For software verification process activities, independence is achieved when the verification activity is performed by a person(s) other than the developer of the item being verified, and a tool(s) may be used to achieve equivalence to the human verification activity. (2) For the software quality assurance process, independence also includes the authority to ensure corrective action.",
    "Integral process - A process which assists the software development processes and other integral processes and, therefore, remains active throughout the software life cycle. The integral processes are the software verification process, the software quality assurance process, the software configuration management process, and the certification liaison process. \n\nIntegrity - An attribute of the system or an item indicating that it can be relied upon to work correctly on demand.",
    "Interrupt - A suspension of a task, such as the execution of a computer program, caused by an event external to that task, and performed in such a way that the task can be resumed. \n\nLow-level requirements - Software requirements developed from high-level requirements, derived requirements, and design constraints from which Source Code can be directly implemented without further information.",
    "Means of compliance - The intended method(s) to be used by the applicant to satisfy the requirements stated in the certification basis for an aircraft, engine, propeller, or, by region, auxiliary power unit. Examples include statements, drawings, analyses, calculations, testing, simulation, inspection, and environmental qualification. Advisory material issued by the certification authority is used, if appropriate.",
    "Media - Devices or materials which act as a means of transferring or storing software, for example, programmable read-only memory, magnetic tapes or discs, and paper. \n\nMemory device - An article of hardware capable of storing machine-readable computer programs and associated data. Examples include an integrated circuit chip, a circuit card containing integrated circuit chips, a core memory, a disk, or a magnetic tape.",
    "Modified condition/decision coverage - Every point of entry and exit in the program has been invoked at least once, every condition in a decision in the program has taken all possible outcomes at least once, every decision in the program has taken all possible outcomes at least once, and each condition in a decision has been shown to independently affect that decision's outcome. A condition is shown to independently affect a decision's outcome by: (1) varying just that condition while holding",
    "to independently affect a decision's outcome by: (1) varying just that condition while holding fixed all other possible conditions, or (2) varying just that condition while holding fixed all other possible conditions that could affect the outcome.",
    "Monitoring - The act of witnessing or inspecting selected instances of test, inspection, or other activity, or records of those activities, to assure that the activity is under control and that the reported results are representative of the expected results. Monitoring is usually associated with activities done over an extended period of time where 100% witnessing is considered impractical or unnecessary. Monitoring permits authentication that the claimed activity was performed as planned.",
    "Multiple-version dissimilar software - Two or more software components that satisfy the same functional requirements, but are intentionally different from one another. Example approaches include the use of separate development organizations or the use of different development techniques. Common mode errors may be minimized by using multipleversion dissimilar software techniques.",
    "Object code - A low-level representation of the computer program not usually in a form directly usable by the target computer but in a form which includes relocation information in addition to the processor instruction information. \n\nObjective - When this document is identified as a means of compliance to the regulations, the objectives are requirements that should be met to demonstrate compliance.",
    "Parameter data item - A set of data that, when in the form of a Parameter Data Item File, influence the behavior of the software without modifying the Executable Object Code and that is managed as a separate configuration item. Examples include databases and configuration tables.",
    "Parameter Data Item File - The representation of the parameter data item that is directly usable by the processing unit of the target computer. A Parameter Data Item File is an instantiation of the parameter data item containing defined values for each data element. \n\nPart number - A set of numbers, letters, or other characters used to identify a configuration item. \n\nPartitioning - A technique for providing isolation between software components to contain and/or isolate faults.",
    "Patch - Modification to Executable Object Code in which one or more of the planned steps of re-compiling, re-assembling, or re-linking is bypassed. This does not include embedded identifiers. \n\nPreviously developed software - Software already developed for use. This encompasses a wide range of software, including COTS software through software developed to previous or current software guidance.",
    "Process - A collection of activities performed in the software life cycle to produce a definable output or product. \n\nProduct service history - A contiguous period of time during which the software is operated within a known environment, and during which successive failures are recorded. \n\nRelease - The act of formally making available and authorizing the use of a retrievable configuration item.",
    "Reverification - The evaluation of the results of a modification process, for example correction of errors or the introduction of new or additional functionality, to ensure correctness and consistency with respect to the inputs and standards provided to that process.",
    "Reverse engineering - The process of developing higher level software data from existing software data. Examples include developing Source Code from object code or Executable Object Code, or developing high level requirements from low level requirements. \n\nRobustness - The extent to which software can continue to operate correctly despite abnormal inputs and conditions.",
    "Safety monitoring - A means of protecting against specific failure conditions by directly monitoring a function for failures that would result in a failure condition.",
    "Service experience - Intervals of time during which the software is operated within a known relevant and controlled environment, during which successive failures are recorded.  \nService history data –Data collected during the service history period. Simulator - A device, computer program, or system used during software verification, that accepts the same inputs and produces the same output as a given system, using object code that is derived from the original object code.",
    "Single event upset - Random bit flip in data that can occur in hardware. Software - Computer programs and, possibly, associated documentation and data pertaining to the operation of a computer system. \n\nSoftware architecture - The structure of the software selected to implement the software requirements. \n\nSoftware assurance - The planned and systematic actions necessary to provide confidence and evidence that a software product or process satisfies given requirements.",
    "Software change - A modification in Source Code, object code, Executable Object Code, or its related documentation from its baseline. \n\nSoftware conformity review - A review, typically conducted at the end of a software development project, for the purpose of assuring that the software life cycle processes are complete, software life cycle data is complete, and the Executable Object Code is controlled and can be regenerated.",
    "Software development standards –Standards which define the rules and constraints for the software development processes. The software development standards include the Software Requirements Standards, the Software Design Standards, and the Software Code Standards.",
    "Software integration - The process of combining code components. Software level - The designation that is assigned to a software component as determined by the system safety assessment process. The software level establishes the rigor necessary to demonstrate compliance with this document.",
    "Note: Other industry documents may use a different term for the designation resulting from the system safety assessment process. One example is the term \"item development assurance level\" (IDAL), which for software is synonymous with the term \"software level.\"",
    "Software library - A controlled repository containing a collection of software and related data and documents designed to aid in software development, use, or modification. Examples include software development library, master library, program library, and software repository.",
    "Software life cycle - (1) An ordered collection of processes determined by an organization to be sufficient and adequate to produce a software product. (2) The period of time that begins with the decision to produce or modify a software product and ends when the product is retired from service.",
    "Software partitioning - The process of separating, usually with the express purpose of isolating one or more attributes of the software, to prevent specific interactions and crosscoupling interference. \n\nSoftware product - The set of computer programs, and associated documentation and data, designated for delivery to a user. In the context of this document, this term refers to software intended for use in airborne applications and the associated software life cycle data.",
    "Software requirement - A description of what is to be produced by the software given the inputs and constraints. Software requirements include both high-level requirements and low-level requirements. \n\nSoftware tool - A computer program used to help develop, test, analyze, produce, or modify another program or its documentation. Examples are an automated design tool, a compiler, test tools, and modification tools.",
    "Source Code - Code written in source languages, such as assembly language and/or high level language, in a machine-readable form for input to an assembler or a compiler. \n\nStandard - A rule or basis of comparison used to provide both guidance in and assessment of the performance of a given activity or the content of a specified data item. \n\nStatement coverage - Every statement in the program has been invoked at least once. \n\nNote: Statement is as defined by the programming language.",
    "Note: Statement is as defined by the programming language. \n\nStructural coverage analysis - An evaluation of the code structure, including interfaces, exercised during requirements-based testing.",
    "Structure - A specified arrangement or interrelation of parts to form a whole. Supplement - Guidance used in conjunction with this document that addresses the unique nature of a specific approach, method, or technique. A supplement adds, deletes, or otherwise modifies: objectives, activities, explanatory text, and software life cycle data in this document. \n\nSystem - A collection of hardware and software components organized to accomplish a specific function or set of functions.",
    "System architecture - The structure of the hardware and the software selected to implement the system requirements. \n\nSystem safety assessment process - An ongoing, systematic, comprehensive evaluation of the proposed system to show that relevant safety-related requirements are satisfied.",
    "The major activities within this process include: functional hazard assessment, preliminary system safety assessment, and system safety assessment. The rigor of the activities will depend on the criticality, complexity, novelty, and relevant service experience of the system concerned.",
    "Task - The basic unit of work from the standpoint of a control program. Test case - A set of test inputs, execution conditions, and expected results developed for a particular objective, such as to exercise a particular program path or to verify compliance with a specific requirement. \n\nTest procedure - Detailed instructions for the set-up and execution of a given set of test cases, and instructions for the evaluation of results of executing the test cases.",
    "Testing - The process of exercising a system or system component to verify that it satisfies specified requirements and to detect errors. \n\nTool qualification - The process necessary to obtain certification credit for a software tool within the context of a specific airborne system.",
    "Trace data - Data providing evidence of traceability of development and verification processes' software life cycle data without implying the production of any particular artifact. Trace data may show linkages, for example, through the use of naming conventions or through the use of references or pointers either embedded in or external to the software life cycle data.",
    "Traceability - An association between items, such as between process outputs, between an output and its originating process, or between a requirement and its implementation. \n\nTransition criteria - The minimum conditions, as defined by the software planning process, to be satisfied to enter a process.",
    "Type design - For the purposes of this document, the type design consists of the following: (1) The drawings and specifications, and a listing of those drawings and specifications, necessary to define the configuration and the design features of the certified product shown to comply with the requirements for that product; (2) Any other data necessary to allow, by comparison, the determination of the airworthiness of later products of the same type.",
    "Note: Use of the term \"product\" in this definition refers to any item for which type certificates are granted by the certification authorities, for example, aircraft, engines, propellers, and, by region, auxiliary power units. \n\nUnbounded recursive algorithm - An algorithm that directly invokes itself (self recursion) or indirectly invokes itself (mutual recursion), and does not have a mechanism to limit the number of times it can do this before completing.",
    "User-modifiable software - Software intended for modification without review by the certification authority, the airframe manufacturer, or the equipment vendor, if within the modification constraints established during the original certification project. \n\nValidation - The process of determining that the requirements are the correct requirements and that they are complete. The system life cycle processes may use software requirements and derived requirements in system validation.",
    "Verification\n - The evaluation of the outputs of a process to ensure correctness and consistency with respect to the inputs and standards provided to that process. \n\n \n\n## Appendix A Background Of Do-178/Ed-12 Document 1. Prior Document Version History",
    "In May 1980, the Radio Technical Commission for Aeronautics, now RTCA, Inc., established Special Committee 145 (SC-145), \"Digital Avionics Software,\" to develop and document software practices that would support the development of software-based airborne systems and equipment. The European Organisation for Civil Aviation Electronics, now the European Organisation for Civil Aviation Equipment (EUROCAE), had previously established Working Group 12 (WG-12) to produce a similar document and, in",
    "had previously established Working Group 12 (WG-12) to produce a similar document and, in October 1980, was ready to publish document ED-35, \"Recommendations on Software Practice and Documentation for Airborne Systems.\" EUROCAE elected to withhold publication of its document and, instead, to work in concert with RTCA to develop a common guidance. SC-145 produced RTCA Document DO-178, \"Software Considerations in Airborne Systems and Equipment Certification,\" which was approved by the RTCA",
    "Considerations in Airborne Systems and Equipment Certification,\" which was approved by the RTCA Executive Committee and published by RTCA in January 1982. EUROCAE published ED-12 shortly thereafter; its technical content was identical to DO-",
    "178. Early in 1983, the RTCA Executive Committee determined that DO-178 should be revised to reflect the experience gained in the certification of the aircraft and engines containing software based systems and equipment. It established Special Committee 152 (SC-152) for this purpose.",
    "As a result of this committee's work, a revised RTCA document, DO-178A, \"Software Considerations in Airborne Systems and Equipment Certification,\" was published in 1985. Shortly thereafter, EUROCAE published ED-12A (through WG-12), which was identical in technical content to DO-178A. In early 1989, the Federal Aviation Administration (FAA) formally requested that RTCA establish a Special Committee for the review and revision of DO-178A. Since its release in 1985, the aircraft manufacturers, the",
    "for the review and revision of DO-178A. Since its release in 1985, the aircraft manufacturers, the avionics industry, and the certification authorities throughout the world had used DO-178A, or the equivalent EUROCAE ED-12A, as the primary source of the guidelines to determine the acceptability of airborne systems and equipment containing software. However, rapid advances in software technology, which were not envisioned by SC-152",
    "(or WG-12), and differing interpretations which were applied to some crucial areas, indicated that the guidance required revision. Accordingly, an RTCA Ad Hoc committee was formed with representatives from ARINC, the Airline Pilots Association, the National Business Aircraft Association, the Air Transport Association, and the FAA to consider the FAA request. The group reviewed the issues and experience associated with the application of DO-178A and concluded that a Special Committee should be",
    "associated with the application of DO-178A and concluded that a Special Committee should be authorized to revise DO-178A. The RTCA Executive Committee established Special Committee 167 (SC-167) during the autumn of 1989 to accomplish this task. EUROCAE",
    "WG-12 was re-established to work with SC-167.",
    "The cooperative efforts of SC-167 and WG-12 culminated in the publication of RTCA \ndocument DO-178B in December 1992. Shortly thereafter, EUROCAE published ED-\n12B, which was identical in technical content to DO-178B. \n\n## 2. Rtca/Eurocae Committee Activities In The Production Of This Document",
    "Since 1992, the aviation industry and certification authorities around the world have used the considerations in DO-178B/ED-12B as an acceptable means of compliance for software approval in the certification of airborne systems and equipment. As experience was gained in the use of DO-178B/ED-12B, questions arose regarding the document's content and application. Some of these questions were addressed through the work of SC- 190/WG-52 in the development of DO-248B/ED-94B. However, DO-248B/ED-94B",
    "through the work of SC- 190/WG-52 in the development of DO-248B/ED-94B. However, DO-248B/ED-94B did not contain additional guidance for use as compliance, only clarifications. Additionally, advances in hardware and software technology resulted in software development methodologies and issues which were not adequately addressed in DO-178B/ED-12B or DO-248B/ED-94B. In 2004, the FAA and aviation industry representatives initiated a discussion with RTCA concerning the advances in software",
    "industry representatives initiated a discussion with RTCA concerning the advances in software technology since 1992, when DO-178B/ED-12B",
    "was published. RTCA then requested a Software Ad Hoc committee evaluate the issues and determine the need for improved guidance in light of these advancements in technology. The Software Ad Hoc committee, which included European participants, recommended to RTCA that a special committee be formed to address these issues. In December 2004, RTCA and EUROCAE approved the sponsorship of such a joint special committee/working group, Special Committee 205/Working Group 71, SC-205/WG-71.",
    "a. The Terms of Reference (TORs) provided to the SC/WG were: \n1. Modify DO-178B/ED-12B to become DO-178C/ED-12C, or other document \nnumber. \n2. Modify DO-248B/ED-94B to become DO-248C/ED-94C, or other document \nnumber. \n3. Consolidate Software Development Guidance. 4. Consolidate Software Development Guidelines. \n5. Develop and document technology-specific or method-specific guidance and \nguidelines. \n6. Determine, document and report the effects of DO-178C/ED-12C or other",
    "guidelines. \n6. Determine, document and report the effects of DO-178C/ED-12C or other \nmodified documents to DO-278/ED-109 and recommend direction to ensure consistency. \n7. Develop and document rationale for each DO-178B/ED-12B objective. 8. Evaluate the issues in the 'Software Issues List.xls' spreadsheet produced by the \nSoftware Ad Hoc committee and other identified issues. Determine 'if', 'where' and 'how' each issue should be addressed.",
    "9. Coordinate SC/WG products with software certification authorities via \nCertification Authorities Software Team (CAST) or other appropriate groups. \n10. Coordinate with other groups and existing organizations (for example, SAE S18, \nWG-63, SC-200/WG-60), as appropriate. \n11. Report to the SC/WG's governing body the direction being taken by the \ncommittee within 6-9 months after the first SC/WG meeting. \n12. Work with RTCA and EUROCAE to explore and implement ways of expanding",
    "12. Work with RTCA and EUROCAE to explore and implement ways of expanding \nthe usability of the deliverables (for example, hypertext electronic versions). \n13. Modify DO-278/ED-109 to become DO-278A/ED-109A, or other document \nnumber. \n14. Submit to RTCA and EUROCAE a DO-178C/ED-12C and DO-278A/ED-109A \ncommonality analysis when documents are finalized. \nb. The RTCA Program Management Committee (PMC) directed the SC/WG to \nmaintain or adhere to the following while accomplishing the TORs:",
    "maintain or adhere to the following while accomplishing the TORs: \n1. Maintain the current objective-based approach for software assurance. \n2. Maintain the technology independent nature of the DO-178B/ED-12B and \nDO-278/ED-109 objectives. \n3. Evaluate issues as brought forth to the SC/WG. For any candidate guidance \nmodifications determine if the issue can be satisfied first in guideline related documents. \n4. Modifications to DO-178B/ED-12B and DO-278/ED-109 should: \ni.",
    "4. Modifications to DO-178B/ED-12B and DO-278/ED-109 should: \ni. \nIn the context of maintaining backward compatibility with DO-178B/ED-\n12B, make those changes to the existing text that are needed to adequately address the current states of the art and practice in software development in support of system safety, to address emerging trends, and to allow change with technology. \nii. Consider the economic impact relative to system certification or approval \nwithout compromising system safety.",
    "without compromising system safety. \niii. Address clear errors or inconsistencies in DO-178B/ED-12B and DO-\n278/ED-109. \niv. Fill any clear gaps in DO-178B/ED-12B and DO-278/ED-109. \nv. Meet a documented need to a defined assurance benefit. vi. Report any proposed changes to the number of software levels or mapping of",
    "levels to hazard categories to the SC/WG's governing body and provide a documented substantiated need, at the earliest feasible opportunity. Communicate back to the SC/WG at large, any concerns of the governing \nbody. \nvii. Ensure that all deliverables produced by the committee contain consistent \nand complete usability mechanisms (for example, indexes, glossaries). \nc. Seven joint RTCA/EUROCAE sub-groups were formed to address the TORs: \n1. Documentation Integration. \n2. Issues and Rationale.",
    "1. Documentation Integration. \n2. Issues and Rationale. \n3. Tool Qualification. \n4. Model-Based Development and Verification. 5. Object-Oriented Technology. \n6. Formal Methods. \n7. Special Considerations and CNS/ATM. \nd. The cooperative efforts of SC-205 and WG-71 culminated in the publication of \nRTCA document DO-178C and EUROCAE document ED-12C.",
    "## 3. Summary Of Differences Between Do-178C And Do-178B\n\nDO-178C is an update to DO-178B. The DO-178C updates fall into a variety of categories:",
    "a. Errors and Inconsistencies\n:  DO-178C addressed DO-178B's known errors and \ninconsistencies. For example, DO-178C has addressed the errata of DO-178B and has \nremoved inconsistencies between the different tables of DO-178B Annex A. \nConsistent Terminology\nb. \n:  DO-178C addressed issues regarding the use of specific \nterms such as \"guidance\", \"guidelines\", \"purpose\", \"goal\", \"objective\", and",
    "terms such as \"guidance\", \"guidelines\", \"purpose\", \"goal\", \"objective\", and \n\"activity\" by changing the text so that the use of those terms is consistent throughout the document. \nWording Improvements\nc. \n:  DO-178C made wording improvements throughout the \ndocument. All such changes were made simply to make the document more precise; \nthey were not meant to change the original intent of DO-178B. \nObjectives and Activities\nd. \n:  DO-178C reinforced the point that, in order to fully",
    "Objectives and Activities\nd. \n:  DO-178C reinforced the point that, in order to fully \nunderstand the recommendations, the full body of this document should be considered. For example, Annex A now includes references to each activity as well \nas to each objective; and section 1.4, titled \"How to Use This Document\" reinforces \nthe point that activities are a major part of the overall guidance. \nSupplements\ne. \n:  DO-178C recognized that new software development techniques may",
    "Supplements\ne. \n:  DO-178C recognized that new software development techniques may \nresult in new issues. Rather than expanding text to account for all the current software development techniques (and being revised yet again to account for future techniques), DO-178C acknowledged that one or more supplements may be used in \nconjunction with DO-178C to modify the guidance for specific techniques. Section",
    "conjunction with DO-178C to modify the guidance for specific techniques. Section \n12 was impacted since planned supplements more completely address those specific techniques. \nf. \nTool Qualification (Section 12.2):  The terms \"development tool\" and \"verification \ntool\" are replaced by three tool qualification criteria that determine the applicable",
    "tool\" are replaced by three tool qualification criteria that determine the applicable \ntool qualification level (TQL) in regard of the software level. The guidance to qualify a tool is removed in DO-178C, but provided in a domain independent, external document, referenced in section 12.2",
    "g. Coordinated System/Software Aspects\n:  DO-178C updated section 2, which provides \nsystem aspects relating to software development, to reflect current system practices. The updates were based upon coordination with the committees which were updating ARP4754 (system-level guidance) at the same time SC-205/WG-71 was updating DO-178B (software-level guidance). \nDO-178B \"Hidden\" Objectives\nh. \n:  DO-178C added the so-called \"hidden objectives\" \nto Annex A:",
    "DO-178B \"Hidden\" Objectives\nh. \n:  DO-178C added the so-called \"hidden objectives\" \nto Annex A: \n1. A means for detecting additional code that is not directly traceable to the Source \nCode and a means to ensure its verification coverage are defined (see objective 9 of \nTable A-7\n). \n). \n2. Assurance is obtained that software plans and standards are developed and \nreviewed for consistency (see objective 1 of \nTable A-9\ni.",
    "reviewed for consistency (see objective 1 of \nTable A-9\ni. \nGeneral Topics:  DO-178C addressed some general topics that resulted in changes to several sections of the document. The topics included a variety of subjects such as applicant's oversight of suppliers, parameter data items, and traceability. In addressing these topics, two additional objectives were added to Annex A: \n). \n1. Parameter Data Item File is correct and complete (see objective 8 of Table A-5\nTable A-",
    "). \n1. Parameter Data Item File is correct and complete (see objective 8 of Table A-5\nTable A-\n2. Verification of Parameter Data Item File is achieved (see objective 9 of \n5\n). \nAlso, Trace Data was identified as software life cycle data (see 11.21, Table A-2, and Table A-6).",
    "j. \nDO-178B Gaps and Clarifications:  DO-178C addressed several specific issues that \nresulted in change to only one or two paragraphs. Each such change may have an \nimpact upon the applicant as these changes either addressed clear gaps in DO-178B or clarified guidance that was subject to differing interpretations. \n1. Examples of gaps addressed include: \ni. \nThe \"Modified Condition/Decision Coverage\" (MC/DC) definition changed.",
    "i. \nThe \"Modified Condition/Decision Coverage\" (MC/DC) definition changed. \nMasking MC/DC and Short Circuit, as well as DO-178B's interpretation of \nMC/DC (often termed Unique-Cause MC/DC), are now allowed (see Glossary). \nii. Derived requirements should now be provided to the system processes, \nincluding the system safety assessment process, rather than just provided to the system safety assessment process (see 5.1.1.b and 5.2.1.b). \n2. Examples of clarifications include: \ni.",
    "2. Examples of clarifications include: \ni. \nClarified that the structural coverage analysis of data and control coupling between code components should be achieved by assessing the results of the \nrequirements-based tests (see 6.4.4.2.c). \nii. Clarified that all tests added to achieve structural coverage are based on \nrequirements (see 6.4.4.2.d).",
    "## Appendix B Committee Membership Executive Committee Members",
    "| Jim Krodel, Pratt & Whitney                      |                                     |                                    | SC-205 Chair                  |\n|--------------------------------------------------|-------------------------------------|------------------------------------|-------------------------------|\n| Gérard Ladier, Airbus/Aerospace Valley           |                                     | WG-71 Chair                        |                               |",
    "| Mike DeWalt, Certification Services, Inc./FAA    | SC-205 Secretary (until March 2008) |                                    |                               |\n| Leslie Alford, Boeing Company                    |                                     | SC-205 Secretary (from March 2008) |                               |\n| Ross Hannan, Sigma Associates (Aerospace)        |                                     | WG-71 Secretary                    |                               |",
    "| Barbara Lingberg, FAA                            |                                     |                                    | FAA Representative/CAST Chair |\n| Jean-Luc Delamaide, EASA                         |                                     |                                    | EASA Representative           |\n| John Coleman, Dawson Consulting                  |                                     | Sub-group Liaison                  |                               |",
    "| Matt Jaffe, Embry-Riddle Aeronautical University | Web Site Liaison                    |                                    |                               |\n| Todd R. White, L-3 Communications/Qualtech       | Collaborative Technology Software   |                                    |                               |\n| Liaison                                          |                                     |                                    |                               |",
    "## Sub-Group Leadership Sg-1 - Document Integration",
    "| Ron Ashpole, SILVER ATENA                            |                                  |\n|------------------------------------------------------|----------------------------------|\n| Tom Ferrell, Ferrell and Associates Consulting       | SG-1 Co-chair (until March 2008) |\n| Marty Gasiorowski, Worldwide Certification Services  | SG-1 Co-chair (from March 2008)  |\n| Tom Roth, Airborne Software Certification Consulting | SG-1 Secretary                   |\n\n## Sg-2 - Issues And Rationale",
    "| Ross Hannan, Sigma Associates (Aerospace)     |                                  | SG-2 Co-chair                   |\n|-----------------------------------------------|----------------------------------|---------------------------------|\n| Mike DeWalt, Certification Services, Inc./FAA | SG-2 Co-chair (until March 2008) |                                 |\n| Will Struck, FAA                              |                                  |                                 |",
    "| Fred Moyer, Rockwell Collins                  |                                  | SG-2 Co-chair (from April 2009) |\n| John Angermayer, Mitre                        |                                  |                                 |",
    "## Sg-3 - Tool Qualification",
    "| Frédéric Pothon, ACG Solutions            |     |                   | SG-3 Co-chair                      |\n|-------------------------------------------|-----|-------------------|------------------------------------|\n| Leanna Rierson, Digital Safety Consulting |     | SG-3 Co-chair     |                                    |\n| Bernard Dion, Esterel Technologies        |     | SG-3 Co-secretary |                                    |",
    "| Gene Kelly, CertTech                      |     |                   | SG-3 Co-secretary (until May 2009) |\n| Mo Piper, Boeing Company                  |     |                   | SG-3 Co-secretary (from May 2009)  |",
    "## Sg-4 - Model-Based Development And Verification",
    "| Pierre Lionne, EADS APSYS         |     |                | SG-4 Co-chair    |\n|-----------------------------------|-----|----------------|------------------|\n| Mark Lillis, Goodrich GPECS       |     | SG-4 Co-chair  |                  |\n| Hervé Delseny, Airbus             |     |                | SG-4 Co-chair    |\n| Martha Blankenberger, Rolls-Royce |     | SG-4 Secretary |                  |\n|                                   |     |                |                  |",
    "## Sg-5 - Object-Oriented Technology",
    "| Peter Heller, Airbus Operations GmbH    |                                   | SG-5 Co-chair (until February 2009)    |\n|-----------------------------------------|-----------------------------------|----------------------------------------|\n| Jan-Hendrik Boelens, Eurocopter         | SG-5 Co-chair (Feb 2009 to August |                                        |\n| 2010)                                   |                                   |                                        |",
    "| James Hunt, aicas                       |                                   |                                        |\n| Jim Chelini, Verocel                    |                                   |                                        |\n| Greg Millican, Honeywell                | SG-5 Co-chair (Oct 2009 to August |                                        |\n| 2011)                                   |                                   |                                        |",
    "| Jim Chelini, Verocel                    |                                   |                                        |",
    "## Sg-6 - Formal Methods\n\n|                      |    | Duncan Brown, Aero Engine Controls (Rolls-Royce)    | SG-6 Co-chair    |\n|----------------------|----|-----------------------------------------------------|------------------|\n| Kelly Hayhurst, NASA |    |                                                     | SG-6 Co-chair    |\n\n## Sg-7 - Special Considerations And Cns/Atm",
    "| David Hawken, NATS            |     |                                   | SG-7 Co-chair (until June 2010)    |\n|-------------------------------|-----|-----------------------------------|------------------------------------|\n| Jim Stewart, NATS             |     |                                   | SG-7 Co-chair (from June 2010)     |\n| Don Heck, Boeing Company      |     |                                   | SG-7 Co-chair                      |",
    "| Leslie Alford, Boeing Company |     | SG-7 Secretary (until March 2008) |                                    |\n| Marguerite Baier, Honeywell   |     |                                   | SG-7 Secretary (from March 2008)   |",
    "## Rtca Representative:",
    "| Rudy Ruana    |     |     |                               | RTCA Inc. (until September 2009)    |\n|---------------|-----|-----|-------------------------------|-------------------------------------|\n| Ray Glennon   |     |     |                               | RTCA Inc. (until March 2010)        |\n| Hal Moses     |     |     |                               | RTCA Inc. (until August 2010)       |",
    "| Hal Moses     |     |     |                               | RTCA Inc. (until August 2010)       |\n| Cyndy Brown   |     |     | RTCA Inc. (until August 2011) |                                     |\n| Hal Moses     |     |     |                               | RTCA Inc. (from August 2011)        |",
    "## Eurocae Representative:\n\n| Gilbert Amato     |     |     | EUROCAE (until September 2009)    |\n|-------------------|-----|-----|-----------------------------------|\n| Roland Mallwitz   |     |     | EUROCAE (from October 2009)       |\n|                   |     |     |                                   |\n\n## Editorial Committee",
    "## Editorial Committee\n\n| Leanna Rierson, Digital Safety Consulting     |     | Editorial Committee Chair    |\n|-----------------------------------------------|-----|------------------------------|\n| Ron Ashpole, SILVER ATENA                     |     | Editorial Committee          |\n| Alex Ayzenberg, Boeing Company                |     | Editorial Committee          |\n| Patty (Bartels) Bath, Esterline AVISTA        |     | Editorial Committee          |",
    "Dewi Daniels, Verocel \n \n \nEditorial Committee Hervé Delseny,  \nAirbus \n \n \nEditorial Committee \n\n| Andrew Elliott, Design Assurance    |     | Editorial Committee    |\n|-------------------------------------|-----|------------------------|\n\nKelly Hayhurst, NASA \n \n \nEditorial Committee Barbara Lingberg, FAA \n \n \nEditorial Committee Steven C. Martz, Garmin \n \n \nEditorial Committee",
    "| Steve Morton, TBV Associates     |     | Editorial Committee    |\n|----------------------------------|-----|------------------------|\n\nMarge Sonnek, Honeywell \n \n \nEditorial Committee \n\n \n\n## Committee Membership\n\nName \nOrganization",
    "Kyle Achenbach Rolls-Royce Dana E. Adkins Kidde Aerospace Leslie Alford Boeing Company Carlo Amalfitano Certon Software, Inc Gilbert Amato EUROCAE \nPeter Amey Praxis High Integrity Systems Robert Annis GE Aviation Allan Gilmour Anderson Embraer Håkan Anderwall Saab AB \nJoseph Angelo NovAtel Inc, Canada John Charles Angermayer Mitre Corp Ron Ashpole SILVER ATENA",
    "Joseph Angelo NovAtel Inc, Canada John Charles Angermayer Mitre Corp Ron Ashpole SILVER ATENA \nAlex Ayzenberg Boeing Company Marguerite Baier Honeywell Fred Barber Avidyne Clay Barber Garmin International Gerald F. Barofsky L-3 Communications Patty (Bartels) Bath Esterline AVISTA \nBrigitte Bauer  \nThales Phillipe Baufreton SAGEM DS   Safran Group Connie Beane ENEA Embedded Technology Inc Bernard Beaudouin EADS APSYS",
    "Germain Beaulieu Independent Consultant Martin Beeby Seaweed Systems Scott Beecher Pratt & Whitney Haik Biglari Fairchild Controls Peter Billing Aviya Technologies Inc Denise Black Embedded Plus Engineering Brad Blackhurst Independent Consultant Craig Bladow Woodward  \nMartha Blankenberger Rolls-Royce Holger Blasum SYSGO \nThomas Bleichner Rohde & Schwarz  \nDon Bockenfeld CMC Electronics Jan-Hendrik Boelens Eurocopter Eric Bonnafous CommunicationSys Jean-Christophe Bonnet CEAT",
    "Hugues Bonnin Cap Gemini Matteo Bordin AdaCore Feliks Bortkiewicz Boeing Julien Bourdeau DND (Canada) \nPaul Bousquet Volpe National Transportation Systems Center David Bowen EUROCAE \nElizabeth Brandli FAA \nAndrew Bridge EASA \nPaul Brook Thales Daryl Brooke Universal Avionics Systems Corporation Cyndy Brown RTCA, Inc.",
    "Duncan Brown Aero Engine Controls (Rolls-Royce) \n\n| Name              | Organization    |\n|-------------------|-----------------|\n| Thomas Buchberger | Siemens AG      |\n\nBrett Burgeles Consultant \n\nBernard Buscail \nAirbus",
    "Bob Busser Systems and Software Consortium Christopher Caines QinetiQ \nCristiano Campos Almeida De Freitas Embraer Jean-Louis Camus Esterel Technologies Richard Canis EASA \nYann Carlier DGAC \nLuc Casagrande EADS Apsys Mark Chapman Hamilton Sundstrand Scott Chapman FAA \nJim Chelini Verocel Daniel Chevallier",
    "Jim Chelini Verocel Daniel Chevallier  \nThales John Chilenski Boeing Company Subbiah Chockalingam HCL Technologies Chris Clark Sysgo Darren Cofer Rockwell Collins Keith Coffman Goodrich John Coleman Dawson Consulting Cyrille Comar AdaCore Ray Conrad Lockheed Martin Mirko Conrad The MathWorks, Inc.",
    "Nathalie Corbovianu DGAC \nAna Costanti Embraer Dewi Daniels Verocel Eric Danielson Rockwell Collins Henri De La Vallée Poussin SABCA \nMichael Deitz Gentex Corporation Jean-Luc Delamaide EASA \nHervé Delseny Airbus Patrick Desbiens Transport Canada Mike DeWalt Certification Services, Inc./FAA \nMansur Dewshi Ultra Electronics Controls Bernard Dion Esterel Technologies Antonio Jose Vitorio Domiciano Embraer Kurt Doppelbauer TTTech Cheryl Dorsey Digital Flight Rick Dorsey",
    "Digital Flight John Doughty Garmin International Vincent Dovydaitis III \nFoliage Software Systems, Inc.",
    "Georges Duchein DGA \nBranimir Dulic Transport Canada Gilles Dulon SAGEM DS   Safran Group Paul Dunn Northrop Grumman Corporation Andrew Eaton UK CAA \nBrian Eckmann Universal Avionics Systems Corporation Vladimir Eliseev Sukhoi Civil Aircraft Company (SCAC) \nAndrew Elliott Design Assurance Mike Elliott Boeing Company Joao Esteves Critical Software \n\n## Name Organization",
    "Rowland Evans Pratt & Whitney Canada Louis Fabre Eurocopter Martin Fassl Siemens AG \nMichael Fee Aero Engine Controls (Rolls-Royce) \n \nTom Ferrell Ferrell and Associates Consulting Uma Ferrell Ferrell and Associates Consulting Lou Fisk GE Aviation Ade Fountain Penny and Giles Claude Fournier Liebherr Pierre Francine Thales Timothy Frey Honeywell  \nStephen J. Fridrick GE Aviation Leonard Fulcher TTTech",
    "Stephen J. Fridrick GE Aviation Leonard Fulcher TTTech  \nRandall Fulton Seaweed Systems Francoise Gachet Dassault-Aviation Victor Galushkin GosNIIAS \nMarty Gasiorowski Worldwide Certification Services Stephanie Gaudan Thales Jean-Louis Gebel Airbus Dries Geldof BARCO \nDimitri Gianesini Airbus Jim Gibbons Boeing Company Dara Gibson FAA \nGreg Gicca AdaCore Steven Gitelis Lumina Engineering Ian Glazebrook WS Atkins Santiago Golmayo GMV SA",
    "Ben Gorry British Aerospace Systems Florian Gouleau DGA Techniques Aéronautiques Olivier Graff Intertechnique - Zodiac  \nRussell DeLoy Graham Garmin International Robert Green BAE Systems Mark Grindle Systems Enginuity Peter Grossinger Pilatus Aircraft Mark Gulick Solers, Inc.",
    "Pierre Guyot Dassault Aviation Ibrahim Habli University of York Ross Hannan Sigma Associates (Aerospace) Limited Christopher H. Hansen Rockwell Collins Wue Hao Wen Civil Aviation Administration of China (CAAC) \nKeith Harrison HVR Consulting Services Ltd Bjorn Hasselqvist Saab AB \nKevin Hathaway Aero Engine Controls (Goodrich)  \nDavid Hawken NATS \nKelly Hayhurst NASA \nPeter Heath",
    "David Hawken NATS \nKelly Hayhurst NASA \nPeter Heath  \nSecuraplane Technologies Myron Hecht Aerospace Corporation Don Heck Boeing Company Peter Heller Airbus Operations GmbH \nBarry Hendrix Lockheed Martin Michael Hennell LDRA",
    "Name \nOrganization \n\nMichael Herring Rockwell Collins Ruth Hirt FAA \nKent Hollinger Mitre Corp C. Michael Holloway NASA \n\n| Ian Hopkins    | Aero Engine Controls (Rolls-Royce)    |     |\n|----------------|---------------------------------------|-----|\n\nGary Horan FAA \nChris Hote PolySpace Inc. \n\nSusan Houston FAA \nJames Hummell Embedded Plus Dr. James J. Hunt aicas Rebecca L. Hunt Boeing Company",
    "| Stuart Hutchesson    | Aero Engine Controls (Rolls-Royce)    |     |\n|----------------------|---------------------------------------|-----|",
    "Rex Hyde Moog Inc. Aircraft Group Mario Iacobelli Mannarino Systems Melissa Isaacs FAA \nVladimir Istomin Sukhoi Civil Aircraft Company (SCAC) \nStephen A. Jacklin NASA  \nMatt Jaffe Embry-Riddle Aeronautical University Marek Jaglarz Pilatus Aircraft Myles Jalalian FAA  \nMerlin James Garmin International Tomas Jansson Saab AB \nEric Jenn Thales Lars Johannknecht EADS  \nRikard Johansson Saab AB",
    "Eric Jenn Thales Lars Johannknecht EADS  \nRikard Johansson Saab AB \nJohn Jorgensen Universal Avionics Systems Jeffrey Joyce Critical Systems Labs Chris Karis Ensco Gene Kelly CertTech Anne-Cécile Kerbrat Aeroconseil Randy Key FAA \nCharles W. Kilgore II \nFAA \nWayne King Honeywell Daniel Kinney Boeing Company Judith Klein Lockheed Martin Joachim Klichert Diehl Avionik Systeme Jeff Knickerbocker Sunrise Certification & Consulting, Inc.  \nJohn Knight",
    "John Knight  \nUniversity of Virginia Rainer Kollner Verocel Andrew Kornecki Embry-Riddle Aeronautical University Igor Koverninskiy Gos NIIAS \nJim Krodel Pratt & Whitney Paramesh Kunda Pratt & Whitney Canada Sylvie Lacabanne AIRBUS \nGérard Ladier Airbus/Aerospace Valley Ron Lambalot Boeing Company Boris Langer Diehl Aerospace Susanne Lanzerstorfer APAC GesmbH \nGilles Laplane SAGEM DS   Safran Group Jeanne Larsen Hamilton Sundstrand Emmanuel Ledinot Dassault Aviation",
    "Name \nOrganization",
    "Stephane Leriche Thales Hong Leung Bell Helicopter Textron John Lewis FAA \nJohn Li Thales Mark Lillis Goodrich GPECS \n \nBarbara Lingberg FAA \nPierre Lionne EADS APSYS \nHoyt Lougee Foliage Software Systems Howard Lowe GE Aviation Hauke Luethje NewTec GmbH \nJonathan Lynch Honeywell Françoise Magliozzi Atos Origin Veronique Magnier EASA \nKristine Maine Aerospace Corporation Didier Malescot DSNA/DTI \nVarun Malik Hamilton Sundstrand Patrick Mana EUROCONTROL",
    "Varun Malik Hamilton Sundstrand Patrick Mana EUROCONTROL \nJoseph Mangan Coanda Aerospace Software Ghilaine Martinez DGA Techniques Aéronautiques Steven C. Martz Garmin International Peter Matthews Independent Consultant Frank McCormick Certification Services Inc Scott McCoy Harris Corporation Thomas McHugh FAA \nWilliam McMinn Lockheed Martin Josh McNeil US Army AMCOM SED",
    "William McMinn Lockheed Martin Josh McNeil US Army AMCOM SED \nKevin Meier Cessna Aircraft Company Amanda Melles Bombardier Marc Meltzer Belcan Engineering Steven Miller Rockwell Collins Gregory Millican Honeywell  \nJohn Minihan Resource Group Martin Momberg Cassidian Air Systems Pippa Moore UK CAA \nEmilio Mora-Castro EASA \nEndrich Moritz Technical University Robert Morris CDL Systems Ltd.",
    "Allan Terry Morris NASA \nSteve Morton TBV Associates  \nHarold Moses RTCA, Inc. \n\nNadir Mostefat Mannarino Systems Fred B. Moyer Rockwell Collins Robert D. Mumme Embedded Plus Engineering Arun Murthi AERO&SPACE USA \nArmen Nahapetian Teledyne Controls Gerry Ngu EASA \nElisabeth Nguyen Aerospace Corporation Robert Noel Mitre Corp Sven Nordhoff SQS AG \nPaula Obeid Embedded Plus Engineering Eric Oberle Becker Avionics \n\nName \nOrganization",
    "Brenda Ocker FAA \nTorsten Ostermeier Bundeswehr Frederic Painchaud Defence Research and Development Canada Sean Parkinson Resource Group Dennis Patrick Penza AVISTA \nJean-Phillipe Perrot Turbomeca Robin Perry GE Aviation David Petesch Hamilton Sundstrand John Philbin Northrop Grumman Integrated Systems Christophe Piala Thales Avionics Cyril Picard EADS APSYS \nFrancine Pierre Thales Avionics Patrick Pierre Thales Avionics Gerald Pilj FAA \nBenoit Pinta Intertechnique - Zodiac",
    "Benoit Pinta Intertechnique - Zodiac  \nMo Piper Boeing Company Andreas Pistek ITK Engineering AG \nLaurent Plateaux DGA \nLaurent Pomies Independent Consultant Jennifer Popovich Jeppesen Inc.",
    "Clifford Porter Aircell LLC \nFrédéric Pothon ACG Solutions Bill Potter The MathWorks Inc Sunil Prasad HCL Technologies, Chennai, India Paul J. Prisaznuk ARINC-AEEC \nNaim Rahmani  \nTransport Canada Angela Rapaccini ENAC \nLucas Redding Silver-Atena David Redman Aerospace Vehicle Systems Institute (AVSI) \nTammy Reeve Patmos Engineering Services, Inc.  \nGuy Renault SAGEM DS   Safran Group Leanna Rierson Digital Safety Consulting  \nGeorge Romanski Verocel Cyrille Rosay EASA \nEdward Rosenbloom",
    "George Romanski Verocel Cyrille Rosay EASA \nEdward Rosenbloom  \nKollsman, Inc  \nTom Roth Airborne Software Certification Consulting Jamel Rouahi CEAT \nMarielle Roux  \nRockwell Collins France Benedito Massayuki Sakugawa ANAC Brazil Almudena Sanchez GMV SA \nVdot Santhanam Boeing Company Laurence Scales Thales Deidre Schilling Hamilton Sundstrand Ernst Schmidt Bundeswehr  \nPeter Schmitt Universität Karlsruhe Dr. Achim Schoenhoff EADS Military Aircraft Martin Schwarz TT Technologies",
    "Gabriel Scolan SAGEM DS   Safran Group Christel Seguin ONERA \nBeatrice Sereno Teuchos SAFRAN \nPhillip L. Shaffer GE Aviation",
    "## Name Organization",
    "Jagdish Shah Parker Vadim Shapiro TetraTech/AMT \nJean François Sicard DGA Techniques Aéronautiques Marten Sjoestedt Saab AB \nPeter Skaves FAA \nGreg Slater Rockwell Collins Claudine Sokoloff Atos Origin Marge Sonnek Honeywell  \nGuillaume Soudain EASA \nRoger Souter FAA \nRobin L. Sova FAA \nRichard Spencer FAA \nThomas Sperling The Mathworks William StClair LDRA \nRoland Stalford Galileo Industries Spa  \nJerry Stamatopoulous Aircell LLC \nTom Starnes Cessna Aircraft Company Jim Stewart NATS",
    "Jerry Stamatopoulous Aircell LLC \nTom Starnes Cessna Aircraft Company Jim Stewart NATS \nTim Stockton Certon Victor Strachan Northrop-Grumman John Strasburger FAA \nMargarita Strelnikova Sukhoi Civil Aircraft Company (SCAC) \nRonald Stroup FAA \nWill Struck FAA \nWladimir Terzic SAGEM DS   Safran Group Wolfgang Theurer C-S SI \nJoel Thornton Tier5 Inc Mikael Thorvaldsson KnowIT Technowledge  \nBozena Brygida Thrower Hamilton Sundstrand Christophe Travers Dassault Aviation Fay Trowbridge Honeywell",
    "Nick Tudor Tudor Associates Silpa Uppalapati FAA \nMarie-Line Valentin Airbus Jozef Van Baal Civil Aviation Authorities Netherlands John Van Leeuwen Sikorsky Aircraft Aulis Viik NAV Canada Bertrand Voisin Dassault Aviation Katherine Volk L-3 Communications  \nDennis Wallace FAA \nAndy Wallington Bell Helicopter Yunming Wang Esterel Technologies Don Ward AVSI \nSteve Ward Rockwell Collins Patricia Warner Software Engineering Michael Warren Rockwell Collins Rob Weaver NATS",
    "Yu Wei CAA China Terri Weinstein Parker Hannifin Marcus Weiskirchner EADS Military Aircraft Daniel Weisz Sandel Avionics, Inc.",
    "| Name           | Organization                 |\n|----------------|------------------------------|\n| Rich Wendlandt | Quantum3D                    |\n| Michael Whalen | Rockwell Collins             |\n| Paul Whiston   | High Integrity Solutions Ltd |\n\nTodd R. White L-3 Communications/Qualtech Virginie Wiels ONERA",
    "Todd R. White L-3 Communications/Qualtech Virginie Wiels ONERA \n\n| ElRoy Wiens          | Cessna Aircraft Company     |\n|----------------------|-----------------------------|\n| Terrance Williamson  | Jeppesen Inc.               |\n| Graham Wisdom        | BAE Systems                 |\n| Patricia Wojnarowski | Boeing Commercial Airplanes |\n| Joerg Wolfrum        | Diehl Aerospace             |\n\nKurt Woodham NASA",
    "Kurt Woodham NASA \n\n| Cai Yong     | CAAC (Civil Aviation Administration of China)    |\n|--------------|--------------------------------------------------|\n| Edward Yoon  | Curtiss-Wright Controls, Inc                     |\n| Robert Young | Rolls-Royce                                      |\n| William Yu   | CAAC China                                       |\n| Erhan Yuceer | Savunma Teknolojileri Muhendislik ve Ticaret     |\n\nUli Zanker Liebherr"
  ]
}