{
  "source": "RTCA DO-278A 2011.md",
  "chunks": [
    "RTCA, Inc. \n\n1150 18th Street, NW, Suite 910 \nWashington, DC 20036-3816 USA \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n\n# Software Integrity Assurance Considerations For Communication, Navigation, Surveillance And Air Traffic Management (Cns/Atm) Systems\n\n \nCopies of this document may be obtained from RTCA, Inc. \n\nTelephone: 202-833-9339 \nFacsimile: 202-833-9434 \nInternet: www.rtca.org Please visit the RTCA Online Store for document pricing and ordering information. \n\n## Foreword",
    "## Foreword\n\n \nThis report was prepared by RTCA Special Committee 205 (SC-205) and EUROCAE Working Group 71 \n(WG-71) and approved by the RTCA Program Management Committee (PMC) on December 13, 2011.",
    "RTCA, Incorporated is a not-for-profit corporation formed to advance the art and science of aviation and aviation electronic systems for the benefit of the public. The organization functions as a Federal Advisory Committee and develops consensus-based recommendations on contemporary aviation issues. RTCA's objectives include but are not limited to:",
    "- \ncoalescing aviation system user and provider technical requirements in a manner that helps government and industry meet their mutual objectives and responsibilities; \n- \nanalyzing and recommending solutions to the system technical issues that aviation faces as it \ncontinues to pursue increased safety, system capacity, and efficiency; \n-",
    "continues to pursue increased safety, system capacity, and efficiency; \n- \ndeveloping consensus on the application of pertinent technology to fulfill user and provider requirements, including development of minimum operational performance standards for electronic systems and equipment that support aviation; and \n-",
    "- \nassisting in developing the appropriate technical material upon which positions for the International Civil Aviation Organization and the International Telecommunication Union and other appropriate international organizations can be based.",
    "The organization's recommendations are often used as the basis for government and private sector decisions as well as the foundation for many Federal Aviation Administration Technical Standard Orders. Since the RTCA is not an official agency of the United States Government, its recommendations may not be regarded as statements of official government policy unless so enunciated by the U.S. government organization or agency having statutory jurisdiction over any matters to which the",
    "U.S. government organization or agency having statutory jurisdiction over any matters to which the recommendations relate.",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n \n \nCONSENSUS n. Collective opinion or concord; general agreement or accord. [Latin, from consentire, to agree] \n\n## List Of Figures",
    "Figure 1-1 Document Overview\n \n ................................................................................................................... 5\nFigure 2-1 Information Flow Between System and Software Life Cycle Processes\n \n .................................... 9\nFigure 2-2 Sequence of Events for Software Error Leading to a Failure Condition\n \n \n................................... 12\nFigure 3-1 Example of a Software Project Using Four Different Development Sequences",
    "Figure 3-1 Example of a Software Project Using Four Different Development Sequences\n \n \n....................... 24\nFigure 6-1 Software Testing Activities\n \n \n....................................................................................................... 47\nFigure 12-1 Requirements Intersection\n \n \n..................................................................................................... 103",
    "## List Of Tables",
    "Table 2-1 Example Failure Condition Category Descriptions .................................................................... 13 \nTable 2-2 CNS/ATM to Airborne Level Association ................................................................................. 14 \nTable 7-1 SCM Process Activities Associated with CC1 and CC2 Data.................................................... 60",
    "Table 12-1 Tool Qualification Level Determination .................................................................................. 89 \nTable 12-2 COTS Software Planning Objectives ..................................................................................... 110 \nTable 12-3 COTS Software Acqusition Process Objectives \n..................................................................... 111",
    "..................................................................... 111 \nTable 12-4 COTS Software Verification Process Objectives ................................................................... 112 \nTable 12-5 COTS Software Configuration Management Process Objectives .......................................... 112 Table A-1 Software Planning Process\n \n ...................................................................................................... 126",
    "Table A-2 Software Development Processes\n \n \n............................................................................................ 127\nTable A-3 Verification of Outputs of Software Requirements Process\n \n .................................................... 128\nTable A-4 Verification of Outputs of Software Design Process\n \n \n............................................................... 129\nTable A-5 Verification of Outputs of Sofware Coding and Integration Processes",
    "Table A-5 Verification of Outputs of Sofware Coding and Integration Processes\n \n ................................... 130\nTable A-6 Testing of Outputs of Integration Process\n \n ............................................................................... 131\nTable A-7 Verification of Verification Process Results\n \n ........................................................................... 132\nTable A-8 Software Configuration Mangements Process",
    "Table A-8 Software Configuration Mangements Process\n \n \n......................................................................... 133\nTable A-9 Software Quality Assurance Process\n \n ....................................................................................... 134\nTable A-10 Software Approval Process\n \n \n.................................................................................................... 135",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n \n\n## 1.0 Introduction",
    "The implementation of Communication, Navigation, Surveillance, and Air Traffic Management (CNS/ATM) systems has resulted in increased interdependence of systems providing Air Traffic Services (ATS) and systems onboard aircraft. CNS/ATM systems can include ground, airborne, and space-based elements. In order for these systems to perform their intended function while providing an acceptable level of safety, there is a need to define consistent and/or equivalent means of providing integrity",
    "of safety, there is a need to define consistent and/or equivalent means of providing integrity assurance for the software (SW) in these systems. DO-278, \"Software Integrity Assurance Considerations for Communication, Navigation, Surveillance and Air Traffic Management (CNS/ATM)",
    "Systems,\" was written to satisfy the non-airborne (that is, ground and space) aspects of this need. Appendix A provides the background of this document.",
    "## 1.1 Purpose\n\nThe purpose of this document is to provide guidance for the production of non-airborne software for CNS/ATM systems and equipment that performs its intended function with a level of confidence in safety that complies with approval requirements. This guidance includes:",
    "- \nObjectives for software life cycle processes. \n- \nActivities that provide a means for satisfying those objectives. \n- \nDescriptions of the evidence in the form of software life cycle data that indicate that \nthe objectives have been satisfied. \n- \nVariations in the objectives, independence, software life cycle data, and control categories by assurance level (AL). \n- \nAdditional considerations (for example, previously developed software) that are \napplicable to certain applications. \n-",
    "applicable to certain applications. \n- \nDefinition of terms provided in the glossary. \nIn addition to guidance, supporting information is provided to assist the reader's understanding.",
    "## 1.2 Scope",
    "This document discusses those aspects of approval that pertain to the production of software for CNS/ATM systems. In discussing those aspects, the system life cycle and its relationship with the software life cycle is described to aid in the understanding of the approval process. A complete description of the system life cycle processes, including the system safety assessment and validation processes, or the approval process is not intended. The guidance contained in this document does not",
    "or the approval process is not intended. The guidance contained in this document does not define or imply the level of involvement of an approval authority in an approval process. To understand approval authority involvement, the applicant should refer to applicable regulations and guidance material issued by the relevant approval authority.",
    "This document does not attempt to define firmware. Firmware such as embedded microcode or libraries should be classified as hardware or software and addressed by the applicable processes. This document assumes that during the system definition, functions have been allocated to either software or hardware. Other documents exist that provide guidance for development assurance for functions that are allocated to implementation in hardware. This document provides guidance for functions that are",
    "are allocated to implementation in hardware. This document provides guidance for functions that are allocated to software.",
    "Note\n: This allows an efficient method of implementation and development assurance to \nbe determined at the time the system is specified and functions allocated. All parties should agree with this system decision at the time the allocation is made. \nMatters concerning the structure of the applicant's organization, the commercial relationships between the applicant and its suppliers, and personnel qualification criteria are beyond the scope of this document.",
    "## 1.3 Relationship To Other Documents",
    "During the development of this document, various national and international software integrity assurance standards were reviewed. While the means of compliance may differ, no incompatibilities were identified with these standards. These other standards are available through national and international agencies. In some communities, compliance with these standards may be required. However, it is outside the scope of this document to invoke specific national or international standards, or to",
    "is outside the scope of this document to invoke specific national or international standards, or to propose a means by which these standards might be used as an alternative or in addition to this document. It is recognized that projects may be obliged, through contract or other means, to comply with additional standards as applied by, for example, an air navigation service provider,.",
    "Such standards may be derived from general standards produced or adopted by the manufacturer for its activities. Such standards should be considered by the planning process and considered, as appropriate, when applying supplier oversight. \n\n## 1.4 How To Use This Document\n\nThe following points should be noted when using this document:",
    "a. This document is intended to be used by the international CNS/ATM community. To \naid such use, references to specific national regulations and procedures are \nminimized. Instead, generic terms are used. For example, the term \"approval authority\" is used to denote the organization or person granting approval. \nb. This document recognizes that the guidance herein is not mandated by law, but",
    "b. This document recognizes that the guidance herein is not mandated by law, but \nrepresents a consensus of the CNS/ATM community. It also recognizes that alternative methods to the methods described herein may be available to the applicant. For these reasons, the use of words such as \"shall\" and \"must\" is avoided. \nc. If an applicant adopts this document as a means of compliance, the applicant should",
    "c. If an applicant adopts this document as a means of compliance, the applicant should \nsatisfy all applicable objectives. This document should apply to the applicant and any of its suppliers who are involved with any of the software life cycle processes or the outputs of those processes described herein. The applicant is responsible for \noversight of all of its suppliers. \nd. The applicant should plan a set of activities that satisfy the objectives. This document",
    "d. The applicant should plan a set of activities that satisfy the objectives. This document \ndescribes activities for achieving those objectives. The applicant may plan and, subject to the approval of the approval authority, adopt alternative activities to those \ndescribed in this document. The applicant may also plan and conduct additional activities that are determined to be necessary.",
    "e. The applicant should address any additional considerations in its software plans and \nstandards. \nf. The applicant should perform the planned activities and provide evidence as \nindicated in section 11 to substantiate that the objectives have been satisfied. \ng. Explanatory text is included to aid the reader in understanding the topic under",
    "g. Explanatory text is included to aid the reader in understanding the topic under \ndiscussion. For example, section 2 provides information necessary to understand the interaction between the system life cycle and software life cycle. Similarly, section 3 provides a description of the software life cycle and section 10 an overview of the CNS/ATM system approval process. \nh. Section 11 contains the data generally produced to aid the software aspects of the",
    "h. Section 11 contains the data generally produced to aid the software aspects of the \napproval process. The names of the data are denoted in the text by capitalization of \nthe first letter of each word in the name (for example, Source Code). \ni.",
    "the first letter of each word in the name (for example, Source Code). \ni. \nSection 12 discusses additional considerations including guidance for the use of previously developed software, tool qualification, and the use of alternative methods to those described in sections 2 through 11. Section 12 may not apply to every project. \nj.",
    "j. \nAnnex A specifies the applicability of the objectives, activities, and software life cycle data for each assurance level as well as the variation in the independence and control categories for each assurance level. In order to fully understand the guidance, the full body of this document should be considered. \nk. In cases where examples are used to indicate how the guidance might be applied,",
    "k. In cases where examples are used to indicate how the guidance might be applied, \neither graphically or through narrative, the examples are not to be interpreted as the preferred method. In these cases, the examples are considered supporting information. \nl. \nA list of items does not imply the list is all-inclusive. \nm. Notes in this document are supporting information used to provide explanatory",
    "m. Notes in this document are supporting information used to provide explanatory \nmaterial, emphasize a point, or draw attention to related items that are not entirely within context. \nn. Major sections are numbered as X.0 throughout this document. It should be noted \nthat references to an entire section are identified as \"section X\"; whereas, references to the content between section headers X.0 and X.1 are referenced as \"section X.0\".",
    "o. One or more supplements to this document exist and extend the guidance in this \ndocument to a specific technique. Supplements are used in conjunction with this \ndocument and may be used in conjunction with one another. Unless alternatives are",
    "document and may be used in conjunction with one another. Unless alternatives are \nused (see 1.4.i), if a supplement exists for a specific technique, the supplement should be used to add, delete, or otherwise modify objectives, activities, explanatory text, and software life cycle data in this document to address that technique, as defined",
    "appropriately in each supplement. It is the responsibility of the applicant to ensure that the supplement's use is acceptable to the appropriate approval authority. As part \nof the software planning process, the applicant should review all potentially relevant \nsupplements and identify those that will be used. The information in supplements",
    "supplements and identify those that will be used. The information in supplements \nshould be used with and in the same way as this document. Annex C of each supplement identifies how the objectives of this document are revised relative to the specific technique addressed by the supplement.",
    "p. Compliance is achieved when all applicable objectives have been satisfied by \nperforming all planned activities and capturing the related evidence. \nNote: Unlike its predecessor (DO-278), this document is a stand-alone document that is \nnot dependent on DO-178C when being used. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nFigure 1-1\n is a pictorial overview of this document's sections and their relationship to each other.",
    "## System Aspects Relating To Software Development - Section 2",
    "SOFTWARE LIFE CYCLE - SECTION 3\nSOFTWARE LIFE CYCLE PROCESSES\nSOFTWARE PLANNING PROCESS - SECTION 4\nSOFTWARE DEVELOPMENT PROCESSES - SECTION 5\nSOFTWARE REQUIREMENTS PROCESS\nSOFTWARE DESIGN PROCESS\nSOFTWARE VERIFICATION PROCESS - SECTION 6\nSOFTWARE CONFIGURATION MANAGEMENT PROCESS - SECTION 7\nSOFTWARE QUALITY ASSURANCE PROCESS - SECTION 8\nAPPROVAL LIAISON PROCESS - SECTION 9\nOVERVIEW OF CNS/ATM SYSTEM APPROVAL PROCESS - SECTION 10\nSOFTWARE LIFE CYCLE DATA - SECTION 11",
    "OVERVIEW OF CNS/ATM SYSTEM APPROVAL PROCESS - SECTION 10\nSOFTWARE LIFE CYCLE DATA - SECTION 11\nADDITIONAL CONSIDERATIONS - SECTION 12",
    "## 2.0 System Aspects Relating To Software Development\n\nThis section discusses those aspects of the system life cycle processes necessary to understand the software life cycle processes. System life cycle processes can be found in other industry documents. \n\nDiscussed in this section are:",
    "- \nSystem requirements allocation to software (see 2.1). \n- \nThe information flow between the system and software life cycle processes and between the software and hardware life cycle processes (see 2.2). \n- \nThe system safety assessment process, assurance level definitions and assurance level \ndetermination (see 2.3). \n- \nArchitectural considerations (see 2.4). \n- \nAdditional system considerations (see 2.5). \n- \nSoftware considerations in system life cycle processes (see 2.6).",
    "- \nSoftware considerations in system life cycle processes (see 2.6). \nThe term \"system\" in the context of this document refers to CNS/ATM systems and equipment only, not to the wider definition of a system that might include operators, operational procedures, etc.",
    "## 2.1 System Requirements Allocation To Software",
    "As part of the system life cycle processes, system requirements are developed from the system operational requirements and other considerations such as safety-related, security, and performance requirements. The safety-related requirements result from the system safety assessment process, and may include functional, integrity, and reliability requirements, as well as design constraints. Within the safety assessment process, safety-related requirements are defined to ensure the integrity of the",
    "safety assessment process, safety-related requirements are defined to ensure the integrity of the system by specifying the desired immunity from, and system responses to, failure conditions. These requirements are identified for hardware and software to preclude or limit the effects of faults, and may provide fault detection, fault tolerance, fault removal and fault avoidance. The system processes are responsible for the refinement and allocation of system requirements to hardware and/or",
    "are responsible for the refinement and allocation of system requirements to hardware and/or software as determined by the system architecture.",
    "System requirements allocated to software, including safety-related requirements, are developed and refined into software requirements that are verified by the software verification process activities. These requirements and the associated verification should establish that the software performs its intended functions under any foreseeable operating condition. System requirements allocated to software may include:",
    "a. Functional and operational requirements. b. Interface requirements. \nc. Performance requirements. \nd. Safety-related requirements, including safety strategies, design constraints and design \nmethods, such as, partitioning, dissimilarity, redundancy, or safety monitoring. In cases where the system is a component of another system, the requirements and \nfailure conditions for that other system may also form part of the system requirements allocated to software. \ne. Security requirements.",
    "e. Security requirements. \nf. Maintenance requirements. g. Approval requirements, including any applicable approval authority regulations, \nissue papers etc. \nh. Additional requirements needed to aid the system life cycle processes.",
    "## 2.2 Information Flow Between System And Software Life Cycle Processes\n\nFigure 2-1 is an overview of the information flow between system life cycle processes and the software life cycle processes. This information flow includes the system safety aspects. Due to interdependence of the system safety assessment process and the system design process, the flow of information described in these sections is iterative. \n\n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n\n## 2.2.1 Information Flow From System Processes To Software Processes\n\nThe following data is passed to the software life cycle processes by the system processes either as part of the requirements allocation or during the development life cycle:",
    "a. System requirements allocated to software. \nb. System safety objectives. \nc. Assurance level for software components and a description of associated failure \ncondition(s), if applicable. \nd. System description and hardware definition. e. Design constraints, including external interfaces, partitioning requirements etc. \nf. Details of any system activities proposed to be performed as part of the software life",
    "f. Details of any system activities proposed to be performed as part of the software life \ncycle. Note that system requirement validation is not usually part of the software life cycle processes. The system life cycle processes are responsible for assuring any system activities proposed to be performed as part of the software life cycle. \ng. Evidence of the acceptability, or otherwise, of any data provided by the software",
    "g. Evidence of the acceptability, or otherwise, of any data provided by the software \nprocesses on which any activity has been conducted by the system processes. Examples of such activity are the system processes' evaluations of:  \n1. Derived requirements provided by the software processes to determine if there is \nany impact on the system safety assessment and system requirements. \n2. Issues raised by the software processes with respect to the clarification or",
    "2. Issues raised by the software processes with respect to the clarification or \ncorrection of system requirements allocated to software. \nEvidence of software verification activities performed by the system life cycle processes, if any. Any evidence provided by the system processes (see 2.2.1.f and 2.2.1.g) should be considered by the software processes to be Software Verification Results (see 11.14).",
    "## 2.2.2 Information Flow From Software Processes To System Processes",
    "The software life cycle processes analyze the system requirements allocated to software as part of the software requirements process. If such an analysis identifies any system requirements as inadequate or incorrect, the software life cycle processes should capture the issues and refer them to the system processes for resolution. Furthermore, as the software design and implementation evolves, details are added and modifications made that may affect system safety assessment and system",
    "details are added and modifications made that may affect system safety assessment and system requirements. To aid the evaluation of the evolving design and changes to the design, the software life cycle processes should make data available to the system processes including the system safety assessment process. This data will facilitate analyses and evaluations to establish the effect on the system safety assessment and system requirements. It may be advantageous for such analyses and",
    "the system safety assessment and system requirements. It may be advantageous for such analyses and evaluations to be performed jointly by the systems and software processes. Such data includes:",
    "a. Details of derived requirements created during the software life cycle processes. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "b. A description of the software architecture, including software partitioning. \nc. Evidence of system activities performed by the software life cycle processes, if any. \nd. Problem or change documentation, including problems identified in the system \nrequirements allocated to software and identified incompatibilities between the hardware and the software. \ne. Any limitations of use. \nf. Configuration identification and any configuration status constraints.",
    "f. Configuration identification and any configuration status constraints. \ng. Performance, timing, and accuracy characteristics. \nh. Data to facilitate integration of the software into the system. \ni. \nDetails of software verification activities proposed to be performed during system \nverification, if any.",
    "## 2.2.3 Information Flow Between Software Processes And Hardware Processes\n\nData is passed between the software life cycle processes and the hardware life cycle processes either as part of the system requirements allocation or during the development life cycle. Such data includes:",
    "a. All requirements, including derived requirements, needed for hardware/software \nintegration, such as definition of protocols, timing constraints, and addressing schemes for the interface between hardware and software. \nb. Instances where hardware and software verification activities require coordination. \nc. Identified incompatibilities between the hardware and the software. \n\n## 2.3 System Safety Assessment Process And Assurance Level",
    "## 2.3 System Safety Assessment Process And Assurance Level\n\nThis section provides a brief introduction to how the assurance level for software components is determined and how architectural considerations may influence the allocation of an assurance level. It is not the intent of this document to prescribe how these activities are performed; this has to be established and performed as part of the system life cycle processes.",
    "The assurance level of a software component is based upon the contribution of software to potential failure conditions as determined by the system safety assessment process by establishing how an error in a software component relates to the system failure condition(s) and the severity of that failure condition(s). The assurance level establishes the rigor necessary to demonstrate compliance with this document.",
    "Development of software to an assurance level does not imply the assignment of a failure rate for that software. Thus, software reliability rates based on assurance levels cannot be used by the system safety assessment process in the same way as hardware failure rates.",
    "Only partitioned software components (see 2.4.1) can be assigned individual assurance levels by the system safety assessment process. If partitioning between software components cannot be demonstrated, the software components should be viewed as a single component when assigning assurance levels, that is, all components are assigned the assurance level associated with the most severe failure condition to which the software can contribute. The applicant should establish the system safety",
    "condition to which the software can contribute. The applicant should establish the system safety assessment process to be used based on approval authority guidance. The assurance level for each software component of the system should then be assigned based on this process and agreed with the approval authorities.",
    "## 2.3.1 Relationship Between Software Errors And Failure Conditions\n\n.",
    "Figure 2-2 shows a sequence of events in which a software error leads to the failure condition at system level. A software error may be latent and, thus, not immediately produce a failure. This model is intentionally a simple, linear representation. In actual operation the sequence of events that leads from a software error to a failure condition may be complex and not easily represented by a sequence of events as shown in Figure 2-",
    "2 It is important to realize that the likelihood that the software contains an error cannot be quantified in the same way as for random hardware failures.",
    "Architectural considerations (see 2.4) and/or system external factors may also be considered as part of the system safety assessment process when assigning the assurance level to each software component. \n\n## 2.3.2 Assurance Level Definitions\n\nIn broad terms, the consequences of a software error related failure condition at CNS/ATM system of equipment level falls in to two categories:",
    "a. Those failure conditions with a potential impact upon the safety of the airspace (for \nexample, the loss of separation). \nb. Those failure conditions which have no likely consequential impact on the airborne \ndomain or the flying public, but may, for example, contribute to the exposure of ground personnel to safety-related risks and hazards (for example, through the \nanomalous behavior of the software controlling radiation cut-outs for ground-based \nradar systems under maintenance).",
    "radar systems under maintenance). \nFor a complete definition of relevant failure condition categories, the applicant should refer to applicable regulations and guidance material issued by or recognized by the relevant approval authority. The example failure condition categories listed in Table 2-1 \nare valid for large transport aircraft based on established advisory material for the system safety assessment process. They are only included as examples to assist in the use of this document.",
    "## 1 Example Failure Condition Category Descriptions",
    "| Category                                                                |\n|-------------------------------------------------------------------------|\n| Catastrophic                                                            |\n| Failure conditions, which would result in multiple fatalities, usually  |\n| with the loss of the aircraft.                                          |\n| Hazardous                                                               |",
    "| Hazardous                                                               |\n| Failure conditions which would reduce the capability of the aircraft    |\n| or the ability of the flight crew to cope with adverse operating        |\n| conditions to the extent that there would be the following:             |\n| - A large reduction in safety margins or functional capabilities;       |\n| - Physical distress or higher workload such that the flight crew cannot |",
    "| - Physical distress or higher workload such that the flight crew cannot |\n| be relied upon to perform their tasks accurately or completely, or      |\n| - Serious or fatal injury to a relatively small number of the occupants |\n| other than the flight crew.                                             |\nMajor",
    "| other than the flight crew.                                             |\nMajor \nFailure conditions which would reduce the capability of the aircraft or the ability of the crew to cope with adverse operating conditions to the extent that there would be, for example, a significant reduction in safety margins or functional capabilities, a significant increase in crew workload or in conditions impairing crew efficiency, or",
    "discomfort to the flight crew, or physical distress to passengers or cabin crew, possibly including injuries. \nMinor \nFailure conditions which would not significantly reduce aircraft safety, and which involve crew actions that are well within their \ncapabilities. Minor failure conditions may include, for example, a",
    "capabilities. Minor failure conditions may include, for example, a \nslight reduction in safety margins or functional capabilities, a slight increase in crew workload, such as routine flight plan changes, or some physical discomfort to passengers or cabin crew. \nNo Safety Effect \nFailure conditions that would have no effect on safety; for example, \nfailure conditions that would not affect the operational capability of \nthe aircraft or increase crew workload.",
    "## 2.3.3 Assurance Level Definitions\n\nThis document recognizes six assurance levels (AL1 to AL6). For the example failure condition categories listed in section 2.3.2, the relationships between these assurance levels and failure conditions are:",
    "a. Level AL1: Software whose anomalous behavior, as shown by the system safety \nassessment process, would cause or contribute to a failure of a CNS/ATM system function resulting in a catastrophic failure condition for the aircraft. \nb. Level AL2: Software whose anomalous behavior, as shown by the system safety \nassessment process, would cause or contribute to a failure of a CNS/ATM system function resulting in a hazardous failure condition for the aircraft.",
    "c. Level AL3: Software whose anomalous behavior, as shown by the system safety \nassessment process, would cause or contribute to a failure of a CNS/ATM system \nfunction resulting in a major failure condition for the aircraft. \nd. Level AL4 is not associated with any failure condition category. Annex A \nsummarizes those objectives, activities and other attributes to be applied to software \nassigned AL4. \ne. Level AL5: Software whose anomalous behavior, as shown by the system safety",
    "assigned AL4. \ne. Level AL5: Software whose anomalous behavior, as shown by the system safety \nassessment process, would cause or contribute to a failure of a CNS/ATM system function resulting in a minor failure condition for the aircraft. \nf. Level AL6: Software whose anomalous behavior, as shown by the system safety",
    "f. Level AL6: Software whose anomalous behavior, as shown by the system safety \nassessment process, would cause or contribute to a failure of a CNS/ATM system function with no effect on aircraft operational capability or pilot workload. If a software component is determined to be AL6 and this is confirmed by the approval authority, no further guidance contained in this document applies.",
    "The applicant should always consider the appropriate approval guidance and system development considerations for allocation of the assurance level. Nothing precludes the assignment of assurance levels to failure condition categories other than those shown in Table 2-1. Table 2-2 associates the CNS/ATM assurance levels with the airborne software levels.",
    "The definitions of the levels associated in Table 2-2 are not identical, but the assurance is equivalent. \n\nDO-278A Assurance Level \nDO-178C Software Level \nAL1 \nA \n\n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n\n| AL2    | B             |\n|--------|---------------|\n| AL3    | C             |\n| AL4    | No Equivalent |\n| AL5    | D             |\n| AL6    | E             |\n\n \n\n## 2.3.4 Assurance Level Determination",
    "The system safety assessment process determines the assurance level(s) appropriate to the software components of a particular system based upon the failure condition which may result from anomalous behavior of the software. The impact of, both loss of function and malfunction should be analyzed. External factors such as adverse environmental conditions as well as architectural strategies (as described in section 2.4) may be considered when determining the assurance level.",
    "Note\n: The applicant may want to consider planned functionality to be added during \nfuture developments, as well as potential changes in system requirements allocated to software that may result in a higher assurance level. It may be desirable to develop the software to an assurance level higher than that",
    "determined by the system safety assessment process of the original application, since later development of software life cycle data for substantiating a higher assurance level application may be difficult. \nIf the anomalous behavior of a software component contributes to more than one failure condition, then the software component should be assigned the assurance level associated with the most severe failure condition to which the software can contribute, including combined failure conditions.",
    "## Architectural Considerations 2.4\n\nThis section provides information on several architectural strategies that may limit the impact of failures, or detect failures and provide acceptable system responses to contain them. These architectural techniques are typically identified during system design and should not be interpreted as the preferred or required solutions.",
    "A serial implementation is one in which multiple software components are used for a system function such that anomalous behavior of any of the components could produce the failure condition. In this implementation, the software components will have the assurance level associated with the failure condition of that system function.",
    "In implementations where anomalous behavior of two or more partitioned software components is needed in order to cause the failure condition, this may be taken into consideration by the system safety assessment process when assigning the assurance level for these software components. The system safety assessment process needs to establish that sufficient independence exist between software components with respect to both function, that is, high-level requirements and design (for example, common",
    "with respect to both function, that is, high-level requirements and design (for example, common design elements, languages, and tools).",
    "If partitioning and independence between software components cannot be demonstrated, the software components should be viewed as a single software component when assigning assurance levels (that is, all components are assigned the assurance level associated with the most severe failure condition to which the software can contribute). \n\n## 2.4.1 Partitioning",
    "Partitioning is a technique for providing isolation between software components to contain and/or isolate faults and potentially reduce the effort of the software verification process. Partitioning between software components may be achieved by allocating unique hardware resources to each component (that is, only one software component is executed on each hardware platform in a system). Alternatively, partitioning provisions may be made to allow multiple software components to run on the same",
    "partitioning provisions may be made to allow multiple software components to run on the same hardware platform.",
    "Regardless of the method, the following should be ensured for partitioned software components:",
    "a. A partitioned software component should not be allowed to contaminate another \npartitioned software component's code, input/output (I/O), or data storage areas. \nb. A partitioned software component should be allowed to consume shared processor \nresources only during its scheduled period of execution. \nc. Failures of hardware unique to a partitioned software component should not cause \nadverse effects on other partitioned software components.",
    "adverse effects on other partitioned software components. \nd. Any software providing partitioning should have the same or higher assurance level \nas the highest level assigned to any of the partitioned software components. \ne. Any hardware providing partitioning should be assessed by the system safety \nassessment process to ensure that it does not adversely affect safety.",
    "assessment process to ensure that it does not adversely affect safety. \nThe software life cycle processes should address the partitioning design considerations. These include the extent and scope of interactions permitted between the partitioned components and whether the protection is implemented by hardware or by a combination of hardware and software.",
    "## 2.4.2 Multiple-Version Dissimilar Software\n\nMultiple-version dissimilar software is a system design technique that involves producing two or more components of software that provide the same function in a way that may avoid some sources of common errors between the components. Multiple-version dissimilar software is also referred to as multi-version software, multi-version independent software, dissimilar software, N-version programming, or software diversity.",
    "Software life cycle processes completed or activated before dissimilarity is introduced into a development, remain potential error sources. System requirements may specify a hardware configuration that provides for the execution of multiple-version dissimilar software. The degree of dissimilarity, and hence the degree of protection, is not usually measurable.",
    "Probability of loss of system function will increase to the extent that the safety monitoring associated with dissimilar software versions detects actual errors using comparator differences greater than threshold limits. Dissimilar software versions are usually used, therefore, as a means of providing additional protection after the software verification process objectives for the assurance level, as described in section 6, have been satisfied. Dissimilar software verification methods may be",
    "as described in section 6, have been satisfied. Dissimilar software verification methods may be reduced from those used to verify single version software if it can be shown that the resulting potential loss of system function is acceptable as determined by the system safety assessment process.",
    "Verification of multiple-version dissimilar software is discussed in section 12.3.2. \n\n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n\n## 2.4.3 Safety Monitoring",
    "Safety monitoring is a means of protecting against specific failure conditions by directly monitoring a function for failures that would result in a failure condition. Monitoring functions may be implemented in hardware, software, or a combination of hardware and software. Through the use of monitoring techniques, the assurance level of the monitored software may be assigned an assurance level associated with the loss of its related system function. To allow this assignment, there are three",
    "associated with the loss of its related system function. To allow this assignment, there are three important attributes of the monitor that should be determined:",
    "a. Assurance level\n: Safety monitoring software is assigned the assurance level \nassociated with the most severe failure condition category for the monitored function. \nSystem fault coverage\nb. \n: Assessment of the system fault coverage of a monitor ensures \nthat the monitor's design and implementation are such that the faults which it is intended to detect will be detected under all necessary conditions. \nIndependence of function and monitor\nc. \n: The monitor and protective mechanism are",
    "Independence of function and monitor\nc. \n: The monitor and protective mechanism are \nnot rendered inoperative by the same failure that causes the failure condition.",
    "## 2.5 Additional System Considerations 2.5.1 System Communication\n\nAs different hardware and software components communicate with each other, faults in one component have the potential to propagate to other components. Safety analyses should consider data paths through a system to ensure a function of one assurance level does not corrupt data or associated functions of a higher assurance level. CNS/ATM systems development should specifically address fault propagation and data paths.",
    "## 2.5.2 Security\n\nSecurity requirements should be addressed for CNS/ATM systems. Potential conflicts between security and safety requirements should be resolved at the system level. The resultant requirements to be allocated to software should be defined. \n\n## 2.5.3 Adaptability\n\nThe safety considerations of system adaptation should be addressed for CNS/ATM systems.  \n\n## 2.5.4 Cutover (Hot Swapping)",
    "## 2.5.4 Cutover (Hot Swapping)\n\nCNS/ATM systems are sometimes required to run 24 hours per day. Some CNS/ATM systems need provisions for replacing components or software while the system is operational. This approach is commonly referred to as \"cutover\" or \"hot swapping.\" \nCutover considerations include, but are not limited to:",
    "a. Mechanisms for installing and hot swapping software should ensure that appropriate \navailability and integrity levels are maintained. \nb. Assurance should be provided such that the safety objectives defined in a safety \nassessment process have not been compromised during or after cutover. \nc. After a swap, the ability to revert to the previous configuration should be provided. \nd. Compatibility with other system components and the appropriate software version \nshould be maintained.",
    "## 2.5.5 Post-Development Life Cycle\n\nUsers of CNS/ATM systems should address long-term maintenance, spares, retirement, training, etc. While the guidance contained in this document primarily addresses the development of the software for CNS/ATM systems, this does not negate the need to address post-development maintenance and revision activities. \n\n## 2.6 Software Considerations In System Life Cycle Processes",
    "## 2.6 Software Considerations In System Life Cycle Processes\n\nThis section provides an overview of those software-related issues (not necessarily mutually exclusive) that should be considered, as appropriate, by the system life cycle processes: \n\na. Adaptation data items. \nb. User-modifiable software. \nc. Commercial-Off-The-Shelf (COTS) software. \nd. Option-selectable software. \ne. Field-loadable Software. \nf. Software considerations in system verification. \n\n## 2.6.1 Adaptation Data Items",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "Software consists of Executable Object Code and/or data, and can comprise one or more configuration items. A data set or an adaptation data set that influences the behavior of the software without modifying the Executable Object Code and is managed as a separate configuration item is called an adaptation data item. That is, when discussing adaptation data items and Executable Object Code, it is implied that the adaptation data items are not part of the Executable Object Code. An adaptation data",
    "that the adaptation data items are not part of the Executable Object Code. An adaptation data item comprises a structure of individual elements where each element can be assigned a single value. Each element has attributes such as type, range, or set of allowed values. Examples of adaptation data items include configuration tables and databases but not aeronautical data as they are beyond the scope of this document. Adaptation data items may contain data that can:",
    "a. Influence paths executed through the Executable Object Code. \nb. Activate or deactivate software components and functions. \nc. Adapt the software computations to the system configuration. \nd. Be used as computational data. \ne. Establish time and memory partitioning allotments. \nf. Provide initial values to the software component. \ng. Define physical parameters of equipment such as: data rates, transmission frequency \nband, router configuration files, port addresses, etc.",
    "band, router configuration files, port addresses, etc. \nh. Define site specific parameters such as: \n- \nGeographical data:  Latitude, longitude, and elevation, or mask angles \ncorresponding to the location of a fixed asset or reference, for example, radar \nsite, navigation aid, and waypoint. \n- \nEnvironmental data:  Dynamic data associated with the status of resources, \nweather information, and flow constraint information. \n-",
    "weather information, and flow constraint information. \n- \nAirspace data:  Three-dimensional (3-D) volume definitions specifying the \nboundary of airspace components. Examples of airspace data include enroute \nsector boundaries, enroute facility boundaries and approach control facility boundaries. Additional attributes to support specific air traffic control functionality can be associated with airspace data, for example, flight data processing coordination and control.",
    "Depending on how the adaptation data item is to be used in the CNS/ATM system, the following should be addressed:",
    "- \nUser-modifiable software guidance. \n- \nOption-selectable software guidance. In cases where the adaptation data item activates or deactivates functions, the guidance for deactivated code should be addressed as well. \n- \nField-loadable software guidance. Of particular concern is detection of corrupted adaptation data items, as well as incompatibility between the Executable Object Code \nand adaptation data items.",
    "and adaptation data items. \nAn adaptation data item should be assigned the same assurance level as the software component using it.",
    "## 2.6.2 User-Modifiable Software",
    "A user-modifiable component is that part of the software that may be changed by the user within the modification constraints without approval authority review, if the system requirements provide for user modification. A non-modifiable component is that which is not intended to be changed by the user. The potential effects of user modification are determined by the system safety assessment process and used to develop the software requirements, and then, the software verification process",
    "process and used to develop the software requirements, and then, the software verification process activities. Designing for usermodifiable software is discussed further in section 5.2.3. A change that affects the nonmodifiable software, its protection, or the modifiable software boundaries is a software modification and is discussed in section 12.1.1.",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nGuidance for user-modifiable software includes:",
    "a. The user-modifiable software should not adversely affect safety, operational \ncapabilities, CNS/ATM system user workload, or any non-modifiable software \ncomponents or any software protection mechanism used. Unless this can be \nestablished, the software may not be classified as user-modifiable. The safety impact \nof displaying information based on user-modifiable software should also be considered. \nb. When the system requirements provide for user modification, then users may modify",
    "b. When the system requirements provide for user modification, then users may modify \nsoftware within the modification constraints without approval authority review. \nc. The system requirements should specify the mechanisms that prevent the user \nmodification from affecting system safety whether or not they are correctly \nimplemented. The software that provides the protection for user modification should \nbe at the same assurance level as the function it is protecting from errors in the",
    "be at the same assurance level as the function it is protecting from errors in the \nmodifiable component. \nd. If the system requirements do not include provision for user modification, then the \nsoftware should not be modified by the user unless compliance with this document is demonstrated for the modification. \ne. At the time of the user modification, the user should take responsibility for all aspects",
    "e. At the time of the user modification, the user should take responsibility for all aspects \nof the user-modifiable software, for example, software configuration management, software quality assurance, and software verification. \nf. The applicant should provide the necessary information to enable the user to manage \nthe software in such a way that the safety of the CNS/ATM system is not compromised.",
    "## 2.6.3 Commercial-Off-The-Shelf Software\n\nCOTS software included in CNS/ATM systems or equipment should satisfy the objectives of this document. Additional information may be found in section 12.4.",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nIf deficiencies exist in the software life cycle data of COTS software, the data should be augmented to satisfy the objectives of this document. The guidance in section 12.1.4, Upgrading a Development Baseline, and section 12.3.4, Product Service Experience, may be relevant in this instance. \n\n## 2.6.4 Option-Selectable Software",
    "Some CNS/ATM systems and equipment may include optional functions that may be selected by software-programmed options rather than by hardware connector pins. The option-selectable software functions are used to select a particular configuration within the target computer. See sections 4.2h, 5.2.4, and  6.4.4.3d(2) for guidance on deactivated code. When software programmed options are included, a means should be provided to ensure that inadvertent selections involving non-approved configurations",
    "should be provided to ensure that inadvertent selections involving non-approved configurations for the target computer within the installation environment cannot be made.",
    "## 2.6.5 Field-Loadable Software\n\nField-loadable CNS/ATM software refers to software that can be loaded without removing the system or equipment from its installation. The safety-related requirements associated with the software loading function are part of the system requirements. If the inadvertent enabling of the software loading function could induce a system failure condition, then a safety-related requirement for the software loading function is specified in the system requirements.",
    "System safety considerations relating to field-loadable software include:",
    "- \nDetection of corrupted or partially loaded software. \n- \nDetermination of the effects of loading the inappropriate software. \n- \nHardware/software compatibility. \n- \nSoftware/software compatibility. \n- \nCNS/ATM and software compatibility. \n- \nInadvertent enabling of the field-loading function. \n- \nLoss or corruption of the software configuration identification display. \nGuidance for field-loadable software includes:",
    "a. Unless otherwise justified by the system safety assessment process, the detection \nmechanism for partial or corrupted software loads should be assigned the same assurance level as the highest assurance level associated with the function that uses \nthe software load. \nb. If a system recovers to a default mode or safe state upon detection of a corrupted or",
    "b. If a system recovers to a default mode or safe state upon detection of a corrupted or \ninappropriate software load, then each partitioned component of the system should have safety-related requirements specified for recovery to and operation in this mode. \nInterfacing systems may also need to be reviewed for proper operation with the default mode. \nc. The software loading function, including support systems and procedures, should",
    "c. The software loading function, including support systems and procedures, should \ninclude a means to detect incorrect software and/or hardware and/or CNS/ATM combinations and should provide protection appropriate to the failure condition of the function. If the software consists of multiple configuration items their compatibility \nshould be ensured. \nd. If software is part of an CNS/ATM display mechanism that is the means for ensuring",
    "d. If software is part of an CNS/ATM display mechanism that is the means for ensuring \nthat the CNS/ATM system conforms to an approved configuration, then that software \nshould either be developed to the highest assurance level of the software to be loaded, or the system safety assessment process should justify the integrity of an end-to-end \ncheck of the software configuration identification.",
    "## 2.6.6 Software Considerations In System Verification\n\nGuidance for system verification is beyond the scope of this document. However, the software life cycle processes aid and interact with the system verification process and \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nmay be able to satisfy some system verification process objectives. Software design details that relate to the system functionality need to be made available to aid system verification.",
    "## 2.7 System Considerations In Software Life Cycle Processes\n\nCredit may be taken from system life cycle processes for the satisfaction, or partial satisfaction, of the software objectives as defined in this document. In such cases, the system activities for which credit is being sought should be shown to meet the applicable objectives of this document with evidence of the completion of planned activities and their outputs identified as part of the software life cycle data.",
    "## 3.0 Software Life Cycle\n\nThis section discusses the software life cycle processes, software life cycle definition, and transition criteria between software life cycle processes. This document does not prescribe preferred software life cycles and interactions between them. The separation of the processes is not intended to imply a structure for the organization(s) that perform them. For each software product, the software life cycle(s) is constructed that includes these processes.",
    "## 3.1 Software Life Cycle Processes\n\nThe software life cycle processes are:",
    "a. The software planning process that defines and coordinates the activities of the \nsoftware development and integral processes for a project. Section 4 describes the \nsoftware planning process. \nb. The software development processes that produce the software product. These \nprocesses are the software requirements process, the software design process, the software coding process, and the integration process. Section 5 describes the software development processes.",
    "c. The integral processes that ensure the correctness of control of, and confidence in the",
    "software life cycle processes and their outputs. The integral processes are the software verification process, the software configuration management process, the software quality assurance process, and the approval liaison process. It is important to understand that the integral processes are performed concurrently with the software planning and development processes throughout the software life cycle. Sections 6 through 9 describe the integral processes.",
    "## 3.2 Software Life Cycle Definition",
    "A project defines one or more software life cycle(s) by choosing the activities for each process, specifying a sequence for the activities, and assigning responsibilities for the activities. For a specific project, the sequencing of these processes is determined by attributes of the project, such as system functionality and complexity, software size and complexity, requirements stability, use of previously developed software, development strategies, and hardware availability. The usual sequence",
    "developed software, development strategies, and hardware availability. The usual sequence through the software development processes is requirements, design, coding, and integration.",
    "Figure 3-1 illustrates the sequence of software development processes for several components of a single software product with different software life cycles. Component W implements a set of system requirements by developing the software requirements, using those requirements to define a software design, implementing that design into Source Code, and then integrating the component into the hardware. Component X illustrates the use of previously developed software used in a approved system.",
    "Component Y illustrates the use of a simple, partitioned function that can be coded directly from the software requirements. Component Z illustrates the use of a prototyping strategy. Usually, the goals of prototyping are to better understand the software requirements and to mitigate development and technical risks. The initial requirements are used as the basis to implement a prototype. This prototype is evaluated in an environment representative of the intended use of the system under",
    "prototype is evaluated in an environment representative of the intended use of the system under development. Results of the evaluation are used to refine the requirements. The processes of a software life cycle may be iterative, that is, entered and re-entered.",
    "The timing and degree of iteration varies due to the incremental development of system functions, complexity, requirements development, hardware availability, feedback to previous processes, and other attributes of the project. The various parts of the selected software life cycle are tied together with a combination of incremental integration process and software verification process activities. \n\nNote: For simplicity, the software planning and integral processes are not shown.",
    "Note: For simplicity, the software planning and integral processes are not shown.\n\n## 3.3 Transition Criteria Between Processes",
    "Transition criteria are used to determine whether a process may be entered or re-entered. Each software life cycle process performs activities on inputs to produce outputs. A",
    "process may produce feedback to other processes and receive feedback from others. The definition of feedback includes how information is recognized, controlled, and resolved by the receiving process. An example of a feedback is problem reporting. The transition criteria will depend on the planned sequence of software development processes and integral processes, and may be affected by the assurance level. Examples of transition criteria that may be chosen are: the software verification process",
    "level. Examples of transition criteria that may be chosen are: the software verification process reviews",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nhave been performed, the input is an identified configuration item, and a traceability analysis has been completed for the input.",
    "Every input to a process need not be complete before that process can be initiated, if the transition criteria established for the process are satisfied. \n\nIf a process acts on partial inputs, the inputs to the process should be examined to ensure that they meet transition criteria. Also, subsequent inputs to the process should be examined to determine that the previous outputs of the software development and software verification processes are still valid. \n\n## 4.0 Software Planning Process",
    "## 4.0 Software Planning Process\n\nThis section discusses the objectives and activities of the software planning process. This process produces the software plans and standards that direct the software development processes and the integral processes. Table A-1\n of Annex A is a summary of the objectives and outputs of the software planning process by assurance level. \n\n## 4.1 Software Planning Process Objectives",
    "## 4.1 Software Planning Process Objectives\n\nThe purpose of the software planning process is to define the means of producing software that will satisfy its requirements and provide the level of confidence that is consistent with the assurance level. The objectives of the software planning process are:",
    "a. The activities of the software development processes and integral processes of the \nsoftware life cycle that will address the system requirements and assurance level(s) \nare defined (see 4.2). \nb. The software life cycle(s), including the inter-relationships between the processes, \ntheir sequencing, feedback mechanisms, and transition criteria are determined (see 3). \nc. The software life cycle environment, including the methods and tools to be used for",
    "c. The software life cycle environment, including the methods and tools to be used for \nthe activities of each software life cycle process has been selected and defined (see \n4.4). \nd. Additional considerations, such as those discussed in section 12, have been \naddressed, if necessary. \ne. Software development standards consistent with the system safety objectives for the \nsoftware to be produced are defined (see 4.5). \nf. Software plans that comply with sections 4.3 and 11 have been produced.",
    "f. Software plans that comply with sections 4.3 and 11 have been produced. \ng. Development and revision of the software plans are coordinated (see 4.3).",
    "## 4.2 Software Planning Process Activities\n\nEffective planning is a determining factor in producing software that satisfies the guidance of this document. Activities for the software planning process include:",
    "a. The software plans should be developed that provide direction to the personnel \nperforming the software life cycle processes. Also see section 9.1. \nb. The software development standards to be used for the project should be defined or \nselected. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "c. Methods and tools should be chosen that aid error prevention and provide defect \ndetection in the software development processes. \nd. The software planning process should provide coordination between the software \ndevelopment and integral processes to provide consistency among strategies in the software plans. \ne. The means should be specified to revise the software plans as a project progresses. \nf. When multiple-version dissimilar software is used in a system, the software planning",
    "f. When multiple-version dissimilar software is used in a system, the software planning \nprocess should choose the methods and tools to achieve the dissimilarity necessary to satisfy the system safety objectives. \ng. For the software planning process to be complete, the software plans and software \ndevelopment standards should be under change control and reviews of them completed (see 4.6). \nh. If deactivated code is planned, the software planning process should describe how the",
    "h. If deactivated code is planned, the software planning process should describe how the \ndeactivation mechanism and deactivated code will be defined and verified to satisfy \nsystem safety objectives. \ni. \nIf user-modifiable software is planned, related processes, tools, environment, and \ndata items substantiating the design (see 5.2.3) should be specified in the software \nplans and standards. \nj. \nWhen adaptation data items are planned, the following should be addressed:",
    "j. \nWhen adaptation data items are planned, the following should be addressed: \n1. The way that adaptation data items are used. \n2. The assurance level of the adaptation data items. \n3. The processes to develop, verify, and modify adaptation data items, and any \nassociated tool qualification. \n4. Software load control and compatibility. \nk. The software planning process should address any additional considerations that are \napplicable. \nl.",
    "applicable. \nl. \nIf software development activities will be performed by a supplier, planning should address supplier oversight. \nOther software life cycle processes may begin before completion of the software planning process if transition criteria for the specific process activity are satisfied.",
    "## 4.3 Software Plans\n\nThe software plans define the means of satisfying the objectives of this document. They specify the organizations that will perform those activities. The software plans are:",
    "- \nThe Plan for Software Aspects of Approval (see 11.1) serves as the primary means \nfor communicating the proposed development methods to the approval authority for \nagreement, and defines the means of compliance with this document. \n- \nThe Software Development Plan (see 11.2) defines the software life cycle(s),  \nsoftware development environment, and the means by which the development process objectives will be satisfied. \n-",
    "- \nThe Software Verification Plan (see 11.3) defines the means by which the software \nverification process objectives will be satisfied. \n- \nThe Software Configuration Management Plan (see 11.4) defines the means by which \nthe software configuration management process objectives will be satisfied. \n- \nThe Software Quality Assurance Plan (see 11.5) defines the means by which the \nsoftware quality assurance process objectives will be satisfied. \nActivities for the software plans include:",
    "a. The software plans should comply with this document. \nb. The software plans should define the transition criteria for software life cycle \nprocesses by specifying: \n1. The inputs to the process, including feedback from other processes. \n2. Any integral process activities that may be required to act on these inputs. 3. Availability of tools, methods, plans, and procedures. \nc. The software plans should state the procedures to be used to implement software",
    "c. The software plans should state the procedures to be used to implement software \nchanges prior to use on a approved system. Such changes may be as a result of \nfeedback from other processes and may cause a change to the software plans.",
    "## 4.4 Software Life Cycle Environment Planning",
    "Planning for the software life cycle environment defines the methods, tools, procedures, programming languages, and hardware that will be used to develop, verify, control, and produce the software life cycle data (see 11) and software product. Examples of how the software environment chosen can have a beneficial effect on the software include enforcing standards, detecting errors, and implementing error prevention and fault tolerance methods. The software life cycle environment is a potential",
    "error prevention and fault tolerance methods. The software life cycle environment is a potential error source that can contribute to failure conditions. Composition of this software life cycle environment may be influenced by the safety-related requirements determined by the system safety assessment process, for example, the use of dissimilar, redundant components. The goal of error prevention methods is to avoid errors during the software development processes that might contribute to a",
    "methods is to avoid errors during the software development processes that might contribute to a failure condition. The basic principle is to choose requirements development and design methods, tools, and programming languages that limit the opportunity for introducing errors, and verification methods that ensure that errors introduced are detected. The goal of fault tolerance methods is to include safety features in the software design or Source Code to ensure that the software will respond",
    "safety features in the software design or Source Code to ensure that the software will respond correctly to input data errors and prevent output and control errors. The need for error prevention or fault tolerance methods is determined by the system requirements and the system safety assessment process. The considerations presented above may affect:",
    "a. The methods and notations used in the software requirements process and software \ndesign process. \nb. The programming language(s) and methods used in the software coding process. \nc. The software development environment tools. \nd. The software verification and software configuration management tools. \ne. The need for tool qualification (see 12.2). \n\n## 4.4.1 Software Development Environment",
    "The software development environment is a significant factor in the production of high quality software. The software development environment can also adversely affect the production of software in several ways. For example, a compiler could introduce errors by producing a corrupted output or a linker could fail to reveal a memory allocation error that is present. Activities for the selection of software development environment methods and tools include:",
    "a. During the software planning process, the software development environment should \nbe chosen to reduce its potential risk to the software being developed. \nb. The use of tools or combinations of tools and parts of the software development \nenvironment should be chosen to achieve the necessary level of confidence that an error introduced by one part would be detected by another. An acceptable \nenvironment is produced when both parts are consistently used together. This",
    "environment is produced when both parts are consistently used together. This \nselection includes the assessment of the need for tool qualification. \nc. The software verification process activities or software development standards, which \ninclude consideration of the assurance level, should be defined to reduce potential \nsoftware development environment-related errors. \nd. If approval credit is sought for use of the tools in combination, the sequence of",
    "d. If approval credit is sought for use of the tools in combination, the sequence of \noperation of the tools should be specified in the appropriate plan. \ne. If optional features of software tools are chosen for use in a project, the effects of the \noptions should be examined and specified in the appropriate plan. This is especially important for compilers and autocode generators. \nf. Known tool problems and limitations should be assessed and those issues which can",
    "f. Known tool problems and limitations should be assessed and those issues which can \nadversely affect CNS/ATM software should be addressed.",
    "## 4.4.2 Language And Compiler Considerations\n\nUpon successful completion of verification of the software product, the compiler is considered acceptable for that product. For this to be valid, the software verification process needs to consider particular features of the programming language and compiler. The software planning process considers these features when choosing a programming language and planning for verification. Activities include:",
    "a. Some compilers have features intended to optimize performance of the object code. If \nthe test cases give coverage consistent with the assurance level, the correctness of the optimization need not be verified. Otherwise, the impact of these features on structural coverage analysis should be determined. Additional information can be found in section 6.4.4.2. \nb. To implement certain features, compilers for some languages may produce object",
    "b. To implement certain features, compilers for some languages may produce object \ncode that is not directly traceable to the Source Code, for example, initialization, \nbuilt-in error detection, or exception handling (see 6.4.4.2.b). The software planning \nprocess should provide a means to detect this object code and to ensure verification coverage, and should define the means in the appropriate plan. \nc. If a new compiler, linkage editor, or loader version is introduced, or compiler options",
    "c. If a new compiler, linkage editor, or loader version is introduced, or compiler options \nare changed during the software life cycle, previous tests and coverage analyses may \nno longer be valid. The verification planning should provide a means of reverification that is consistent with sections 6 and 12.1.3.",
    "Note\n: Although the compiler is considered acceptable once all of the verification \nobjectives are satisfied, the compiler is only considered acceptable for that product and not necessarily for other products. \n\n## 4.4.3 Software Test Environment",
    "## 4.4.3 Software Test Environment\n\nSoftware test environment planning defines the methods, tools, procedures, and hardware that will be used to test the outputs of the integration process. Testing may be performed using the target computer, a target computer emulator, or a host computer simulator. Activities include:",
    "a. The emulator or simulator may need to be qualified as described in section 12.2. \nb. The differences between the target computer and the emulator or simulator, and the \neffects of these differences on the ability to detect errors and verify functionality, should be considered. Detection of those errors should be provided by the software verification process and specified in the Software Verification Plan. \n\n## 4.5 Software Development Standards",
    "Software development standards define the rules and constraints for the software development processes. The software development standards include the Software Requirements Standards, the Software Design Standards, and the Software Code Standards. The software verification process uses these standards as a basis for evaluating the compliance of actual outputs of a process with intended outputs. Activities for development of the software standards include:",
    "a. The software development standards should comply with section 11. \nb. The software development standards should enable software components of a given \nsoftware product or related set of products to be uniformly designed and implemented. \nc. The software development standards should disallow the use of constructs or \nmethods that produce outputs that cannot be verified or that are not compatible with safety-related requirements.",
    "d. Robustness should be considered in the software development standards. \nNote 1: In developing standards, consideration can be given to previous experience. \nConstraints and rules on development, design, and coding methods can be included to control complexity. Defensive programming practices may be considered to improve robustness. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "Note 2: If allocated to software by system requirements, practices to detect and control \nerrors in stored data, and refresh and monitor hardware status and configuration may be used to mitigate single event upsets. \n\n## 4.6 Review Of The Software Planning Process\n\nReviews of the software planning process are conducted to ensure that the software plans and software development standards comply with the guidance of this document and means are provided to execute them. Activities include:",
    "a. Methods should be chosen that enable the objectives of this document to be satisfied. \nb. Software life cycle processes should be able to be applied consistently. c. Each process should produce evidence that its outputs can be traced to its activity and \ninputs, showing the degree of independence of the activity, the environment, and the methods to be used. \nd. The outputs of the software planning process should be consistent and comply with \nsection 11.",
    "d. The outputs of the software planning process should be consistent and comply with \nsection 11. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "## 5.0 Software Development Processes\n\nThis section discusses the objectives and activities of the software development processes. The software development processes are applied as defined by the software planning process (see 4) and the Software Development Plan (see 11.2). Table A-2\n of Annex A is a summary of the objectives and outputs of the software development processes by assurance level. The software development processes are:",
    "- \nSoftware requirements process. \n- \nSoftware design process. \n- \nSoftware coding process. \n- \nIntegration process. \nSoftware development processes produce one or more levels of software requirements.",
    "High-level requirements are produced directly through analysis of system requirements and system architecture. Usually, these high-level requirements are further developed during the software design process, thus producing one or more successive, lower levels of requirements. However, if Source Code is generated directly from high-level requirements, then the high-level requirements are also considered low-level requirements and the guidance for low-level requirements also apply.",
    "Note\n: The applicant may be required to justify software development processes that produce a single level of requirements.",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nThe development of software architecture involves decisions made about the structure of the software. During the software design process, the software architecture is defined and low-level requirements are developed. Low-level requirements are software requirements from which Source Code can be directly implemented without further information.",
    "Each software development process may produce derived requirements. Some examples of requirements that might be determined to be derived requirements are:",
    "- \nThe need for interrupt handling software to be developed for the chosen target computer. \n- \nThe specification of a periodic monitor's iteration rate when not specified by the \nsystem requirements allocated to software. \n- \nThe addition of scaling limits when using fixed point arithmetic.",
    "- \nThe addition of scaling limits when using fixed point arithmetic. \nHigh-level requirements may include derived requirements, and low-level requirements may include derived requirements. In order to determine the effects of derived requirements on the system safety assessment and system requirements, all derived requirements should be made available to the system processes including the system safety assessment process.",
    "## 5.1 Software Requirements Process\n\nThe software requirements process uses the outputs of the system life cycle processes to develop the high-level requirements. These high-level requirements include functional, performance, interface, and safety-related requirements. \n\n## 5.1.1 Software Requirements Process Objectives\n\nThe objectives of the software requirements process are:",
    "The objectives of the software requirements process are: \n\na. High-level requirements are developed. \nb. Derived high-level requirements are defined and provided to the system processes, \nincluding the system safety assessment process. \n\n## 5.1.2 Software Requirements Process Activities",
    "Inputs to the software requirements process include the system requirements, the hardware interface and system architecture (if not included in the requirements, from the system life cycle process), and the Software Development Plan and the Software Requirements Standards from the software planning process. When the planned transition criteria have been satisfied, these inputs are used to develop the high-level requirements. The primary output of this process is the Software Requirements Data",
    "the high-level requirements. The primary output of this process is the Software Requirements Data (see 11.9).",
    "The software requirements process is complete when its objectives and the objectives of the integral processes associated with it are satisfied. Activities for this process include:",
    "a. The system functional and interface requirements that are allocated to software \nshould be analyzed for ambiguities, inconsistencies, and undefined conditions. \nb. Inputs to the software requirements process detected as inadequate or incorrect \nshould be reported as feedback to the input source processes for clarification or correction. \nc. Each system requirement that is allocated to software should be specified in the highlevel requirements.",
    "d. High-level requirements that address system requirements allocated to software to \npreclude system hazards should be defined. \ne. The high-level requirements should conform to the Software Requirements \nStandards, be verifiable, and be consistent. \nf. The high-level requirements should be stated in quantitative terms with tolerances \nwhere applicable. \ng. The high-level requirements should not describe design or verification detail except \nfor specified and justified design constraints.",
    "for specified and justified design constraints. \nh. Derived high-level requirements and the reason for their existence should be defined. \ni. \nDerived high-level requirements should be provided to the system processes, \nincluding the system safety assessment process. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "j. \nIf adaptation data items are planned, the high-level requirements should describe how \nany adaptation data item is used by the software. The high-level requirements should also specify their structure, the attributes for each of their data elements, and, when \napplicable, the value of each element. The values of the adaptation data item \nelements should be consistent with the structure of the adaptation data item and the \nattributes of its data elements. \n\n## 5.2 Software Design Process",
    "## 5.2 Software Design Process\n\nThe high-level requirements are refined through one or more iterations in the software design process to develop the software architecture and the low-level requirements that can be used to implement Source Code. \n\n## 5.2.1 Software Design Process Objectives\n\nThe objectives of the software design process are:",
    "## 5.2.1 Software Design Process Objectives\n\nThe objectives of the software design process are: \n\na. The software architecture and low-level requirements are developed from the highlevel requirements. \nb. Derived low-level requirements are defined and provided to the system processes, \nincluding the system safety assessment process. \n\n## 5.2.2 Software Design Process Activities",
    "## 5.2.2 Software Design Process Activities\n\nThe software design process inputs are the Software Requirements Data, the Software Development Plan, and the Software Design Standards. When the planned transition criteria have been satisfied, the high-level requirements are used in the design process to develop software architecture and low-level requirements. This may involve one or more lower levels of requirements.",
    "The primary output of the process is the Design Description (see 11.10) which includes the software architecture and the low-level requirements. \n\nThe software design process is complete when its objectives and the objectives of the integral processes associated with it are satisfied. Activities for this process include:",
    "a. Low-level requirements and software architecture developed during the software \ndesign process should conform to the Software Design Standards and be traceable, verifiable, and consistent. \nb. Derived low-level requirements and the reason for their existence should be defined \nand analyzed to ensure that the higher level requirements are not compromised. \nc. Software design process activities could introduce possible modes of failure into the",
    "c. Software design process activities could introduce possible modes of failure into the \nsoftware or, conversely, preclude others. The use of partitioning or other architectural \nmeans in the software design may alter the assurance level assignment for some components of the software. In such cases, additional data should be defined as derived requirements and provided to the system processes, including the system safety assessment process.",
    "d. Interfaces between software components, in the form of data flow and control flow, \nshould be defined to be consistent between the components. \ne. Control flow and data flow should be monitored when safety-related requirements \ndictate, for example, watchdog timers, reasonableness-checks, and cross-channel comparisons. \nf. Responses to failure conditions should be consistent with the safety-related \nrequirements.",
    "f. Responses to failure conditions should be consistent with the safety-related \nrequirements. \ng. Inadequate or incorrect inputs detected during the software design process should be \nprovided to the system life cycle processes, the software requirements process, or the software planning process as feedback for clarification or correction. \nNote\n: The current state of software engineering does not permit a quantitative",
    "Note\n: The current state of software engineering does not permit a quantitative \ncorrelation between complexity and the attainment of system safety objectives. While no objective guidance can be provided, the software design process should avoid introducing complexity because as the complexity of software increases, it becomes more difficult to verify the design and to show that the safety-related requirements are satisfied.",
    "## 5.2.3 Designing For User-Modifiable Software",
    "User-modifiable software is designed to be modified by its users. A modifiable component is that part of the software that is intended to be changed by the user and a non-modifiable component is that which is not intended to be changed by the user. Usermodifiable software may vary in complexity. Examples include a single memory bit used to select one of two equipment options, a table of messages, or a memory area that can be programmed, compiled, and linked for maintenance functions. Software",
    "or a memory area that can be programmed, compiled, and linked for maintenance functions. Software of any level can include a modifiable component.",
    "The activities for user-modifiable software include:",
    "a. The non-modifiable component should be protected from the modifiable component \nto prevent interference in the safe operation of the non-modifiable component. This protection can be enforced by hardware, by software, by the tools used to make the change, or by a combination of the three. If the protection is provided by software, it should be designed and verified at the same assurance level as the non-modifiable",
    "software. If the protection is provided by a tool, the tool should be categorized and qualified as defined in section 12.2. \nb. The means provided to change the modifiable component should be shown to be the \nonly means by which the modifiable component can be changed. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "## 5.2.4 Designing For Deactivated Code\n\nSystems or equipment may be designed to include several configurations, not all of which are intended to be used in every application. This can lead to deactivated code that cannot be executed, such as unselected functionality or unused library functions, or data that is not used. Deactivated code differs from dead code. The activities for deactivated code include:",
    "a. A mechanism should be designed and implemented to assure that deactivated \nfunctions or components have no adverse impact on active functions or components. \nb. Evidence should be available that the deactivated code is disabled for the \nenvironments where its use is not intended. Unintended execution of deactivated \ncode due to abnormal system conditions is the same as unintended execution of activated code.",
    "c. The development of deactivated code, like the development of the active code, should \ncomply with the objectives of this document. \n\n## 5.3 Software Coding Process\n\nIn the software coding process, the Source Code is implemented from the software architecture and the low-level requirements. \n\nNote\n: \nFor the purpose of this document, compiling, linking, and loading are dealt with \nunder the Integration Process (see 5.4). \n\n## 5.3.1 Software Coding Process Objectives",
    "## 5.3.1 Software Coding Process Objectives\n\nThe objective of the software coding process is: \n\na. Source Code is developed from the low-level requirements. \n\n## 5.3.2 Software Coding Process Activities",
    "## 5.3.2 Software Coding Process Activities\n\nThe coding process inputs are the low-level requirements and software architecture from the software design process, the Software Development Plan, and the Software Code Standards. The software coding process may be entered or re-entered when the planned transition criteria are satisfied. The Source Code is produced by this process based upon the software architecture and the low-level requirements.",
    "The primary output of this process is Source Code (see 11.11). \n\nThe software coding process is complete when its objectives and the objectives of the integral processes associated with it are satisfied. Activities for this process include:",
    "a. The Source Code should implement the low-level requirements and conform to the \nsoftware architecture. \nb. The Source Code should conform to the Software Code Standards. \nc. Inadequate or incorrect inputs detected during the software coding process should be \nprovided to the software requirements process, software design process, and/or \nsoftware planning process as feedback for clarification or correction.",
    "software planning process as feedback for clarification or correction. \nd. Use of autocode generators should conform to the constraints defined in the planning \nprocess.",
    "## 5.4 Integration Process\n\nThe target computer and the Source Code from the software coding process are used with the compiling, linking, and loading data (see 11.16) in the integration process to develop the integrated system or equipment. \n\n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n\n## 5.4.1 Integration Process Objectives\n\nThe objective of the integration process is:",
    "## 5.4.1 Integration Process Objectives\n\nThe objective of the integration process is: \n\na. The Executable Object Code and its associated Adaptation Data Item Files, if any, \nare produced and loaded into the target hardware for hardware/software integration. \n\n## 5.4.2 Integration Process Activities",
    "## 5.4.2 Integration Process Activities\n\nThe integration process consists of software integration and hardware/software integration. The integration process may be entered or re-entered when the planned transition criteria have been satisfied. The integration process inputs are the software architecture from the software design process, and the Source Code from the software coding process.",
    "The outputs of the integration process are the object code; Executable Object Code (see section 11.12); Adaptation Data Item File (see 11.22); and the compiling, linking, and loading data (see 11.11). The integration process is complete when its objectives and the objectives of the integral processes associated with it are satisfied. Activities for this process include:",
    "a. The object code and Executable Object Code should be generated from the Source \nCode and compiling, linking, and loading data. Any Adaptation Data Item File \nshould be generated. \nb. Software integration should be performed on a host computer, a target computer \nemulator, or the target computer. \nc. The software should be loaded into the target computer for hardware/software \nintegration. \nd. Inadequate or incorrect inputs detected during the integration process should be",
    "integration. \nd. Inadequate or incorrect inputs detected during the integration process should be \nprovided to the software requirements process, the software design process, the software coding process, or the software planning process as feedback for clarification or correction. \ne. Patches should not be used in software submitted for use in an approved system to",
    "e. Patches should not be used in software submitted for use in an approved system to \nimplement changes in requirements or architecture, or changes found necessary as a result of the software verification process activities. Patches may be used on a limited, case-by-case basis, for example, to resolve known deficiencies in the software development environment such as a known compiler problem. \nf. When a patch is used, these should be available:",
    "f. When a patch is used, these should be available: \n1. Confirmation that the software configuration management process can effectively \ntrack the patch. \n2. An analysis to provide evidence that the patched software satisfies all the \napplicable objectives. \n3. Justification in the Software Accomplishment Summary for use of a patch.",
    "## 5.5 Software Development Process Traceability\n\nSoftware development process traceability activities include:",
    "a. Trace Data, showing the bi-directional association between system requirements \nallocated to software and high-level requirements, is developed. The purpose of this \nTrace Data is to: \n1. Enable verification of the complete implementation of the system requirements \nallocated to software. \n2. Give visibility to those derived high-level requirements that are not directly \ntraceable to system requirements. \nb. Trace Data, showing the bi-directional association between the high-level",
    "b. Trace Data, showing the bi-directional association between the high-level \nrequirements and low-level requirements is developed. The purpose of this Trace Data is to: \n1. Enable verification of the complete implementation of the high-level \nrequirements. \n2. Give visibility to those derived low-level requirements that are not directly \ntraceable to high-level requirements and to the architectural design decisions \nmade during the software design process.",
    "made during the software design process. \nc. Trace Data, showing the bi-directional association between low-level requirements \nand Source Code, is developed. The purpose of this Trace Data is to: \n1. Enable verification that no Source Code implements an undocumented function. \n2. Enable verification of the complete implementation of the low-level \nrequirements.",
    "## 6.0 Software Verification Process",
    "This section discusses the objectives and activities of the software verification process. Verification is a technical assessment of the outputs of the software planning process, software development processes, and the software verification process. The software verification process is applied as defined by the software planning process (see 4) and the Software Verification Plan (see 11.3). See 4.6 for the verification of the outputs of the planning process.",
    "Verification is not simply testing. Testing, in general, cannot show the absence of errors. As a result, the following sections use the term \"verify\" instead of \"test\" to discuss the software verification process activities, which are typically a combination of reviews, analyses, and tests. \n\nTables A-3 through A-7 of Annex A contain a summary of the objectives and outputs of the software verification process, by assurance level. \n\nNote\n: For lower assurance levels, less emphasis is on:",
    "- Verification of Source Code. - Verification of low-level requirements. \n- Verification of the software architecture. - Degree of test coverage. - Control of verification procedures. \n- Independence of software verification process activities. \n- Overlapping software verification process activities, that is, multiple \nverification activities, each of which may be capable of detecting the same class of error. \n- Robustness testing.",
    "- Robustness testing. \n- Verification activities with an indirect effect on error prevention or detection, \nfor example, conformance to software development standards.",
    "## 6.1 Purpose Of Software Verification\n\nThe purpose of the software verification process is to detect and report errors that may have been introduced during the software development processes. Removal of the errors is an activity of the software development processes. The software verification process verifies that:",
    "a. The system requirements allocated to software have been developed into high-level \nrequirements that satisfy those system requirements. \nb. The high-level requirements have been developed into software architecture and lowlevel requirements that satisfy the high-level requirements. If one or more levels of software requirements are developed between high-level requirements and low-level",
    "requirements, the successive levels of requirements are developed such that each successively lower level satisfies its higher level requirements. If code is generated directly from high-level requirements, this does not apply.",
    "c. The software architecture and low-level requirements have been developed into \nSource Code that satisfies the low-level requirements and software architecture. \nd. The Executable Object Code satisfies the software requirements,(that is, intended \nfunction) and provides confidence in the absence of unintended functionality. \ne. The Executable Object Code is robust with respect to the software requirements such \nthat it can respond correctly to abnormal inputs and conditions.",
    "that it can respond correctly to abnormal inputs and conditions. \nf. The means used to perform this verification are technically correct and complete for \nthe assurance level.",
    "## 6.2 Overview Of Software Verification Process Activities",
    "Software verification process objectives are satisfied through a combination of reviews, analyses, the development of test cases and procedures, and the subsequent execution of those test procedures. Reviews and analyses provide an assessment of the accuracy, completeness, and verifiability of the software requirements, software architecture, and Source Code. The development of test cases and procedures may provide further assessment of the internal consistency and completeness of the",
    "and procedures may provide further assessment of the internal consistency and completeness of the requirements. The execution of the test procedures provides a demonstration of compliance with the requirements. The inputs to the software verification process include the system requirements, the software requirements, software architecture, Trace Data, Source Code, Executable Object Code, and the Software Verification Plan.",
    "The outputs of the software verification process are recorded in the Software Verification Cases and Procedures (see 11.13), the Software Verification Results (see 11.14), and the associated Trace Data (see 11.21).",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nThe need for the requirements to be verifiable once they have been implemented in the software may itself impose additional requirements or constraints on the software development processes. Software verification considerations include:",
    "a. If the code tested is not identical to the CNS/ATM software, those differences should \nbe specified and justified. \nb. When it is not possible to verify specific software requirements by exercising the \nsoftware in a realistic test environment, other means should be provided and their justification for satisfying the software verification process objectives defined in the \nSoftware Verification Plan or Software Verification Results.",
    "Software Verification Plan or Software Verification Results. \nc. Deficiencies and errors discovered during the software verification process should be \nreported to other software life cycle development processes for clarification and \ncorrection as applicable. \nd. Reverification should be conducted following corrective actions and/or changes that \ncould impact the previously verified functionality. Reverification should ensure that \nthe modification has been correctly implemented.",
    "the modification has been correctly implemented. \ne. Verification independence is achieved when the verification activity is performed by \na person(s) other than the developer of the item being verified. A tool may be used to achieve equivalence to the human verification activity. For independence, the person who created a set of low-level requirements-based test cases should not be the same \nperson who developed the associated Source Code from those low-level \nrequirements.",
    "## 6.3 Software Reviews And Analyses",
    "Reviews and analyses are applied to the outputs of the software development processes. One distinction between reviews and analyses is that analyses provide repeatable evidence of correctness and reviews provide a qualitative assessment of correctness. A",
    "review may consist of an inspection of an output of a process guided by a checklist or similar aid. An analysis may examine in detail the functionality, performance, traceability, and safety implications of a software component, and its relationship to other components within the system or equipment.",
    "There may be cases where the verification objectives described in this section cannot be completely satisfied via reviews and analyses alone. In such cases those verification objectives may be satisfied with additional testing of the software product. For example, a combination of reviews, analyses, and tests may be developed to establish the worstcase execution time or verification of the stack usage. \n\n## 6.3.1 Reviews And Analyses Of High-Level Requirements",
    "## 6.3.1 Reviews And Analyses Of High-Level Requirements\n\nThese review and analysis activities detect and report requirements errors that may have been introduced during the software requirements process. These review and analysis activities confirm that the high-level requirements satisfy these objectives:",
    "a. Compliance with system requirements\n:  The objective is to ensure that the system \nfunctions to be performed by the software are defined; that the functional, \nperformance, and safety-related requirements of the system are satisfied by the highlevel requirements; and that derived requirements and the reason for their existence \nare correctly defined. \nAccuracy and consistency\nb. \n:  The objective is to ensure that each high-level",
    "Accuracy and consistency\nb. \n:  The objective is to ensure that each high-level \nrequirement is accurate, unambiguous, and sufficiently detailed, and that the requirements do not conflict with each other. \nCompatibility with the target computer\nc. \n:  The objective is to ensure that no conflicts \nexist between the high-level requirements and the hardware/software features of the target computer, especially system response times and input/output hardware. \nVerifiability\nd.",
    "Verifiability\nd. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    ":  The objective is to ensure that each high-level requirement can be \nverified. \ne. \nConformance to standards:  The objective is to ensure that the Software \nRequirements Standards were followed during the software requirements process and that deviations from the standards are justified. \nf. Traceability\n:  The objective is to ensure that the functional, performance, and safetyrelated requirements of the system that are allocated to software were developed into the high-level requirements.",
    "Algorithm aspects\ng. \n:  The objective is to ensure the accuracy and behavior of the \nproposed algorithms, especially in the area of discontinuities.",
    "## 6.3.2 Reviews And Analyses Of Low-Level Requirements\n\nThese review and analysis activities detect and report requirements errors that may have been introduced during the software design process. These review and analysis activities confirm that the low-level requirements satisfy these objectives:",
    "a. Compliance with high-level requirements:\n  The objective is to ensure that the lowlevel requirements satisfy the high-level requirements and that derived requirements and the design basis for their existence are correctly defined. \nAccuracy and consistency\nb. \n:  The objective is to ensure that each low-level \nrequirement is accurate and unambiguous, and that the low-level requirements do not conflict with each other. \nCompatibility with the target computer\nc.",
    "Compatibility with the target computer\nc. \n:  The objective is to ensure that no conflicts \nexist between the low-level requirements and the hardware/software features of the target computer, especially the use of resources such as bus loading, system response times, and input/output hardware. \nVerifiability\nd. \n:  The objective is to ensure that each low-level requirement can be \nverified. \nConformance to standards:\ne. \n  The objective is to ensure that the Software Design",
    "verified. \nConformance to standards:\ne. \n  The objective is to ensure that the Software Design \nStandards were followed during the software design process and that deviations from the standards are justified. \nTraceability\nf. \n:  The objective is to ensure that the high-level requirements and derived \nrequirements were developed into the low-level requirements. \nAlgorithm aspects\ng. \n:  The objective is to ensure the accuracy and behavior of the",
    "Algorithm aspects\ng. \n:  The objective is to ensure the accuracy and behavior of the \nproposed algorithms, especially in the area of discontinuities.",
    "## 6.3.3 Reviews And Analyses Of Software Architecture\n\nThese review and analysis activities detect and report errors that may have been introduced during the development of the software architecture. These review and analysis activities confirm that the software architecture satisfies these objectives:",
    "a. Compatibility with the high-level requirements\n:  The objective is to ensure that the \nsoftware architecture does not conflict with the high-level requirements, especially \nfunctions that ensure system integrity, for example, partitioning schemes. \nb. \nConsistency:  The objective is to ensure that a correct relationship exists between the",
    "b. \nConsistency:  The objective is to ensure that a correct relationship exists between the \ncomponents of the software architecture. This relationship exists via data flow and control flow. If the interface is to a component of a lower assurance level, it should also be confirmed that the higher assurance level component has appropriate \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nprotection mechanisms in place to protect itself from potential erroneous inputs from the lower assurance level component.",
    "c. Compatibility with the target computer\n:  The objective is to ensure that no conflicts \nexist, especially initialization, asynchronous operation, synchronization, and \ninterrupts, between the software architecture and the hardware/software features of the target computer. \nVerifiability\nd. \n:  The objective is to ensure that the software architecture can be verified, \nfor example, there are no unbounded recursive algorithms. \nConformance to standards\ne.",
    "for example, there are no unbounded recursive algorithms. \nConformance to standards\ne. \n:  The objective is to ensure that the Software Design \nStandards were followed during the software design process and that deviations to the standards are justified, for example, deviations to complexity restriction and design construct rules. \nPartitioning integrity\nf. \n:  The objective is to ensure that partitioning breaches are \nprevented.",
    "## 6.3.4 Reviews And Analyses Of Source Code\n\nThese review and analysis activities detect and report errors that may have been introduced during the software coding process. Primary concerns include correctness of the code with respect to the software requirements, the software architecture, and conformance to the Software Code Standards. These review and analysis activities are usually confined to the Source Code and confirm that the Source Code satisfies these objectives:",
    "a. Compliance with the low-level requirements\n:  The objective is to ensure that the \nSource Code is accurate and complete with respect to the low-level requirements and that no Source Code implements an undocumented function. \nCompliance with the software architecture\nb. \n:  The objective is to ensure that the Source \nCode matches the data flow and control flow defined in the software architecture. \nVerifiability\nc. \n:  The objective is to ensure the Source Code does not contain statements",
    "Verifiability\nc. \n:  The objective is to ensure the Source Code does not contain statements \nand structures that cannot be verified and that the code does not have to be altered to test it. \nConformance to standards\nd. \n:  The objective is to ensure that the Software Code",
    "Conformance to standards\nd. \n:  The objective is to ensure that the Software Code \nStandards were followed during the development of the code, for example, complexity restrictions and code constraints. Complexity includes the degree of coupling between software components, the nesting levels for control structures, and the complexity of logical or numeric expressions. This analysis also ensures that deviations to the standards are justified. \nTraceability\ne.",
    "Traceability\ne. \n:  The objective is to ensure that the low-level requirements were \ndeveloped into Source Code. \nf. \nAccuracy and consistency:  The objective is to determine the correctness and \nconsistency of the Source Code, including stack usage, memory usage, fixed point \narithmetic overflow and resolution, floating-point arithmetic, resource contention and",
    "arithmetic overflow and resolution, floating-point arithmetic, resource contention and \nlimitations, worst-case execution timing, exception handling, use of uninitialized variables, cache management, unused variables, and data corruption due to task or \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\ninterrupt conflicts. The compiler (including its options), the linker (including its options), and some hardware features may have an impact on the worst-case execution timing and this impact should be assessed.",
    "## 6.3.5 Reviews And Analyses Of The Outputs Of The Integration Process\n\nThese review and analysis activities detect and report errors that may have been introduced during the integration process. The objective is to: \n\na. Ensure that the outputs of the integration process are complete and correct. \nActivities include conducting a detailed examination of the compiling, linking and loading data, and memory map. Typical examples of potential errors include:",
    "- \nCompiler warnings. \n- \nIncorrect hardware addresses. \n- \nMemory overlaps. \n- \nMissing software components. \n\n## 6.4 Software Testing\n\nSoftware testing is used to demonstrate that the software satisfies its requirements and to demonstrate with a high degree of confidence that errors that could lead to unacceptable failure conditions, as determined by the system safety assessment process, have been removed.",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nThe objectives of software testing are to execute the software to confirm that:",
    "a. The Executable Object Code complies with the high-level requirements. b. The Executable Object Code is robust with the high-level requirements. \nc. The Executable Object Code complies with the low-level requirements. \nd. The Executable Object Code is robust with the low-level requirements. \ne. The Executable Object Code is compatible with the target computer. \nFigure 6-1",
    "e. The Executable Object Code is compatible with the target computer. \nFigure 6-1\n is a diagram of the software testing activities that may be used to achieve the the software testing objectives. The diagram shows three types of testing, which are:",
    "Hardware/software integration testing:\n- \n  To verify correct operation of the software in \nthe target computer environment. \nSoftware integration testing\n- \n:  To verify the interrelationships between software \nrequirements and components and to verify the implementation of the software requirements and software components within the software architecture. \nLow-level testing:  To verify the implementation of low-level requirements. \n- \nNote",
    "Low-level testing:  To verify the implementation of low-level requirements. \n- \nNote\n: If a test case and its corresponding test procedure are developed and executed for \nhardware/software integration testing or software integration testing, and satisfy the requirements-based coverage and structural coverage, it is not necessary to duplicate the test for low-level testing. Substituting nominally equivalent low-level",
    "tests for high-level tests may be less effective due to the reduced amount of overall functionality tested.",
    "## 6.4.1 Test Environment\n\nMore than one test environment may be needed to satisfy the objectives for software testing. A preferred test environment includes the software loaded into the target computer and tested in an environment that closely resembles the behavior of the target computer environment.",
    "Note\n: In many cases, the requirements-based coverage and structural coverage \nnecessary can be achieved only with more precise control and monitoring of the test inputs and code execution than generally possible in a fully integrated environment. Such testing may need to be performed on a small software component that is functionally isolated from other software components.",
    "Approval credit may be given for testing done using a target computer emulator or a host computer simulator. Activities related to the test environment include:",
    "a. Selected tests should be performed in the integrated target computer environment, \nsince some errors are only detected in this environment. \n\n## 6.4.2 Requirements-Based Test Selection\n\nRequirements-based testing is emphasized because this strategy has been found to be the most effective at revealing errors. Activities for requirements-based test selection include:",
    "a. Specific test cases should be developed to include normal range test cases and \nrobustness (abnormal range) test cases. \nb. The specific test cases should be developed from the software requirements and the \nerror sources inherent in the software development processes. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "Note\n: Robustness test cases are requirements-based. The robustness testing criteria \ncannot be fully satisfied if the software requirements do not specify the correct software response to abnormal conditions and inputs. The test cases may reveal inadequacies in the software requirements, in which case the software requirements should be modified. Conversely, if a complete set of requirements exists that covers all abnormal conditions and inputs, the",
    "robustness test cases will follow from those software requirements. \nc. Test procedures are generated from the test cases.",
    "## 6.4.2.1 Normal Range Test Cases\n\nNormal range test cases demonstrate the ability of the software to respond to normal inputs and conditions. Activities include:",
    "a. Real and integer input variables should be exercised using valid equivalence classes  \nand boundary values. \nb. For time-related functions, such as filters, integrators, and delays, multiple iterations \nof the code should be performed to check the characteristics of the function in context. \nc. For state transitions, test cases should be developed to exercise the transitions \npossible during normal operation.",
    "possible during normal operation. \nd. For software requirements expressed by logic equations, the normal range test cases \nshould verify the variable usage and the Boolean operators.",
    "## 6.4.2.2 Robustness Test Cases\n\nRobustness test cases demonstrate the ability of the software to respond to abnormal inputs and conditions. Activities include:",
    "a. Real and integer variables should be exercised using equivalence class selection of \ninvalid values. \nb. System initialization should be exercised during abnormal conditions. c. The possible failure modes of the incoming data should be determined, especially \ncomplex, digital data strings from an external system. \nd. For loops where the loop count is a computed value, test cases should be developed",
    "d. For loops where the loop count is a computed value, test cases should be developed \nto attempt to compute out-of-range loop count values, and thus demonstrate the robustness of the loop-related code. \ne. A check should be made to ensure that protection mechanisms for exceeded frame \ntimes respond correctly. \nf. For time-related functions, such as filters, integrators, and delays, test cases should be \ndeveloped for arithmetic overflow protection mechanisms.",
    "developed for arithmetic overflow protection mechanisms. \ng. For state transitions, test cases should be developed to provoke transitions that are not \nallowed by the software requirements.",
    "## 6.4.3 Requirements-Based Testing Methods\n\nThe requirements-based testing methods discussed in this document are requirementsbased hardware/software integration testing, requirements-based software integration testing, and requirements-based low-level testing. \n\nWith the exception of hardware/software integration testing, these methods do not prescribe a specific test environment or strategy. Activities include:",
    "a. Requirements-Based Hardware/Software Integration Testing\n:  This testing method \nshould concentrate on error sources associated with the software operating within the target computer environment, and on the high-level functionality. requirements-based hardware/software integration testing ensures that the software in the target computer \nwill satisfy the high-level requirements. Typical errors revealed by this testing \nmethod include: \n- \nIncorrect interrupt handling. \n-",
    "method include: \n- \nIncorrect interrupt handling. \n- \nFailure to satisfy execution time requirements. \n- \nIncorrect software response to hardware transients or hardware failures, for example, start-up sequencing, transient input loads, and input power transients. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "- \nDatabus and other resource contention problems, for example, memory mapping. \n- \nInability of built-in test to detect failures. \n- \nErrors in hardware/software interfaces. \n- \nIncorrect behavior of control loops. \n- \nIncorrect control of memory management hardware or other hardware devices under software control. \n- \nStack overflow. \n- \nIncorrect operation of mechanism(s) used to confirm the correctness and compatibility of field-loadable software. \n- \nViolations of software partitioning.",
    "- \nViolations of software partitioning. \nb. Requirements-Based Software Integration Testing\n:  This testing method should \nconcentrate on the inter-relationships between the software requirements, and on the implementation of requirements by the software architecture. Requirements-based \nsoftware integration testing ensures that the software components interact correctly",
    "software integration testing ensures that the software components interact correctly \nwith each other and satisfy the software requirements and software architecture. This method may be performed by expanding the scope of requirements through successive integration of code components with a corresponding expansion of the scope of the test cases. Typical errors revealed by this testing method include: \n- \nIncorrect initialization of variables and constants. \n- \nParameter passing errors. \n-",
    "- \nIncorrect initialization of variables and constants. \n- \nParameter passing errors. \n- \nData corruption, especially global data. \n- \nInadequate end-to-end numerical resolution. \n- \nIncorrect sequencing of events and operations. \nc. Requirements-Based Low-Level Testing\n:  This testing method should concentrate on \ndemonstrating that each software component complies with its low-level",
    "demonstrating that each software component complies with its low-level \nrequirements. Requirements-based low-level testing ensures that the software components satisfy their low-level requirements. Typical errors revealed by this testing method include: \n- \nFailure of an algorithm to satisfy a software requirement. \n- \nIncorrect loop operations. \n- \nIncorrect logic decisions. \n- \nFailure to process correctly legitimate combinations of input conditions. \n-",
    "- \nFailure to process correctly legitimate combinations of input conditions. \n- \nIncorrect responses to missing or corrupted input data. \n- \nIncorrect handling of exceptions, such as arithmetic faults or violations of array \nlimits. \n- \nIncorrect computation sequence. \n- \nInadequate algorithm precision, accuracy, or performance.",
    "## 6.4.4 Test Coverage Analysis",
    "Test coverage analysis is a two step process involving requirements-based coverage analysis and structural coverage analysis. The first step analyzes the test cases in relation to the software requirements to confirm that the selected test cases satisfy the specified criteria. The second step confirms that the requirements-based test procedures exercised the code structure to the applicable coverage criteria. If the structural coverage analysis showed the applicable coverage was not met,",
    "coverage criteria. If the structural coverage analysis showed the applicable coverage was not met, additional activities are identified for resolution of such situations as dead code (see 6.4.4.3)",
    "The objectives for test coverage analysis are:",
    "a. Test coverage of high-level requirements is achieved. \nb. Test coverage of low-level requirements is achieved. \nc. Test coverage of software structure to the appropriate coverage criteria is achieved. \nd. Test coverage of software structure, both data coupling and control coupling, is \nachieved. \n\n## 6.4.4.1 Requirements-Based Test Coverage Analysis",
    "## 6.4.4.1 Requirements-Based Test Coverage Analysis\n\nThis analysis determines how well the requirements-based testing verified the implementation of the software requirements. This analysis may reveal the need for additional requirements-based test cases. Activities include:",
    "a. Analysis, using the associated Trace Data, to confirm that test cases exist for each \nsoftware requirement. \nb. Analysis to confirm that test cases satisfy the criteria of normal and robustness testing \nas defined in section 6.4.2. \nc. Resolution of any deficiencies identified in the analysis. Possible solutions are adding \nor enhancing test cases. \nd. Analysis to confirm that all the test cases, and thus all the test procedures, used to",
    "d. Analysis to confirm that all the test cases, and thus all the test procedures, used to \nachieve structural coverage, are traceable to requirements.",
    "## 6.4.4.2 Structural Coverage Analysis\n\nThis analysis determines which code structure, including interfaces between components, was not exercised by the requirements-based test procedures. The requirements-based test cases may not have completely exercised the code structure, including interfaces, so structural coverage analysis is performed and additional verification produced to provide structural coverage. Activities include:",
    "a. Analysis of the structural coverage information collected during requirements-based \ntesting to confirm that the degree of structural coverage is appropriate to the assurance level. \nb. Structural coverage analysis may be performed on the Source Code, object code, or",
    "b. Structural coverage analysis may be performed on the Source Code, object code, or \nExecutable Object Code. Independent of the code form on which the structural coverage analysis is performed, if the assurance level is AL1 and a compiler, linker, or other means generates additional code that is not directly traceable to Source Code \nstatements, then additional verification should be performed to establish the \ncorrectness of such generated code sequences. \nNote",
    "correctness of such generated code sequences. \nNote\n:  \"Additional code that is not directly traceable to Source Code statements\" is \ncode that introduces branches or side effects that are not immediately apparent at the Source Code level. This means that compiler-generated arraybound checks, for example, are not considered to be directly traceable to Source Code statements for the purposes of structural coverage analysis, and should be subjected to additional verification.",
    "c. Analysis to confirm that the requirements-based testing has exercised the data and \ncontrol coupling between code components. \nd. Structural coverage analysis resolution (see 6.4.4.3).",
    "## 6.4.4.3 Structural Coverage Analysis Resolution\n\nStructural coverage analysis may reveal code structure including interfaces that were not exercised during testing. Resolution will require additional software verification process activity. Causes of unexecuted code structure including interfaces, and associated activities to resolve them include:",
    "a. Shortcomings in requirements-based test cases or procedures\n:  The test cases should \nbe augmented or test procedures changed to provide the missing coverage. The method(s) used to perform the requirements-based coverage analysis may need to be reviewed. \nInadequacies in software requirements\nb. \n:  The software requirements should be \nmodified and additional test cases developed and test procedures executed. \nc. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "Extraneous code, including dead code:  The code should be removed and an analysis \nperformed to assess the effect and the need for reverification. If extraneous code is found at the Source Code or object code level, it may be allowed to remain only if analysis shows it does not exist in the Executable Object Code (for example, due to smart compiling, linking, or some other mechanism) and procedures are in place to \nprevent inclusion in future builds. \nd. Deactivated code",
    "prevent inclusion in future builds. \nd. Deactivated code\n:  Deactivated code should be handled in one of two ways, \ndepending upon its defined category. \nCategory One\n1. \n:  Deactivated code that is not intended to be executed in any \ncurrent configuration used within any CNS/ATM system. For this category, a",
    "current configuration used within any CNS/ATM system. For this category, a \ncombination of analysis and testing should show that the means by which the deactivated code could be inadvertently executed are prevented, isolated, or eliminated. Any reassignment of the software level for the deactivated code should be justified by the system safety assessment process and documented in",
    "the Plan for Software Aspects of Approval. Similarly, any alleviation of the software verification process for the deactivated code should be justified by the \nsoftware development processes and documented in the Plan for Software Aspects of Approval. \nCategory Two\n2. \n:  Deactivated code that is only executed in certain approved \nconfigurations of the target computer environment. The operational configuration \nneeded for normal execution of this code should be established and additional",
    "needed for normal execution of this code should be established and additional \ntest cases and test procedures developed to satisfy the required coverage objectives.",
    "## 6.4.5 Reviews And Analyses Of Test Cases, Procedures, And Results\n\nThese review and analysis activities confirm that the software testing satisfies these objectives:",
    "a. Test cases\n:  The objectives related to verification of test cases are presented in \nsections 6.4.4a and 6.4.4b. \nTest procedures\nb. \n:  The objective is to verify that the test cases, including expected \nresults, were correctly developed into test procedures. \nTest results:\nc. \n  The objective is to ensure that the test results are correct and that \ndiscrepancies between actual and expected results are explained. \n\n## 6.5 Software Verification Process Traceability",
    "## 6.5 Software Verification Process Traceability\n\nSoftware verification process traceability activities include:",
    "a. Trace Data, showing the bi-directional association between software requirements \nand test cases, is developed. This Trace Data supports the requirements-based test \ncoverage analysis. \nb. Trace Data, showing the bi-directional association between test cases and test \nprocedures, is developed. This Trace Data enables verification that the complete set of test cases has been developed into test procedures.",
    "c. Trace Data, showing the bi-directional association between test procedures and test \nresults, is developed. This Trace Data enables verification that the complete set of test procedures has been executed.",
    "## 6.6 Verification Of Adaptation Data Items\n\nVerification of an adaptation data item can be conducted separately from the development and verification of the Executable Object Code if all of the following apply:",
    "- \nThe Executable Object Code has been developed and verified by normal range testing to correctly handle all Adaptation Data Item Files that comply with their defined \nstructure and attributes.. \n- The Executable Object Code is robust with respect to Adaptation Data Item Files \nstructures and attributes. \n- \nAll behavior of the Executable Object Code resulting from the contents of the Adaptation Data Item File can be verified. \n-",
    "- \nThe structure of the life cycle data allows the adaptation data item to be managed separately. \nUnless all conditions above are met, adaptation data items should not be verified separately from the rest of the software. In this case, the Executable Object Code and the Adaptation Data Item Files should be verified together.",
    "For adaptation data items that can be verified separately from verification of the Executable Object Code, the objectives identified below apply. The objectives below may be achieved by a combination of tests, analyses, and reviews.",
    "a. The Adaptation Data Item File should be verified to comply with its structure as \ndefined by the high-level requirements. This verification includes ensuring that the Adaptation Data Item File does not contain any elements not defined by the highlevel requirements. Each data element in the Adaptation Data Item File should also be shown to have the correct value, to be consistent with other data elements, and to \ncomply with its attributes as defined by the high-level requirements. \nNote",
    "comply with its attributes as defined by the high-level requirements. \nNote\n: For certain data elements, their attributes may be the only aspect that needs \nto be verified. In other cases, the value of the data element may need to be verified. \nb. All elements of the Adaptation Data Item File have been covered during verification.",
    "b. All elements of the Adaptation Data Item File have been covered during verification. \nIf changes are made to the structure or attributes of the adaptation data item, then the need for modification and reverification of the Executable Object Code should be analyzed.",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n\n## 7.0 Software Configuration Management Process\n\nThis section discusses the objectives and activities of the software configuration management (SCM) process. The SCM process is applied as defined by the software planning process (see 4) and the Software Configuration Management Plan (see 11.4).",
    "Outputs of the SCM process are recorded in Software Configuration Management Records (see 11.18) or in other software life cycle data. The SCM process, working in cooperation with the other software life cycle processes, assists in:",
    "a. Providing a defined and controlled configuration of the software throughout the \nsoftware life cycle. \nb. Providing the ability to consistently replicate the Executable Object Code and \nAdaptation Data Item Files, if any, for software manufacture or to regenerate it in \ncase of a need for investigation or modification. \nc. Providing control of process inputs and outputs during the software life cycle that \nensures consistency and repeatability of process activities.",
    "ensures consistency and repeatability of process activities. \nd. Providing a known point for review, assessing status, and change control by control \nof configuration items and the establishment of baselines. \ne. Providing controls that ensure problems receive attention and changes are recorded, \napproved, and implemented. \nf. Providing evidence of approval of the software by control of the outputs of the \nsoftware life cycle processes.",
    "software life cycle processes. \ng. Assessing the software product compliance with requirements. \nh. Ensuring that secure physical archiving, recovery, and control are maintained for the \nconfiguration items.",
    "## 7.1 Software Configuration Management Process Objectives\n\nThe SCM process objectives are:",
    "a. Each configuration item and its successive versions are labeled unambiguously so \nthat a basis is established for the control and reference of configuration items. \nb. Baselines are defined for further software life cycle process activity and allow \nreference to, control of, and traceability between, configuration items. \nc. The problem reporting process records process non-compliance with software plans",
    "c. The problem reporting process records process non-compliance with software plans \nand standards, records deficiencies of outputs of software life cycle processes, records anomalous behavior of software products, and ensures resolution of these problems. \nd. Change control provides for recording, evaluation, resolution, and approval of \nchanges throughout the software life cycle. \ne. Change review ensures problems and changes are assessed, approved or disapproved,",
    "e. Change review ensures problems and changes are assessed, approved or disapproved, \napproved changes are implemented, and feedback is provided to affected processes through problem reporting and change control methods defined during the software planning process. \nf. Status accounting provides data for the configuration management of software life \ncycle processes with respect to configuration identification, baselines, Problem Reports, and change control.",
    "g. Archival and retrieval ensures that the software life cycle data associated with the \nsoftware product can be retrieved in case of a need to duplicate, regenerate, retest or modify the software product. The objective of the release activity is to ensure that only authorized software is used, especially for software manufacturing, in addition to being archived and retrievable. \nh. Software load control ensures that the Executable Object Code and Adaptation Data",
    "h. Software load control ensures that the Executable Object Code and Adaptation Data \nItem Files, if any, are loaded into the system or equipment with appropriate \nsafeguards. \ni. \nSoftware life cycle environment control ensures that the tools used to produce the \nsoftware are identified, controlled, and retrievable. \nThe objectives for SCM are independent of assurance level. However, two categories of software life cycle data may exist based on the SCM controls applied to the data (see \n7.3).",
    "## 7.2 Software Configuration Management Process Activities",
    "The SCM process includes the activities of configuration identification, change control, baseline establishment, and archiving of the software product, including the related software life cycle data. The SCM process does not stop when the software product is approved by the approval authority, but continues throughout the service life of the system or equipment. If software life cycle activities will be performed by a supplier, then configuration management activities should be applied to the",
    "will be performed by a supplier, then configuration management activities should be applied to the supplier.",
    "## 7.2.1 Configuration Identification\n\nActivities include:",
    "a. Configuration identification should be established for the software life cycle data. \nb. Configuration identification should be established for each configuration item, for \neach separately controlled component of a configuration item, and for combinations of configuration items that comprise a software product. \nc. Configuration items should be configuration identified prior to the implementation of \nchange control and traceability analysis.",
    "change control and traceability analysis. \nd. A configuration item should be configuration identified before that item is used by \nother software life cycle processes, referenced by other software life cycle data, or used for software manufacture or software loading. \ne. If the software product identification cannot be determined by physical examination",
    "e. If the software product identification cannot be determined by physical examination \n(for example, part number plate examination), then the Executable Object Code and Adaptation Data Item Files, if any, should contain configuration identification that \ncan be accessed by other parts of the system or equipment. This may be applicable to field-loadable software.",
    "## 7.2.2 Baselines And Traceability\n\nActivities include:",
    "a. Baselines should be established for configuration items used for approval credit. \nIntermediate baselines may be established to aid in controlling software life cycle process activities. \nb. A software product baseline should be established for the software product and \ndefined in the Software Configuration Index (see 11.16). \nNote\n: User-modifiable software is not included in the software product baseline,",
    "Note\n: User-modifiable software is not included in the software product baseline, \nexcept for its associated protection and boundary components. Therefore, modifications may be made to user-modifiable software without affecting the \nconfiguration identification of the software product baseline. \nc. Baselines should be established in controlled software libraries, whether physical,",
    "c. Baselines should be established in controlled software libraries, whether physical, \nelectronic, or other, to ensure their integrity. Once a baseline is established, it should be protected from change. \nd. Change control activities should be followed to develop a derivative baseline from an \nestablished baseline. \ne. A baseline should be traceable to the baseline from which it was derived, if approval",
    "e. A baseline should be traceable to the baseline from which it was derived, if approval \ncredit is sought for software life cycle process activities or data associated with the development of the previous baseline. \nf. A configuration item should be traceable to the configuration item from which it was \nderived, if approval credit is sought for software life cycle process activities or data associated with the development of the previous configuration item.",
    "g. A baseline or configuration item should be traceable either to the output it identifies \nor to the process with which it is associated. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "## 7.2.3 Problem Reporting, Tracking, And Corrective Action\n\nActivities include:",
    "a. A Problem Report should be prepared that describes the process non-compliance with \nplans, output deficiency, or software anomalous behavior, and the corrective action taken, as defined in section 11.17. \nNote\n: Software life cycle process and software product problems may be recorded in \nseparate problem reporting systems. \nb. Problem reporting should provide for configuration identification of affected",
    "b. Problem reporting should provide for configuration identification of affected \nconfiguration item(s) or definition of affected process activities, status reporting of Problem Reports, and approval and closure of Problem Reports. \nc. Problem Reports that require corrective action of the software product or outputs of \nsoftware life cycle processes should invoke the change control activity.",
    "## 7.2.4 Change Control\n\nActivities include:",
    "a. Change control should preserve the integrity of the configuration items and baselines \nby providing protection against their change. \nb. Change control should ensure that any change to a configuration item requires a \nchange to its configuration identification. \nc. Changes to baselines and to configuration items under change control to produce",
    "c. Changes to baselines and to configuration items under change control to produce \nderivative baselines should be recorded, approved, and tracked. Problem reporting is related to change control, since resolution of a reported problem may result in \nchanges to configuration items or baselines. \nNote\n: It is generally recognized that early implementation of change control assists the control and management of software life cycle process activities.",
    "d. Software changes should be traced to their origin and the software life cycle \nprocesses repeated from the point at which the change affects their outputs. For example, an error discovered at hardware/software integration, that is shown to result from an incorrect design, should result in design correction, code correction, and repetition of the associated integral process activities. \ne. Throughout the change activity, software life cycle data affected by the change",
    "e. Throughout the change activity, software life cycle data affected by the change \nshould be updated and records should be maintained for the change control activity. \nThe change control activity is aided by the change review activity.",
    "## 7.2.5 Change Review\n\nActivities include:",
    "a. Assessment of the impact of the problem or proposed change on system \nrequirements. Feedback should be provided to the system processes, including the system safety assessment process, and any responses from the system processes \nshould be assessed. \nb. Assessment of the impact of the problem or proposed change on software life cycle \ndata identifying changes to be made and actions to be taken. \nc. Confirmation that affected configuration items are configuration identified.",
    "c. Confirmation that affected configuration items are configuration identified. \nd. Feedback of Problem Report or change impact and decisions to affected processes.",
    "## 7.2.6 Configuration Status Accounting\n\nActivities include: \n\na. Reporting on configuration item identification, baseline identification, Problem \nReport status, change history, and release status. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n\nb. Definition of the data to be maintained and the means of recording and reporting \nstatus of this data. \n\n## 7.2.7 Archive, Retrieval, And Release\n\nActivities include:",
    "a. Software life cycle data associated with the software product should be retrievable \nfrom the approved source (for example, an archive at the developing organization or \ncompany). \nb. Procedures should be established to ensure the integrity of the stored data, regardless \nof medium of storage, by:",
    "of medium of storage, by: \n1. Ensuring that no unauthorized changes can be made. 2. Selecting storage media that minimize regeneration errors or deterioration. 3. Preventing loss or corruption of data over time. Depending on the storage media \nused, this may include periodically exercising the media or refreshing the archived data. \n4. Storing duplicate copies in physically separate archives that minimize the risk of \nloss in the event of a disaster.",
    "loss in the event of a disaster. \nc. The duplication process should be verified to produce accurate copies, and \nprocedures should exist that ensure error-free copying of the Executable Object Code \nand Adaptation Data Item Files, if any. \nd. Configuration items should be identified and released prior to use for software \nmanufacture and the authority for their release should be established. As a minimum, the components of the software product loaded into the CNS/ATM system or",
    "equipment should be released. This includes the Executable Object Code and Adaptation Data Item Files, if any, and may also include associated media for \nsoftware loading. \nNote\n: Release is generally also required for the data that defines the approved \nsoftware for loading into the CNS/ATM system or equipment. Definition of that data is outside the scope of this document, but may include the Software Configuration Index.",
    "e. Data retention procedures should be established to satisfy approval authority \nrequirements and enable software modifications. \nNote\n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    ": Additional data retention considerations may include items such as business \nneeds and future approval authority reviews, which are outside the scope of \nthis document. \n\n## 7.3 Data Control Categories",
    "Software life cycle data can be assigned to one of two configuration management control categories: Control Category 1 (CC1) and Control Category 2 (CC2). Table 7-1 defines the set of SCM process activities associated with each control category, where - indicates the minimum activities that apply for software life cycle data of that category. CC2",
    "activities are a subset of the CC1 activities. The Annex A tables specify the control category by assurance level for the software life cycle data items.",
    "Table 7-1 SCM Process Activities Associated with CC1 and CC2 Data  \nSCM Process Activity \nReference \nCC1 \nCC2 \nConfiguration Identification \n7.2.1 \n- \n- \n- \n \nBaselines \n7.2.2.a 7.2.2.b 7.2.2.c 7.2.2.d \n7.2.2.e \n- \n- \nTraceability \n7.2.2.f 7.2.2.g \nProblem Reporting \n7.2.3 \n- \n \n- \n- \nChange Control - integrity and identification \n7.2.4.a 7.2.4.b \n- \n \nChange Control - tracking \n7.2.4.c 7.2.4.d 7.2.4.e \nChange Review \n7.2.5 \n- \n \nConfiguration Status Accounting \n7.2.6 \n- \n \nRetrieval \n7.2.7.a",
    "Change Review \n7.2.5 \n- \n \nConfiguration Status Accounting \n7.2.6 \n- \n \nRetrieval \n7.2.7.a \n- \n- \nProtection against Unauthorized Changes \n7.2.7.b.1  \n- \n- \n- \n \nMedia Selection, Refreshing, Duplication \n7.2.7.b.2 7.2.7.b.3 7.2.7.b.4 7.2.7.c \nRelease \n7.2.7.d \n- \n \nData Retention \n7.2.7.e \n- \n-",
    "## 7.4 Software Load Control\n\nSoftware load control refers to the process by which programmed instructions and data are transferred from a master memory device into the system or equipment. For example, methods may include (subject to approval by the approval authority) the installation of factory pre-programmed memory devices or \"in situ\" re-programming of the system or equipment using a field loading device. Whichever method is used, software load control should include:",
    "a. Procedures for part numbering and media identification that identify software \nconfigurations that are intended to be approved for loading into the CNS/ATM \nsystem or equipment. \nb. Whether the software is delivered as an end item or is delivered installed in the \nCNS/ATM system or equipment, records should be kept that confirm software compatibility with the CNS/ATM system or equipment hardware.  \n\n## 7.5 Software Life Cycle Environment Control",
    "## 7.5 Software Life Cycle Environment Control\n\nThe software life cycle environment tools are defined by the software planning process and identified in the Software Life Cycle Environment Configuration Index (see 11.15). \n\nActivities include:",
    "a. Configuration identification should be established for the Executable Object Code, or \nequivalent, of the tools used to develop, control, build, verify, and load the software. \nb. The SCM process for controlling qualified tools should comply with the objectives \nassociated with Control Category 1 or Control Category 2 data (see 7.3), according to \nthe guidance provided by section 12.2.3. \nc. Unless section 7.5b applies, the SCM process for controlling the Executable Object",
    "c. Unless section 7.5b applies, the SCM process for controlling the Executable Object \nCode, or equivalent, for tools used to build and load the software (for example, \ncompilers, assemblers, and linkage editors) should comply with the objectives \nassociated with Control Category 2 data, as a minimum. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "## 8.0 Software Quality Assurance Process",
    "This section discusses the objectives and activities of the software quality assurance (SQA) process. The SQA process is applied as defined by the software planning process (see 4) and the Software Quality Assurance Plan (see 11.5). Outputs of the SQA process activities are recorded in Software Quality Assurance Records (see 11.19) or other software life cycle data. The SQA process assesses the software life cycle processes and their outputs to obtain assurance that the objectives are",
    "the software life cycle processes and their outputs to obtain assurance that the objectives are satisfied; that deficiencies are detected, evaluated, tracked, and resolved; and that the software product and software life cycle data conform to approval requirements.",
    "## 8.1 Software Quality Assurance Process Objectives\n\nThe SQA process objectives provide confidence that the software life cycle processes produce software that conforms to its requirements by assuring that these processes are performed in compliance with the approved software plans and standards. The objectives of the SQA process are to obtain assurance that:",
    "a. Software plans and standards are developed and reviewed for compliance to this \ndocument and for consistency. \nb. Software life cycle processes, including those of suppliers, comply with approved \nsoftware plans and standards. \nc. The transition criteria for the software life cycle processes are satisfied. \nd. A conformity review of the software product is conducted. \n\n## 8.2 Software Quality Assurance Process Activities\n\nActivities for satisfying the SQA process objectives include:",
    "a. The SQA process should take an active role in the activities of the software life cycle \nprocesses, and have those performing the SQA process enabled with the authority, responsibility, and independence to ensure that the SQA process objectives are \nsatisfied. \nb. The SQA process should provide assurance that software plans and standards are \ndeveloped and reviewed for compliance to this document and for consistency.",
    "developed and reviewed for compliance to this document and for consistency. \nc. The SQA process should provide assurance that the software life cycle processes \ncomply with the approved software plans and standards. \nd. The SQA process should include audits of the software life cycle processes during \nthe software life cycle to obtain assurance that: \n1. Software plans are available as specified in section 4.2. \n2. Deviations from the software plans and standards are detected, recorded,",
    "2. Deviations from the software plans and standards are detected, recorded, \nevaluated, tracked, and resolved. \nNote\n: It is generally accepted that early detection of process deviations assists \nefficient achievement of software life cycle process objectives. \n3. Approved deviations are recorded. \n4. The software development environment has been provided as specified in the \nsoftware plans. \n5. The problem reporting, tracking, and corrective action process activities comply",
    "5. The problem reporting, tracking, and corrective action process activities comply \nwith the Software Configuration Management Plan. \n6. Inputs provided to the software life cycle processes by the system processes, \nincluding the system safety assessment process, have been addressed. \nNote\n: Monitoring of the activities of software life cycle processes may be \nperformed to provide assurance that the activities are under control.",
    "performed to provide assurance that the activities are under control. \ne. The SQA process should provide assurance that the transition criteria for the software \nlife cycle processes have been satisfied in compliance with the approved software plans. \nf. The SQA process should provide assurance that software life cycle data is controlled \nin accordance with the control categories as defined in section 7.3 and the tables of Annex A.",
    "in accordance with the control categories as defined in section 7.3 and the tables of Annex A. \ng. Prior to the delivery of software products submitted as part of a approval application, \na software conformity review should be conducted. \nh. The SQA process should produce records of the SQA process activities (see 11.19), \nincluding audit results and evidence of completion of the software conformity review for each software product submitted as part of approval application. \ni.",
    "i. \nThe SQA process should provide assurance that supplier processes and outputs comply with approved software plans and standards.",
    "## 8.3 Software Conformity Review\n\nThe purpose of the software conformity review is to obtain assurance, for a software product submitted as part of an approval application, that the software life cycle processes are complete, software life cycle data is complete, and the Executable Object Code and Adaptation Data Item Files, if any, are controlled and can be regenerated. \n\nThis review should determine that:",
    "a. Planned software life cycle process activities for approval credit, including the \ngeneration of software life cycle data, have been completed and records of their completion are retained. \nb. Software life cycle data developed from specific system requirements, safety-related \nrequirements, or software requirements are traceable to those requirements. \nc. Evidence exists that software life cycle data have been produced in accordance with",
    "c. Evidence exists that software life cycle data have been produced in accordance with \nsoftware plans and standards, and is controlled in accordance with the SCM Plan. \nd. Evidence exists that Problem Reports have been evaluated and have their status \nrecorded. \ne. Software requirement deviations are recorded and approved. \nf. The Executable Object Code and Adaptation Data Item Files, if any, can be \nregenerated from the archived Source Code.",
    "regenerated from the archived Source Code. \ng. The approved software can be loaded successfully through the use of released \ninstructions. \nh. Problem Reports deferred from a previous software conformity review are reevaluated to determine their status. \ni. \nIf approval credit is sought for the use of previously developed software, the current \nsoftware product baseline is traceable to the previous baseline and the approved changes to that baseline.",
    "review activities, as justified by the significance of the change, may be performed.",
    "## 9.0 Approval Liaison Process\n\nThe objectives of the approval liaison process are to:",
    "a. Establish communication and understanding between the applicant and the approval \nauthority throughout the software life cycle to assist the approval process. \nb. Gain agreement on the means of compliance through approval of the Plan for \nSoftware Aspects of Approval. \nc. Provide compliance substantiation. \nThe approval liaison process is applied as defined by the software planning process (see \n4) and the Plan for Software Aspects of Approval (see 11.1). Table A-10",
    "4) and the Plan for Software Aspects of Approval (see 11.1). Table A-10\n of Annex A is a summary of the objectives and outputs of this process.",
    "## 9.1 Means Of Compliance And Planning\n\nThe applicant proposes a means of compliance that defines how the development of the system or equipment will satisfy the approval basis. The Plan for Software Aspects of Approval (see 11.1) defines the software aspects of the system or equipment within the context of the proposed means of compliance. This plan also states the assurance level(s) as determined by the system safety assessment process. Activities include:",
    "a. Submitting the Plan for Software Aspects of Approval and other requested data to the \napproval authority for review. \nb. Resolving issues identified by the approval authority concerning the planning for the \nsoftware aspects of approval. \nc. Obtaining agreement with the approval authority on the Plan for Software Aspects of \nApproval. \n\n## 9.2 Compliance Substantiation",
    "## 9.2 Compliance Substantiation\n\nThe applicant provides evidence that the software life cycle processes satisfy the software plans, by making software life cycle data available to the approval authority for review.",
    "Approval authority reviews may take place at various facilities. For example, the reviews may take place at the applicant's facilities, the applicant's supplier's facilities, or at the approval authority's facilities. This may involve discussions with the applicant or its suppliers. The applicant arranges these reviews of the activities of the software life cycle processes and makes software life cycle data available as needed. \n\nActivities include:",
    "Activities include: \n\na. Resolving issues raised by the approval authority as a result of its reviews. \nb. Submitting the Software Accomplishment Summary (see 11.20) and Software \nConfiguration Index (see 11.16) to the approval authority. \nc. Submitting or making available other data or evidence of compliance requested by \nthe approval authority. \n\n## 9.3 Minimum Software Life Cycle Data Submitted To Approval Authority",
    "## 9.3 Minimum Software Life Cycle Data Submitted To Approval Authority\n\nThe minimum software life cycle data that is submitted to the approval authority is: \n\na. Plan for Software Aspects of Approval. b. Software Configuration Index. \nc. Software Accomplishment Summary. \n\n## 9.4 Software Life Cycle Data Related To The Approval Process",
    "## 9.4 Software Life Cycle Data Related To The Approval Process\n\nUnless otherwise agreed by the approval authority, the guidance concerning retrieval and approval of software life cycle data related to the approval of the product applies to: \n\na. Software Requirements Data. \nb. Design Description. \nc. Source Code. \nd. Executable Object Code and Adaptation Data Item Files, if any. \ne. Software Configuration Index. \nf. Software Accomplishment Summary.",
    "## 10.0 Overview Of Cns/Atm System Approval Process\n\nThis section is an overview of the approval process with respect to software aspects of CNS/ATM systems and equipment, and is provided for information purposes only. The approval authority considers the software as part of the CNS/ATM system; that is, the approval authority does not approve the software as a unique, stand-alone product. \n\n## 10.1 Approval Basis",
    "The approval authority establishes the approval basis for the CNS/ATM system in consultation with the applicant. The approval basis defines the particular regulations and/or requirements together with any special conditions to be applied beyond the published regulations. When CNS/ATM systems are being modified, the approval authority considers the impact the modification has on the original approval basis. In some cases, the approval basis for the modification may not change from the original",
    "basis. In some cases, the approval basis for the modification may not change from the original approval basis; however, the original means of compliance may not be applicable for showing that the modification complies with the approval basis and may need to be changed.",
    "## 10.2 Software Aspects Of Approval",
    "The approval authority assesses the Plan for Software Aspects of Approval for completeness and consistency with the means of compliance that was agreed upon to satisfy the approval basis. The approval authority satisfies itself that the assurance level(s) proposed by the applicant is consistent with the outputs of the system safety assessment process and other system life cycle data. The approval authority informs the applicant of issues with the proposed software plans that need to be",
    "approval authority informs the applicant of issues with the proposed software plans that need to be satisfied prior to approval authority agreement.",
    "## 10.3 Compliance Determination",
    "Prior to approval, the approval authority determines that the CNS/ATM system, including the software aspects of its systems or equipment, complies with the approval basis. For the software, this is accomplished by reviewing the Software Accomplishment Summary and evidence of compliance. The approval authority uses the Software Accomplishment Summary as an overview for the software aspects of approval. The approval authority may review at its discretion the software life cycle processes and",
    "approval. The approval authority may review at its discretion the software life cycle processes and their outputs during the software life cycle as discussed in section 9.2.",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n\n## 11.0 Software Life Cycle Data\n\nData is produced during the software life cycle to plan, direct, explain, define, record, or provide evidence of activities. This data enables the software life cycle processes, system or equipment approval, and post-approval modification of the software product. This section discusses the characteristics, form, configuration management controls, and content of the software life cycle data.",
    ":  Software life cycle data should be: \n\n## A. Characteristics",
    "Unambiguous\n1. \n:  Information is unambiguous if it is written in terms which only \nallow a single interpretation, aided, if necessary, by a definition. \nComplete\n2. \n:  Information is complete when it includes necessary and relevant \nrequirements and/or descriptive material; responses are defined for the range of \nvalid input data; figures used are labeled; and terms and units of measure are \ndefined. \nVerifiable\n3. \n:  Information is verifiable if it can be checked for correctness by a",
    "defined. \nVerifiable\n3. \n:  Information is verifiable if it can be checked for correctness by a \nperson or tool. \n:  Information is consistent if there are no conflicts within it. \nConsistent\n4. \nModifiable\n5. \n:  Information is modifiable if it is structured and has a style such that \nchanges can be made completely, consistently, and correctly while retaining the structure. \nTraceable\n6. \n:  Information is traceable if the origin of its components can be \ndetermined. \nb.",
    "Traceable\n6. \n:  Information is traceable if the origin of its components can be \ndetermined. \nb. \nForm:  The form of the software life cycle data should provide for the efficient \nretrieval and review of software life cycle data throughout the service life of the system or equipment. The data and the specific form of the data should be specified in the Plan for Software Aspects of Approval. \nNote 1: The software life cycle data may be held in a number of forms (for example,",
    "Note 1: The software life cycle data may be held in a number of forms (for example, \nheld electronically or in printed form). \nNote 2: The applicant may package software life cycle data items in any manner the \napplicant finds convenient (for example, as individual data items or as a combined data item). \nNote 3: The Plan for Software Aspects of Approval and the Software \nAccomplishment Summary may be required as separate documents by some approval authorities. \nNote 4",
    "Accomplishment Summary may be required as separate documents by some approval authorities. \nNote 4\n: The term \"data\" refers to evidence and other information and does not \nimply the format such data should take. \nc. \nConfiguration management controls:  Software life cycle data can be placed in one of \ntwo categories associated with the software configuration management controls applied: CC1 and CC2 (see 7.3). The minimum control category assigned to each",
    "data item, and its variation by assurance level is specified in the tables of Annex A. If \nadditional data items than those described herein are produced as evidence to aid the approval process, they should be, as a minimum, under CC2 controls.",
    "d. Content\n:  The software life cycle data descriptions provided in the following sections \nidentify the data that is generally produced by the software life cycle. The",
    "descriptions are not intended to describe all data that may be necessary to develop a software product, and are not intended to imply a particular data packaging method or organization of the data within a package. The descriptions of the content of software life cycle data items provided herein are not all encompassing and should be read in conjunction with the body of this document and adapted to the needs of the applicant.",
    "## 11.1 Plan For Software Aspects Of Approval\n\nThe Plan for Software Aspects of Approval (PSAA) is the primary means used by the approval authority for determining whether a applicant is proposing a software life cycle that is commensurate with the rigor required for the assurance level of software being developed. This plan should include:",
    "a. System overview\n:  This section provides an overview of the system, including a \ndescription of its functions and their allocation to the hardware and software, the architecture, processor(s) used, hardware/software interfaces, and safety features. \nSoftware overview\nb. \n:  This section briefly describes the software functions with",
    "Software overview\nb. \n:  This section briefly describes the software functions with \nemphasis on the proposed safety and partitioning concepts. Examples include resource sharing, redundancy, fault tolerance, mitigation of single event upset, and timing and scheduling strategies. \nApproval considerations\nc. \n:  This section provides a summary of the approval basis,",
    "Approval considerations\nc. \n:  This section provides a summary of the approval basis, \nincluding the means of compliance, as relating to the software aspects of approval. This section also states the proposed assurance level(s) and summarizes the justification provided by the system safety assessment process, including potential software contributions to failure conditions. \nSoftware life cycle\nd. \n:  This section defines the software life cycle to be used and",
    "Software life cycle\nd. \n:  This section defines the software life cycle to be used and \nincludes a summary of each of the software life cycle processes for which detailed information is defined in their respective software plans. The summary explains how the objectives of each software life cycle process will be satisfied, and specifies the organizations to be involved, the organizational responsibilities, and the system life cycle processes and approval liaison process responsibilities.",
    "Software life cycle data\ne. \n:  This section specifies the software life cycle data that will \nbe produced and controlled by the software life cycle processes. This section also describes the relationship of the data to each other or to other data defining the system, the software life cycle data to be submitted to the approval authority, the \nform of the data, and the means by which the data will be made available to the \napproval authority. \nSchedule\nf.",
    "approval authority. \nSchedule\nf. \n:  This section describes the means the applicant will use to provide the \napproval authority with visibility of the activities of the software life cycle processes so reviews can be planned. \ng. \nAdditional considerations:  This section describes specific considerations that may \naffect the approval process. Examples include alternative methods of compliance, tool qualification, previously developed software, option-selectable software, user-",
    "modifiable software, deactivated code, COTS software, field-loadable software, adaptation data items, multiple-version dissimilar software, and product service experience.",
    "h. Supplier oversight\n:  This section describes the means of ensuring that supplier \nprocesses and outputs will comply with approved software plans and standards. \n\n## 11.2 Software Development Plan\n\nThe Software Development Plan (SDP) is a description of the software development procedures and software life cycle(s) to be used to satisfy the software development process objectives. It may be included in the Plan for Software Aspects of Approval. This plan should include:",
    "a. Standards\n:  Identification of the Software Requirements Standards, Software Design \nStandards and Software Code Standards for the project. Also, references to the \nstandards for previously developed software, including COTS software, if those \nstandards are different. \nSoftware life cycle\nb. \n:  A description of the software life cycle processes to be used to",
    "Software life cycle\nb. \n:  A description of the software life cycle processes to be used to \nform the specific software life cycle(s) to be used on the project, including the transition criteria for the software development processes. This description is distinct from the summary provided in the Plan for Software Aspects of Approval, in that it provides the detail necessary to ensure proper implementation of the software life cycle processes. \nSoftware development environment\nc.",
    "Software development environment\nc. \n:  A statement of the chosen software \ndevelopment environment in terms of hardware and software, including: \n1. Requirements development method(s) and tools to be used. 2. Design method(s) and tools to be used. \n3. Coding method(s), programming language(s), coding tool(s) to be used, and \nwhen applicable, options and constraints of autocode generators. \n4. The compilers, linkage editors and loaders to be used.",
    "4. The compilers, linkage editors and loaders to be used. \n5. The hardware platforms for the tools to be used.",
    "## 11.3 Software Verification Plan\n\n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nThe Software Verification Plan (SVP) is a description of the verification procedures to be used to satisfy the software verification process objectives. These procedures may vary by assurance level as defined in the tables of Annex A. This plan should include:",
    "a. Organization\n:  Organizational responsibilities within the software verification process \nand interfaces with the other software life cycle processes. \nb. \nIndependence:  A description of the methods for establishing verification \nindependence, when required. \nc. Verification methods\n:  A description of the verification methods to be used for each \nactivity of the software verification process.",
    "activity of the software verification process. \n1. Review methods, including checklists or other aids. 2. Analysis methods, including traceability and coverage analysis. \n3. Testing methods, including the method for selecting test cases, the test \nprocedures to be used, and the test data to be produced. \nd. Verification environment\n:  A description of the equipment for testing, the testing and \nanalysis tools, and how to apply these tools and hardware test equipment. Section",
    "analysis tools, and how to apply these tools and hardware test equipment. Section \n4.4.3b provides guidance on indicating target computer and simulator or emulator differences \nTransition criteria\ne. \n:  The transition criteria for entering the software verification \nprocess. \nPartitioning considerations\nf. \n:  If partitioning is used, the methods used to verify the \nintegrity of the partitioning. \nCompiler assumptions\ng. \n:  A description of the assumptions made by the applicant",
    "Compiler assumptions\ng. \n:  A description of the assumptions made by the applicant \nabout the correctness of the compiler, linkage editor or loader (see 4.4.2). \nReverification method\nh. \n:  For software modification, a description of the methods for \nidentifying, analyzing and verifying the affected areas of the software and the changed parts of the Executable Object Code. \nPreviously developed software\ni. \n:  For previously developed software, if the initial",
    "Previously developed software\ni. \n:  For previously developed software, if the initial \ncompliance baseline for the verification process does not comply with this document, a description of the methods to satisfy the objectives of this document. \nMultiple-version dissimilar software\nj. \n:  If multiple-version dissimilar software is used, \na description of the software verification process activities (see 12.3.2).",
    "## 11.4 Software Configuration Management Plan\n\nThe Software Configuration Management Plan establishes the methods to be used to achieve the objectives of the SCM process throughout the software life cycle. This plan should include:",
    "a. Environment\n:  A description of the SCM environment to be used, including \nprocedures, tools, methods, standards, organizational responsibilities, and interfaces. \n:  A description of the SCM process activities in the software life cycle: \nActivities\nb. \nConfiguration identification\n1. \n:  Items to be identified, when they will be identified,",
    "b. \nConfiguration identification\n1. \n:  Items to be identified, when they will be identified, \nthe identification methods for software life cycle data (for example, part numbering), and the relationship of software identification and system or equipment identification. \n2. \nBaselines and traceability:  The means of establishing baselines, what baselines",
    "2. \nBaselines and traceability:  The means of establishing baselines, what baselines \nwill be established, when these baselines will be established, the software library controls, and the configuration item and baseline traceability. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "3. Problem reporting\n:  The content and identification of Problem Reports for the \nsoftware product and software life cycle processes, when they will be written, the \nmethod of closing Problem Reports, and the relationship to the change control activity. \nChange control\n4. \n:  Configuration items and baselines to be controlled, when they \nwill be controlled, the problem/change control activities that control them, preapproval controls, post-approval controls, and the means of preserving the",
    "integrity of baselines and configuration items. \nChange review\n5. \n:  The method of handling feedback from and to the software life \ncycle processes; the methods of assessing and prioritizing problems, approving \nchanges, and handling their resolution or change implementation; and the relationship of these methods to the problem reporting and change control activities. \nConfiguration status accounting\n6. \n:  The data to be recorded to enable reporting",
    "Configuration status accounting\n6. \n:  The data to be recorded to enable reporting \nconfiguration management status, definition of where that data will be kept, how it will be retrieved for reporting, and when it will be available. \nArchive, retrieval, and release\n7. \n:  The integrity controls, the release method and \nauthority, and data retention. \nSoftware load control\n8. \n:  A description of the software load control safeguards and \nrecords. \nSoftware life cycle environment controls\n9.",
    "records. \nSoftware life cycle environment controls\n9. \n:  Controls for the tools used to develop, \nbuild, verify and load the software, addressing sections 11.4b(1) through \n11.4b(7). This includes control of tools to be qualified. \n:  Controls associated with CC1 and CC2 data. \nSoftware life cycle data controls\n10. \nTransition criteria\n:  The transition criteria for entering the SCM process. \nc. \nSCM data\nd. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    ":  A definition of the software life cycle data produced by the SCM process, \nincluding SCM Records, the Software Configuration Index and the Software Life Cycle Environment Configuration Index. \nSupplier control\n:  The means of applying SCM process requirements to suppliers. \ne. \n\n## 11.5 Software Quality Assurance Plan",
    "## 11.5 Software Quality Assurance Plan\n\nThe Software Quality Assurance Plan establishes the methods to be used to achieve the objectives of the SQA process. The SQA Plan may include descriptions of process improvement, metrics, and progressive management methods. This plan should include:",
    "a. Environment\n:  A description of the SQA environment, including scope, \norganizational responsibilities and interfaces, standards, procedures, tools, and \nmethods. \nb. \nAuthority:  A statement of the SQA authority, responsibility, and independence, \nincluding the SQA approval of software products. \nc. Activities\n:  The SQA activities that are to be performed for each software life cycle \nprocess and throughout the software life cycle including:",
    "process and throughout the software life cycle including: \n1. SQA methods, for example, reviews, audits, reporting, inspections, and \nmonitoring of the software life cycle processes. \n2. Activities related to the problem reporting, tracking, and corrective action \nsystem. \n3. A description of the software conformity review activity. \n:  The transition criteria for entering the SQA process. \nd. Transition criteria\nTiming\ne.",
    ":  The transition criteria for entering the SQA process. \nd. Transition criteria\nTiming\ne. \n:  The timing of the SQA process activities in relation to the activities of the \nsoftware life cycle processes. \n:  A definition of the records to be produced by the SQA process. \nSQA Records\nf. \nSupplier oversight\ng. \n:  A description of the means of ensuring that suppliers' processes \nand outputs will comply with the plans and standards.",
    "## 11.6 Software Requirements Standards\n\nSoftware Requirements Standards define the methods, rules and tools to be used to develop the high-level requirements. These standards should include:",
    "a. The methods to be used for developing software requirements, such as structured \nmethods. \nb. Notations to be used to express requirements, such as data flow diagrams and formal \nspecification languages. \nc. Constraints on the use of the tools used for requirements development. \nd. The method to be used to provide derived requirements to the system processes. \n\n## 11.7 Software Design Standards",
    "## 11.7 Software Design Standards\n\nSoftware Design Standards define the methods, rules, and tools to be used to develop the software architecture and low-level requirements. These standards should include:",
    "a. Design description method(s) to be used. b. Naming conventions to be used. \nc. Conditions imposed on permitted design methods, for example, scheduling, and the \nuse of interrupts and event-driven architectures, dynamic tasking, re-entry, global data, and exception handling, and rationale for their use. \nd. Constraints on the use of the design tools. \ne. Constraints on design, for example, exclusion of recursion, dynamic objects, data \naliases, and compacted expressions.",
    "aliases, and compacted expressions. \nf. Complexity restrictions, for example, maximum level of nested calls or conditional \nstructures, use of unconditional branches, and number of entry/exit points of code components.",
    "## 11.8 Software Code Standards\n\nSoftware Code Standards define the programming languages, methods, rules, and tools to be used to code the software. These standards should include:",
    "a. Programming language(s) to be used and/or defined subset(s). For a programming \nlanguage, reference the data that unambiguously defines the syntax, the control \nbehavior, the data behavior, and side-effects of the language. This may require \nlimiting the use of some features of a language. \nb. Source Code presentation standards, for example, line length restriction, indentation, \nand blank line usage and Source Code documentation standards, for example, name",
    "and blank line usage and Source Code documentation standards, for example, name \nof author, revision history, inputs and outputs, and affected global data. \nc. Naming conventions for components, subprograms, variables, and constants. d. Conditions and constraints imposed on permitted coding conventions, such as the \ndegree of coupling between software components and the complexity of logical or numerical expressions and rationale for their use. \ne. Constraints on the use of the coding tools.",
    "## 11.9 Software Requirements Data\n\nSoftware Requirements Data is a definition of the high-level requirements including the derived requirements. This data should include:",
    "a. Description of the allocation of system requirements to software, with attention to \nsafety-related requirements and potential failure conditions. \nb. Functional and operational requirements under each mode of operation. c. Performance criteria, for example, precision and accuracy. \nd. Timing requirements and constraints. \ne. Memory size constraints. \nf. Hardware and software interfaces, for example, protocols, formats, frequency of \ninputs, and frequency of outputs.",
    "inputs, and frequency of outputs. \ng. Failure detection and safety monitoring requirements. \nh. Partitioning requirements allocated to software, how the partitioned software \ncomponents interact with each other, and the assurance level(s) of each partition.",
    "## 11.10 Design Description\n\nThe Design Description is a definition of the software architecture and the low-level requirements that will satisfy the high-level requirements. This data should include:",
    "a. A detailed description of how the software satisfies the specified high-level \nrequirements, including algorithms, data structures, and how software requirements are allocated to processors and tasks. \nb. The description of the software architecture defining the software structure to \nimplement the requirements. \nc. The input/output description, for example, a data dictionary, both internally and \nexternally throughout the software architecture.",
    "externally throughout the software architecture. \nd. The data flow and control flow of the design. \ne. Resource limitations, the strategy for managing each resource and its limitations, the \nmargins, and the method for measuring those margins, for example, timing and memory. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "f. Scheduling procedures and inter-processor/inter-task communication mechanisms, \nincluding time-rigid sequencing, preemptive scheduling, Ada rendezvous, and interrupts. \ng. Design methods and details for their implementation, for example, software loading, \nuser-modifiable software, or multiple-version dissimilar software. \nh. Partitioning methods and means of preventing partition breaches. \ni. \nDescriptions of the software components, whether they are new or previously",
    "i. \nDescriptions of the software components, whether they are new or previously \ndeveloped, and, if previously developed, reference to the baseline from which they were taken. \nj. \nDerived requirements resulting from the software design process. \nk. If the system contains deactivated code, a description of the means to ensure that the \ncode cannot be enabled in the target computer. \nl. \nRationale for those design decisions that are traceable to safety-related system \nrequirements.",
    "## 11.11 Source Code\n\nThis data consists of code written in source language(s). The Source Code is used with the compiling, linking, and loading data in the integration process to develop the integrated system or equipment. For each Source Code component, this data should include the software identification, including the name and date of revision and/or version, as applicable. \n\n## 11.12 Executable Object Code",
    "## 11.12 Executable Object Code\n\nThe Executable Object Code consists of a form of code that is directly usable by the processing unit of the target computer and is, therefore, the software that is loaded into the hardware or system. \n\n## 11.13 Software Verification Cases And Procedures\n\nSoftware Verification Cases and Procedures detail how the software verification process activities are implemented. This data should include descriptions of the:",
    "a. Review and analysis procedures\n: The scope and depth of the review or analysis \nmethods to be used, in addition to the description in the Software Verification Plan. \nTest cases\nb. \n: The purpose of each test case, set of inputs, conditions, expected results to \nachieve the required coverage criteria, and the pass/fail criteria. \nTest procedures\nc. \n:  The step-by-step instructions for how each test case is to be set up",
    "Test procedures\nc. \n:  The step-by-step instructions for how each test case is to be set up \nand executed, how the test results are evaluated, and the test environment to be used.",
    "## 11.14 Software Verification Results\n\nThe Software Verification Results are produced by the software verification process activities. Software Verification Results should:",
    "a. For each review, analysis, and test, indicate each procedure that passed or failed \nduring the activities and the final pass/fail results. \nb. Identify the configuration item or software version reviewed, analyzed, or tested. \nc. Include the results of tests, reviews and analyses, including coverage analyses and \ntraceability analyses. \nAny discrepancies found should be recorded and tracked via problem reporting.",
    "Additionally, evidence provided in support of the system processes' assessment of information provided by the software processes (see 2.2.1.f and 2.2.1.g) should be considered to be Software Verification Results. \n\n## 11.15 Software Life Cycle Environment Configuration Index",
    "## 11.15 Software Life Cycle Environment Configuration Index\n\nThe Software Life Cycle Environment Configuration Index (SECI) identifies the configuration of the software life cycle environment. This index is written to aid reproduction of the hardware and software life cycle environment, for software regeneration, reverification, or software modification, and should:",
    "a. Identify the software life cycle environment hardware and its operating system \nsoftware. \nb. Identify the tools to be used during the development of the software. Examples \ninclude compilers, linkage editors, loaders, data integrity tools such as tools that calculate and embed checksums or cyclical redundancy checks, and any autocode generator with its associated options. \nc. Identify the test environment used to verify the software product, for example, the",
    "c. Identify the test environment used to verify the software product, for example, the \nsoftware testing and analysis tools. \nd. Identify qualified tools and their associated tool qualification data. \n: This data may be included in the Software Configuration Index.",
    "Note\n\n## 11.16 Software Configuration Index\n\nThe Software Configuration Index (SCI) identifies the configuration of the software product. Specific configuration identifiers and version identifiers should be provided. \n\nNote\n: The SCI can contain one data item or a set (hierarchy) of data items. The SCI can \ncontain the items listed below or it may reference another SCI or other configuration identified data that specifies the individual items and their versions. \nThe SCI should identify:",
    "a. The software product. b. Executable Object Code and Adaptation Data Item Files, if any. \nc. Each Source Code component. \nd. Previously developed software in the software product, if used. \ne. Software life cycle data. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "f. Archive and release media. \ng. Instructions for building the Executable Object Code and Adaptation Data Item Files, \nif any, including, for example, instructions and data for compiling and linking; and the procedures used to recover the software for regeneration, testing, or modification. \nh. Reference to the Software Life Cycle Environment Configuration Index (see 11.15), \nif it is packaged separately. \ni. \nData integrity checks for the Executable Object Code, if used. \nj.",
    "i. \nData integrity checks for the Executable Object Code, if used. \nj. \nProcedures, methods, and tools for making modifications to the user-modifiable \nsoftware, if any. \nk. Procedures and methods for loading the software into the target hardware. \nNote\n: The SCI may be produced for one software product version or it may be extended \nto contain data for several alternative or successive software product versions.",
    "## 11.17 Problem Reports\n\nProblem Reports are a means to identify and record the resolution to software product anomalous behavior, process non-compliance with software plans and standards, and deficiencies in software life cycle data. Problem Reports should include:",
    "a. Identification of the configuration item and/or the software life cycle process activity \nin which the problem was observed. \nb. Identification of the configuration item(s) to be modified or a description of the \nprocess to be changed. \nc. A problem description that enables the problem to be understood and resolved. The \nproblem description should contain sufficient detail to facilitate the assessment of the \npotential safety or functional effects of the problem.",
    "potential safety or functional effects of the problem. \nd. A description of the corrective action taken to resolve the reported problem.",
    "## 11.18 Software Configuration Management Records\n\nThe results of the SCM process activities are recorded in SCM Records. Examples include configuration identification lists, baseline or software library records, change history reports, archive records, and release records. These examples do not imply that records of these specific types need to be produced. \n\nNote\n: Due to the integral nature of the SCM process, its outputs will often be included \nas parts of other software life cycle data.",
    "## 11.19 Software Quality Assurance Records\n\nThe results of the SQA process activities are recorded in SQA Records. These may include SQA review or audit reports, meeting minutes, records of authorized process deviations, or software conformity review records. \n\n## 11.20 Software Accomplishment Summary\n\nThe Software Accomplishment Summary (SAS) is the primary data item for showing compliance with the Plan for Software Aspects of Approval. This summary should include:",
    "a. System overview\n:  This section provides an overview of the system, including a \ndescription of its functions and their allocation to hardware and software, the architecture, the processor(s) used, the hardware/software interfaces, and safety features. This section also describes any differences from the system overview in the Plan for Software Aspects of Approval. \nSoftware overview\nb. \n:  This section briefly describes the software functions with",
    "Software overview\nb. \n:  This section briefly describes the software functions with \nemphasis on the safety and partitioning concepts used, and explains differences from the software overview proposed in the Plan for Software Aspects of Approval. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "Approval considerations\nc. \n:  This section restates the approval considerations described \nin the Plan for Software Aspects of Approval and describes any differences. \nSoftware life cycle\nd. \n:  This section summarizes the actual software life cycle(s) and \nexplains differences from the software life cycle and software life cycle processes proposed in the Plan for Software Aspects of Approval. \ne. \nSoftware life cycle data:  This section describes any differences from the proposals",
    "e. \nSoftware life cycle data:  This section describes any differences from the proposals \nmade in the Plan for Software Aspects of Approval for the software life cycle data produced, the relationship of the data to each other and to other data defining the",
    "system, and the means by which the data was made available to the approval authority. This section explicitly references, by configuration identifiers and version, the applicable Software Configuration Index and Software Life Cycle Environment \nConfiguration Index. Detailed information regarding configuration identifiers and specific versions of software life cycle data is provided in the Software Configuration Index.",
    "f. Additional considerations\n:  This section summarizes any specific considerations that \nmay warrant the attention of the approval authority. It explains any differences from the proposals contained in the Plan for Software Aspects of Approval regarding such considerations. Reference should be made to data items applicable to these matters, such as contractual agreements or special conditions. \nSupplier oversight\ng. \n:  This section describes how supplier processes and outputs",
    "Supplier oversight\ng. \n:  This section describes how supplier processes and outputs \ncomply with plans and standards. \nSoftware identification\nh. \n:  This section identifies the software configuration by part \nnumber and version. \nSoftware characteristics\ni. \n:  This section states the Executable Object Code size, timing \nmargins including worst-case execution time, memory margins, resource limitations, and the means used for measuring each characteristic. \nChange history\nj.",
    "Change history\nj. \n:  If applicable, this section includes a summary of software changes \nwith attention to changes made due to failures affecting safety, and identifies any changes from and improvements to the software life cycle processes since the previous approval. \nSoftware status\nk. \n:  This section contains a summary of Problem Reports unresolved at",
    "Software status\nk. \n:  This section contains a summary of Problem Reports unresolved at \nthe time of approval. The Problem Report summary includes a description of each problem and any associated errors, functional limitations, operational restrictions, potential adverse effect(s) on safety, together with a justification for allowing the Problem Report to remain open, and details of any mitigating action that has been or \nneeds to be carried out. \nCompliance statement\nl.",
    "needs to be carried out. \nCompliance statement\nl. \n:  This section includes a statement of compliance with this \ndocument and a summary of the methods used to demonstrate compliance with criteria specified in the software plans. This section also addresses additional rulings made by the approval authority and any deviations from the software plans, standards and this document not covered elsewhere in the Software Accomplishment Summary. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "## 11.21 Trace Data\n\nTrace Data establishes the associations between life cycle data items contents. Trace Data should be provided that demonstrates bi-directional associations between: \n\na. System requirements allocated to software and high-level requirements. \nb. High-level requirements and low-level requirements. \nc. Low-level requirements and Source Code. \nd. Software Requirements and test cases. \ne. Test cases and test procedures. \nf. Test procedures and test results.",
    "## 11.22 Adaptation Data Item File\n\nThe Adaptation Data Item File consists of a form of data that is directly usable by the processing unit of the target computer. \n\nSoftware life cycle data should be produced for each instantiation of an Adaptation Data Item. If packaged separately, this data should include a reference to the Software Accomplishment Summary of the associated Executable Object Code. \n\n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n\n## 12.0 Additional Considerations",
    "The previous sections of this document provide guidance for satisfying approval requirements in which the applicant submits evidence of the software life cycle processes as described in those sections. This section provides guidance on additional considerations where objectives and/or activities may replace, modify, or add to some or all of the objectives and/or activities defined in the rest of this document. The use of additional considerations and the proposed impact on the guidance provided",
    "document. The use of additional considerations and the proposed impact on the guidance provided in the other sections of this document should be agreed on a case by case basis with the approval authorities.",
    "## 12.1 Use Of Previously Developed Software",
    "The guidance of this section discusses the issues associated with the use of previously developed software, including the assessment of modifications; the effect of changing a CNS/ATM installation, application environment or development environment; upgrading a development baseline; and SCM and SQA considerations. The intention to use previously developed software is stated in the Plan for Software Aspects of Approval. Unresolved Problem Reports associated with the previously developed software",
    "Aspects of Approval. Unresolved Problem Reports associated with the previously developed software should be evaluated for impact.",
    "## 12.1.1 Modifications To Previously Developed Software\n\nThis guidance discusses modifications to previously developed software where the outputs of the previous software life cycle processes comply with this document. Modification may result from requirement changes, the detection of errors, and/or software enhancements. Activities include:",
    "a. The revised outputs of the system safety assessment process should be reviewed \nconsidering the proposed modifications. \nb. If the assurance level is revised, the guidance of section 12.1.4 should be considered. \nc. Both the impact of the software requirements changes and the impact of software \narchitecture changes should be analyzed, including the consequences of software",
    "architecture changes should be analyzed, including the consequences of software \nrequirement changes upon other requirements and the coupling between several software components that may result in reverification effort involving more than the modified area. \nd. The area affected by a change should be determined. This may be done by data flow \nanalysis, control flow analysis, timing analysis, traceability analysis, or a combination of these analyses.",
    "e. Areas affected by the change should be reverified in accordance with section 6.",
    "## 12.1.2 Reuse Of Previously Approved Software In A Cns/Atm System\n\nCNS/ATM systems or equipment containing software that has been previously approved at a certain assurance level and under a specific approval basis may be used in a new installation. Activities include:",
    "a. The system safety assessment process assesses the new CNS/ATM system and \ndetermines the required assurance level. No additional effort for the previously approved software will be required if compliance can be shown for this assurance level. \nb. If functional modifications are required for the previously approved software, the \nguidance of section 12.1.1 should be satisfied. \nc. If the previous development activity did not produce outputs required to substantiate",
    "c. If the previous development activity did not produce outputs required to substantiate \nthe system safety objectives for the CNS/ATM system under consideration, the \nguidance of section 12.1.4 should be satisfied.",
    "## 12.1.3 Change Of Application Or Development Environment",
    "Use and modification of previously developed software may involve a new development environment, a new target processor or other hardware, or integration with other software than that used for the original application. New development environments may increase or reduce some activities within the software life cycle. New application environments may require activities in addition to software life cycle process activities that address modifications. Changes to an application or development",
    "life cycle process activities that address modifications. Changes to an application or development environment should be identified, analyzed and reverified. Activities include:",
    "a. If a new development environment uses software tools, the guidance of section 12.2 \nmay be applicable. \nb. The rigor of the evaluation of an application change should consider the complexity",
    "b. The rigor of the evaluation of an application change should consider the complexity \nand sophistication of the programming language. For example, the rigor of the evaluation for Ada generics will be greater if the generic parameters are different in the new application. For object-oriented languages, the rigor will be greater if the objects that are inherited are different in the new application. \nc. Using a different autocode generator or a different set of autocode generator options",
    "c. Using a different autocode generator or a different set of autocode generator options \nmay change the Source Code or object code generated. The impact of any changes should be analyzed. \nd. If a different compiler or different set of compiler options are used, resulting in \ndifferent object code, the results from a previous software verification process",
    "different object code, the results from a previous software verification process \nactivity using the object code may not be valid and should not be used for the new application. In this case, previous test results may no longer be valid for the structural coverage criteria of the new application. Similarly, compiler assumptions about optimization may not be valid. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "e. If a different processor is used, then a change impact analysis is performed to \ndetermine: \n1. Software components that are new or will need to be modified as a result of \nchanging the processor, including any modification for hardware/software \nintegration. \n2. The results from a previous software verification process activity directed at the \nhardware/software interface that may be used for the new application.",
    "hardware/software interface that may be used for the new application. \n3. Previous hardware/software integration tests that should be executed for the new \napplication. It is expected that there will always be a minimal set of tests to be run. \n4. Additional hardware/software integration tests and reviews that may be \nnecessary. \nf. If a hardware item, other than the processor, is changed and the design of the \nsoftware isolates the interfacing modules from other modules, then a change impact",
    "software isolates the interfacing modules from other modules, then a change impact \nanalysis should be performed to: \n1. Determine the software modules or interfaces that are new or will be modified to \naccommodate the changed hardware component. \n2. Determine the extent of reverification required. \ng. Verification of software interfaces should be conducted where previously developed",
    "g. Verification of software interfaces should be conducted where previously developed \nsoftware is used with different interfacing software. A change impact analysis may be used to determine the extent of reverification required.",
    "## 12.1.4 Upgrading A Development Baseline\n\nGuidance follows for software whose software life cycle data from a previous application are determined to be inadequate or do not satisfy the objectives of this document, due to the requirements associated with a new application. This guidance is intended to aid in satisfying the objectives of this document when applied to:",
    "- \nCommercial Off-The-Shelf software. \n- \nCNS/ATM System software developed to other guidance. \n- \nCNS/ATM System software developed prior to the existence of this document. \n- \nSoftware previously developed to this document at a lower assurance level. \nActivities for upgrading a development baseline include: \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "a. The objectives of this document should be satisfied while taking advantage of \nsoftware life cycle data of the previous development that satisfy the objectives for the new application. \nb. Software aspects of approval should be based on the failure conditions and assurance \nlevel(s) as determined by the system safety assessment process. Comparison to \nfailure conditions of the previous application will determine areas that may need to be upgraded.",
    "c. Software life cycle data from a previous development should be evaluated to ensure \nthat the software verification process objectives of the assurance level are satisfied \nfor the new application to the necessary level of rigor and independence. \nd. Reverse engineering may be used to regenerate software life cycle data that is",
    "d. Reverse engineering may be used to regenerate software life cycle data that is \ninadequate or missing in satisfying the objectives of this document. In addition to producing the software product, additional activities may need to be performed to satisfy the software verification process objectives. \ne. If use of product service experience is planned to satisfy the objectives of this \ndocument in upgrading a development baseline, section 12.3.4 should be considered.",
    "document in upgrading a development baseline, section 12.3.4 should be considered. \nf. The applicant should specify the strategy for accomplishing compliance with this \ndocument in the Plan for Software Aspects of Approval.",
    "## 12.1.5 Software Configuration Management Considerations\n\nIf previously developed software is used, the software configuration management process activities for the new application should include, in addition to the activities of section 7:",
    "a. Providing traceability from the software product and software life cycle data of the \nprevious application to the new application. \nb. Providing change control that enables problem reporting, problem resolution, and \ntracking of changes to software components used in more than one application. \n\n## 12.1.6 Software Quality Assurance Considerations\n\nIf previously developed software is used, the software quality assurance activities should include, in addition to the activities of section 8:",
    "a. Providing assurance that the software components satisfy or exceed the software life \ncycle criteria of the assurance level for the new application. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n\nb. Providing assurance that changes to the software life cycle processes are stated in the \nsoftware plans. \n\n## 12.2 Tool Qualification 12.2.1 Determining If Tool Qualification Is Needed",
    "Qualification of a tool is needed when processes of this document are eliminated, reduced, or automated by the use of a software tool without its output being verified as specified in section 6. The purpose of the tool qualification process is to ensure that the tool provides confidence at least equivalent to that of the process(es) eliminated, reduced, or automated. The tool qualification process may be applied to a single tool, a collection of tools, or one or more functions within a tool.",
    "may be applied to a single tool, a collection of tools, or one or more functions within a tool. For a tool with multiple functions, if protection of tool functions can be demonstrated, only those functions that are used to eliminate, reduce, or automate software life cycle processes, and whose outputs are not verified, need be qualified. Protection is the use of a mechanism to ensure that a tool function cannot adversely impact another tool function.",
    "A tool is qualified only for use on a specific system where the intention to use the tool is stated in the Plan for Software Aspects of Approval that supports the system. If a tool previously qualified on one system is proposed for use on another system, it should be requalified within the context of that other system. \n\n## 12.2.2 Determining The Tool Qualification Level",
    "## 12.2.2 Determining The Tool Qualification Level\n\nIf tool qualification is needed, the impact of the tool use in the software life cycle processes should be assessed in order to determine its tool qualification level (TQL). The following criteria should be used to determine the impact of the tool:",
    "a. Criteria 1\n: A tool whose output is part of the CNS/ATM software and thus could \ninsert an error. \nCriteria 2\nb. \n: A tool that automates verification process(es) and thus could fail to detect \nan error, and whose output is used to justify the elimination or reduction of: \n1. Verification process(es) other than that automated by the tool, or \n2. Development process(es) that could have an impact on the CNS/ATM software. \nc. Criteria 3",
    "2. Development process(es) that could have an impact on the CNS/ATM software. \nc. Criteria 3\n: A tool that, within the scope of its intended use, could fail to detect an \nerror. \nIf the tool eliminates, reduces, or automates processes in this document and its output is not verified as specified in section 6, the appropriate TQL is as shown in Table 12-1\n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n. Five levels of tool qualification are identified based on the tool use and its potential impact in the software life cycle processes. TQL-1 is the most rigorous level and TQL-5 is the least rigorous.When assessing the impact of a given tool, the criteria should be considered sequentially from criteria 1 to criteria 3. The tool qualification level should be coordinated with the approval authority as early as possible.",
    "Assurance Level \nCriteria \n1 \n2 \n3 \nAL1 \nTQL-1 \nTQL-4 \nTQL-5 \nAL2 \nTQL-2 \nTQL-4 \nTQL-5 \nAL3 \nTQL-3 \nTQL-5 \nTQL-5 \nAL4 \nTQL-4 \nTQL-5 \nTQL-5 \nAL5 \nTQL-4 \nTQL-5 \nTQL-5 \n\n## 12.2.3 Tool Qualification Process\n\nThe objectives, activities, guidance, and life cycle data required for each Tool Qualification Level are described in DO-330, \"Software Tool Qualification Considerations.\" \n\n## 12.3 Alternative Methods",
    "Some methods were not discussed in the previous sections of this document because of inadequate maturity at the time this document was written or limited applicability for CNS/ATM software. It is not the intention of this document to restrict the implementation of any current or future methods. Any single alternative method discussed in this section may be used in satisfying one or more of the objectives in this document. In addition, alternative methods may be used to support one another. An",
    "in this document. In addition, alternative methods may be used to support one another. An alternative method cannot be considered in isolation from the suite of software development processes. The effort for obtaining approval credit of an alternative method is dependent on the assurance level and the impact of the alternative method on the software life cycle processes. Guidance for using an alternative method includes:",
    "a. An alternative method should be shown to satisfy the objectives of this document or \nthe applicable supplement. \nb. The applicant should specify in the Plan for Software Aspects of Approval, and \nobtain agreement from the approval authority for: \n1. The impact of the proposed method on the software development processes. \n2. The impact of the proposed method on the software life cycle data. 3. The rationale for use of the alternative method that shows that the system safety",
    "objectives are satisfied. One technique for presenting the rationale for using an alternative method is an assurance case, in which arguments are explicitly given to link the evidence to the claims of compliance with the system safety \nobjectives. \nc. The rationale should be substantiated by software plans, processes, expected results, \nand evidence of the use of the method.",
    "## 12.3.1 Exhaustive Input Testing\n\nThere are situations where the software component of a CNS/ATM system or equipment is simple and isolated such that the set of inputs and outputs can be bounded. If so, it may be possible to demonstrate that exhaustive testing of this input space can be substituted for one or more of the software verification process activities identified in section 6. \n\nFor this alternative method, activities include:",
    "For this alternative method, activities include: \n\na. Defining the complete set of valid inputs and outputs of the software. \nb. Performing an analysis that confirms the isolation of the inputs to the software. \nc. Developing rationale for the exhaustive input test cases and procedures. d. Developing the test cases, test procedures, and test results. \n\n## 12.3.2 Considerations For Multiple-Version Dissimilar Software Verification",
    "## 12.3.2 Considerations For Multiple-Version Dissimilar Software Verification\n\nGuidance follows concerning the software verification process as it applies to multipleversion dissimilar software. If the software verification process is modified because of the use of multiple-version dissimilar software, evidence should be provided that the software verification process objectives are satisfied and that equivalent error detection is achieved for each software version.",
    "Multiple, dissimilar versions of the software are produced using combinations of these techniques:",
    "- \nThe Source Code is implemented in two or more different programming languages. \n- \nThe object code is generated using two or more different compilers. \n- \nEach software version of Executable Object Code executes on a separate, dissimilar \nprocessor, or on a single processor with the means to provide partitioning between \nthe software versions. \n- \nThe software requirements, software design, and/or Source Code are developed by \ntwo or more development teams whose interactions are managed.",
    "two or more development teams whose interactions are managed. \n- \nThe software requirements, software design, and/or Source Code are developed in \ntwo or more software development environments, and/or each version is verified using separate test environments. \n- \nThe Executable Object Code is linked and loaded using two or more different linkage editors and two or more different loaders. \n- \nThe software requirements, software design, and/or Source Code are developed in",
    "- \nThe software requirements, software design, and/or Source Code are developed in \nconformance with two or more different Software Requirements Standards, Software Design Standards, and/or Software Code Standards, respectively.",
    "When multiple versions of software are used, the software verification methods may be modified from those used to verify single version software. They will apply to software development process activities that are multi-thread, such as separate, multiple development teams. The software verification process is dependent on the combined hardware and software architectures since this affects the dissimilarity of the multiple software versions. Additional software verification process objectives to",
    "of the multiple software versions. Additional software verification process objectives to be satisfied are to demonstrate that:",
    "a. Inter-version compatibility requirements are satisfied, including compatibility during \nnormal and abnormal operations and state transitions. \nb. Equivalent error detection is achieved. \nOther changes in software verification process activities may be agreed with the approval authority, if the changes are substantiated by rationale that confirms equivalent software verification coverage. \n\n## 12.3.2.1 Independence Of Multiple-Version Dissimilar Software",
    "## 12.3.2.1 Independence Of Multiple-Version Dissimilar Software\n\nWhen multiple-version dissimilar software versions are developed independently using a managed method, the development processes have the potential to reveal certain classes of errors such that verification of each software version is equivalent to independent verification of the software development processes. Activities include:",
    "a. The applicant should demonstrate that different teams with limited interaction \ndeveloped each software version's software requirements, software design, and Source Code. \nb. Independent test coverage analyses should still be performed as with a single version. \nNote\n: Section 12.3.2.1 only addresses the subject of independence. Reduction of \nassurance levels is not discussed or intended. \n\n## 12.3.2.2 Multiple Processor-Related Verification",
    "When each version of dissimilar software executes on a different type of processor, the verification of some aspects of compatibility of the code with the processor (see 6.4.3.c) \nmay be replaced by verification to ensure that the multiple types of processor produce the correct outputs. This verification consists of integration tests in which the outputs of the multiple versions are cross-compared in requirements-based test cases. The applicant should complete the following activities:",
    "a. Show that equivalent error detection is achieved. \nb. Show that each processor was designed by a different developer. c. Show that the outputs of the multiple versions are equivalent. \n\n## 12.3.2.3 Multiple-Version Source Code Verification",
    "The guidance for structural coverage analysis (see 6.4.4.2) may be modified for systems or equipment using multiple-version dissimilar software. For structural coverage analysis, the accompanying AL1 activity (see 6.4.4.2.b) to evaluate any additional object code \n(generated by a compiler, linker, or other means) that is not directly traceable to Source Code statements need not be done provided that the applicant completes the following activities:",
    "a. Show that each version of software is coded using a different programming language. \nb. Show that each compiler used is from a different developer. \n\n## 12.3.2.4 Tool Qualification For Multiple-Version Dissimilar Software",
    "If multiple-version dissimilar software is used, the tool qualification process may be modified, if evidence is available that the multiple software tools used in the software development process are dissimilar. This depends on the demonstration of equivalent software verification process activity in the development of the multiple software versions using dissimilar software tools. The applicant should complete the following activities:",
    "a. Show that each tool was obtained from a different developer. \nb. Show that each tool has a dissimilar design. \n\n## 12.3.2.5 Multiple Simulators And Verification",
    "If separate, dissimilar simulators are used to verify multiple-version dissimilar software versions, then the approach to tool qualification of the simulators may be modified. This depends on the demonstration of equivalent software verification process activity in the simulation of the multiple software versions using multiple simulators. Unless it can be justified as unnecessary, for multiple simulators to be dissimilar, that the applicant should complete the following activities:",
    "a. Provide evidence that each simulator was developed by a different team. b. Provide evidence that each simulator has different requirements, a different design, \nand a different programming language. \nc. Provide evidence that each simulator executes on a different processor. \nNote\n: When a multiple processor system using multiple, dissimilar versions of software",
    "Note\n: When a multiple processor system using multiple, dissimilar versions of software \nare executing on identical processors, it may be difficult to demonstrate dissimilarity of simulators because of the reliance on information obtained from a common source, for example, the processor manufacturer.",
    "## 12.3.3 Software Reliability Models\n\nMany methods for predicting software reliability based on developmental metrics have been published, for example, software structure, defect detection rate, etc. This document does not provide guidance for those types of methods, because at the time of writing, the available methods dio not provide an adequate level of confidence. \n\n## 12.3.4 Service Experience",
    "## 12.3.4 Service Experience\n\nIf equivalent safety for the software can be demonstrated by the use of the software's product service experience, some approval credit may be granted. The acceptability of this method is dependent on:",
    "- \nConfiguration management of the software. \n- \nEffectiveness of problem reporting activity. \n- \nStability and maturity of the software. \n- \nRelevance of product service experience environment. \n- \nLength of the product service experience. \n- \nNumber and severity of failures observed during the product service experience. \n- \nImpact of modifications.",
    "- \nImpact of modifications. \nUse of service experience data for assurance credit is predicated upon sufficiency, relevance, and types of problems encountered. The use, conditions of use, and results of software service experience should be defined, assessed by the system processes, including the system safety assessment process, and submitted to the appropriate",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,, approval authority. Guidance for determining applicability of service experience and the length of service experience needed is presented below.",
    "## 12.3.4.1 Relevance Of Service Experience\n\nThe following applies for establishing the relevance of service experience:",
    "a. Type of experience\n:  Applicable service experience time should consist of either \naccumulated in-service hours or accumulated time under test or evaluation. \nKnown configuration\nb. \n:  The applicant should show that the software, operating \nenvironment, and associated evidence used to comply with system safety objectives have been under configuration management throughout the service experience. \nOperating time collection process\nc. \n:  The applicant should show that the means of",
    "Operating time collection process\nc. \n:  The applicant should show that the means of \ncollecting operating time for continuous operation or number of events for eventbased operation is accurate and complete. Operating time or event data should \naccount for changes in any factors that are important to the intended application \nincluding but not limited to: \n1. Software configuration. \n2. Operational mode or state.",
    "including but not limited to: \n1. Software configuration. \n2. Operational mode or state. \n3. System workload, including timing of events, necessary precision of calculations, \nand range of inputs. \n4. Operating environment. \nd. Changes to the software\n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    ": Configuration changes during the product service \nexperience should be identified. An analysis should be conducted to determine whether the changes made to the software alter the applicability of the service experience data for the period preceding the changes. Uncontrolled changes to the \nExecutable Object Code during the service experience may invalidate the use of",
    "Executable Object Code during the service experience may invalidate the use of \nservice experience. The change process should be assessed to determine whether safety-related problems are appropriately tested. \nUsage and Environment\ne. \n:  The intended software usage should be analyzed to show \nthe relevance of the service experience. \n1. It should be assured that software capabilities to be used are exercised in all \noperational modes.",
    "operational modes. \n2. Analysis should also be performed to assure that relevant permutations of input \ndata are executed. \n3. If the usage during the service experience prevented some classes of abnormal \ninputs from reaching the software to be approved, then the applicant needs to either show that similar usage will apply in the intended application or supplement the service experience with additional testing or analysis of those abnormal conditions.",
    "4. The operating environment used to collect the service experience data should be \nassessed to show relevance to the intended use in the proposed application. If the \noperating environments of the existing and proposed applications differ, additional software verification should confirm compliance with the system safety objectives in the target environment.",
    "5. If credit is being sought for compatibility with the hardware environment, then \nthe relationship between the service experience environment and the intended environment should be addressed. The impact of any hardware modifications during the service experience period should also be assessed. \nf. Deactivated code\n:  If the software is a subset of the software that was active during",
    "the service period, then analysis should confirm the equivalency of the new environment with the previous environment. Analysis should also determine those software components that were not executed during normal operation, for example, code that was deactivated. Deactivated code should be shown to provide no adverse effects on system operations, either by showing that it cannot be invoked or by showing that, if inadvertently invoked, its effect on the system is acceptable.",
    "Recovery from failures\ng. \n:  When credit is sought for recovery mechanisms in either the \nsoftware itself or in other system elements interfacing to the software, the following issues should be addressed \n1. Definition of a successful failure recovery. 2. The methods by which recovery effectiveness and recovery time are calculated. 3. Identification and analysis of the causes of unsuccessful recoveries, and analysis \nof the impact of such recovery failures on the intended application.",
    "## 12.3.4.2 Sufficiency Of Accumulated Service Experience\n\nThe required amount of service experience is determined by:",
    "The required amount of service experience is determined by: \n\na. The system safety objectives of the software and the assurance level. \nb. The level of confidence required that the safety objectives have been achieved. \nc. Any differences in service experience environment and system operational \nenvironment. \nd. The objectives from sections 4 to 9 being addressed by service experience. \ne. Evidence, in addition to service experience, addressing those objectives.",
    "## 12.3.4.3 Collection, Reporting, And Analysis Of Problems Found During Service Experience",
    "a. Problem reporting process\n:  The applicant should show that problem reporting during \nthe product service experience period provides assurance that representative data is available and that problems during the service experience period were reported and recorded, and are retrievable. The specific data to be collected should be agreed on with the approval authority and should include the following for each recorded problem, in addition to the items in section 11.17:",
    "1. The time each problem occurred. \n2. The hardware/software configuration in effect when the problem occurred. \n3. The operating environment within which the problem occurred. \n4. The operating mode or state within which the problem occurred. 5. Any application-specific information needed for problem assessment. \n6. Classification of the problem with respect to severity, safety significance, and",
    "6. Classification of the problem with respect to severity, safety significance, and \nwhether the problem was the result of a change in the software configuration since the start of service experience data collection. \n7. Assessment of whether the problem was: \ni. \nReproducible. \nii. Recoverable. \niii. Related to other previously reported problems, including, but not limited to, a \ncommon cause. \niv. Related to other simultaneously occurring problems, that is, cascaded or \ncorrelated failures.",
    "iv. Related to other simultaneously occurring problems, that is, cascaded or \ncorrelated failures. \nThe chronological trend of Problem Reports should be evaluated and any increasing trend explained. The completeness of the software's error history should also be addressed. This includes:",
    "- \nThe ability to detect failures and maintain a record of these failures. \n- \nThe means for users to report problems. \n- \nThe completeness of the software supplier's problem records. \n- \nThe means for the applicant to determine the system safety impact of any open Problem Reports, and to determine whether problems that were not safety-related",
    "during the service experience will be safety-related in the intended environment. Problems that were not safety-related in the service experience environment, but which will be safety-related in the intended environment, might indicate the need \nfor additional verification. \n- \nThe means for the applicant to determine the number of occurrences of a specific problem. \nb. Process-related problems\n:  Those problems that are indicative of an inadequate",
    "b. Process-related problems\n:  Those problems that are indicative of an inadequate \ndevelopment process, such as design or code errors, should be indicated separately from those whose cause are outside the scope of this document, such as hardware failures or system requirements errors. \nc. \nSafety-related problems:  All in-service problems should be evaluated for their",
    "c. \nSafety-related problems:  All in-service problems should be evaluated for their \npotential adverse effect on the intended system operation. Any problem during service experience time, where software implication is established and whose",
    "resulting effect on CNS/ATM operations is not consistent with the safety assessment, should be recorded. Any such safety-related problem should be considered a failure. Failures may invalidate the use of related service experience data for the period of service experience time preceding the correction of that problem.",
    "## 12.3.4.4 Service Experience Information To Be Included In The Plan For Software Aspects Of Approval\n\nThe following items should be specified in the Plan for Software Aspects of Approval and agreed with the approval authority when seeking approval credit for service experience:",
    "a. Rationale for claiming relevant service experience, addressing the items in section \n12.3.4.1. \nb. Length of service experience needed together with the rationale. This should include \nthe items in section 12.3.4.2, any censoring rules for data used in estimation, and \nmeasured parameters, if applicable. This data should be provided using measures \nrelevant to the operations of the system. \nc. Rationale for calculating the number of hours in service, including factors such as",
    "c. Rationale for calculating the number of hours in service, including factors such as \noperational modes, the number of independently operating copies in the installation",
    "and in service, and the definition of \"normal operation\" and \"normal operation time.\" Where a number of operating copies are to be taken into account in calculating service experience time, each copy and its associated operating environment should be shown to be relevant, and a single copy should account for a certain pre-agreed percentage of the total. In order for operating copies to be taken into account, each should be running a different input data set or application.",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "Note\n: If the error rate is greater than that identified in the plan, these errors should \nbe analyzed and the analyses reviewed with the approval authority. The length of the service experience period may need to be extended or service \nexperience may be inapplicable as an alternative means of compliance. \nd. Definition of what was counted as an error and rationale for that definition. This \nshould address the items in section 12.3.4.3.a.",
    "should address the items in section 12.3.4.3.a. \ne. Maximum number and severity of failures that could be accepted during the service \nexperience period and rationale for both the period and number of acceptable failures \nin relation to the system's safety objectives. This should address the items in section \n12.3.4.3.b and 12.3.4.3.c. \nf. Definition of criteria for problems that would invalidate service experience under \nsection 12.3.4.3 or for other reasons.",
    "section 12.3.4.3 or for other reasons. \ng. Criteria for errors that will be corrected, how they will be corrected and verified, and \nrationale for any defects for which no action will be taken. \nh. Objectives in sections 4 to 9 to be addressed through the use of service experience. \nSee section 12.4.11.2 for further guidance on this topic.",
    "See section 12.4.11.2 for further guidance on this topic. \nIf the number or severity of failures encountered is greater than that identified in the plan, these failures should be analyzed and the analyses reviewed with the approval authority.",
    "The length of the service experience period may need to be extended or service experience may be inapplicable as an alternative means of compliance. \n\n## 12.4 Commercial Off-The-Shelf Software 12.4.1 Introduction",
    "The use of COTS software has been widely adopted in software development projects for CNS/ATM systems. Examples of COTS software include operating systems, real-time kernels, user interface software, communication and telecommunication protocols, runtime libraries, and data management systems. COTS software can be purchased apart from or in conjunction with COTS hardware, such as workstations, communication and network equipment, or hardware items (for example, memory, storage, I/O devices).",
    "communication and network equipment, or hardware items (for example, memory, storage, I/O devices). Instances may exist where the use of COTS software is impractical to avoid, for example, library code associated with certain compilers.",
    "Development processes used by COTS software suppliers and procurement processes applied by acquirers may not be equivalent to processes used in this document, and may not be fully consistent with the objectives identified in sections 4 to 9. Nevertheless, it is essential that the level of confidence for COTS software is the same as for any other software used for CNS/ATM systems. \n\n## 12.4.1.1 Purpose",
    "The purpose of this section is to provide a common framework for the evaluation and acceptance of COTS software used in CNS/ATM systems such that the level of confidence for the COTS software is the same as for other software that complies with the guidance in sections 4 to 9. Compliance with sections 4 to 9 is the primary means of obtaining approval of software used in CNS/ATM products. Since COTS software development may not have followed theses processes or may not have supporting data",
    "COTS software development may not have followed theses processes or may not have supporting data available, satisfying some of the objectives in sections 4 to 9 when using COTS software is problematic.",
    "This section provides the following: \n\na. Additional objectives for COTS software life cycle processes. \nb. A description of activities and considerations for achieving those objectives. \nc. A description of the evidence that demonstrates that the objectives have been \nsatisfied. \nd. Some alternative strategies to provide assurance for COTS software that may have \nno, or only partial, evidence of compliance to the objectives in sections 4 to 9. \n\n## 12.4.1.2 Scope",
    "This section applies only to COTS software used for CNS/ATM applications and is not intended to alter or substitute any of the objectives stated in sections 4 to 9, unless justified and accepted by the appropriate approval authority. COTS software encompasses a wide range of software, including purchased software and software previously developed without consideration of this document. This software may or may not have been approved through other \"approval processes.\" Partial data or no data",
    "may or may not have been approved through other \"approval processes.\" Partial data or no data may be available as evidence of objectives of the CNS/ATM developmental processes. For the rest of this section, all such software is referred to as COTS software for the sake of brevity.",
    "This terminology was selected because of the usual use of the term \"COTS\" within the CNS/ATM community. Although COTS software differs from software developed to satisfy the objectives in sections 4 to 9, it is still necessary to provide assurance that the same level of confidence in the software has been achieved as would be the case had the objectives been met. If COTS software is integrated into the development of CNS/ATM development as software then the software integration and",
    "into the development of CNS/ATM development as software then the software integration and hardware/software integration objectives of this document apply. COTS deliverables vary by the contract with the COTS software supplier. They may extend from license rights, executable code, user documentation, and training to the full set of COTS life cycle data, including the Source Code resulting from the COTS software development. COTS software information disclosure relates to cost, protection of",
    "the COTS software development. COTS software information disclosure relates to cost, protection of intellectual properties, and legal questions, for example, ownership of the software, patents, liability, and documentation responsibility. Ownership and commercial and liability considerations are beyond the scope of this guidance material, which addresses only those aspects that are specific to software integrity assurance.",
    "Note\n: COTS software usage may necessitate the development of additional code to \nintegrate the COTS software into the CNS/ATM system. Any such software should be considered CNS/ATM developmental software for which all of the objectives in the main body of this document apply. \n\n## 12.4.1.3 Overview Of Approach\n\nThe approach to gain assurance for COTS software is to:",
    "a. Develop and agree on a plan that defines the process for acquiring and integrating the \nCOTS software including transition criteria, additional considerations, integration, and maintenance. Additional objectives to those contained in sections 4 to 9 should be satisfied when the guidance in this section is applied to COTS software. The planned activities are recorded in the PSAA (see 12.4.4). \nb. Conduct a gap analysis to identify the extent to which the objectives in sections 4 to 9",
    "b. Conduct a gap analysis to identify the extent to which the objectives in sections 4 to 9 \nappropriate to the assurance level can be demonstrated to be achieved for the COTS software. Additional objectives to those contained in sections 4 to 9 should to be satisfied when the guidance in this section is applied for COTS software. The results of the gap analysis are recorded in the COTS Software Integrity Assurance Case. See sections 12.4.8b and 12.4.10.",
    "c. Identify and document how assurance will be provided for any gaps in meeting the \nobjectives in sections 4 to 9 appropriate to the assurance level and gain agreement from the approval authority. The approach to achieve assurance is documented in the \nCOTS Software Integrity Assurance Case. Further guidance on how this can be achieved is detailed in sections 12.4.5.2.e and 12.4.11. \nd. Identify any derived requirements for the COTS software and provide these to the",
    "d. Identify any derived requirements for the COTS software and provide these to the \nsystem safety assessment process. Although this is an objective in section 5, it is \nincluded in this process as it is particularly important for COTS software as part of the assurance strategy. Additional activities are identified for COTS software to satisfy this need. See sections 12.4.4.2.a and 12.4.10.",
    "e. Provide evidence of satisfying the objectives in sections 4 to 9 appropriate to the \nassurance level which can be achieved (see 12.4.1.3.b). The evidence will be \nreferenced from the COTS Software Integrity Assurance Case. The evidence and outputs required are the same as required for any other software satisfying these objectives. \nf. When the gap analysis identifies gaps against objectives for the assurance level,",
    "f. When the gap analysis identifies gaps against objectives for the assurance level, \nprovide assurance that the same level of confidence in the software has been achieved as would be the case had the objectives been met. The assurance will be documented in a COTS Software Integrity Assurance Case and should be agreed with the \napproval authority. Further guidance on how this can be achieved is detailed in sections 12.4.5.2 and 12.4.11.",
    "## 12.4.2 System Aspects Of Cots Software",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "COTS software may need to be integrated into high integrity CNS/ATM systems or equipment. Risk mitigation techniques may be considered to reduce the CNS/ATM system's reliance on the COTS software. The goal of these mitigation techniques is to accommodate the associated failure condition by reducing the effect of anomalous behavior of COTS software on the CNS/ATM system function. Risk mitigation techniques may be achieved through a combination of people, procedures, equipment, or architecture.",
    "One practical approach is to use architectural means to reduce the number of software components that need to be produced to the more stringent assurance level. Techniques that can enable this include partitioning, redundancy, integrity monitoring, COTS software safe subsets by the use of encapsulation or wrappers, data integrity checking, isolation, command and monitoring, and dissimilarity with comparators. \n\n## 12.4.2.1 The Cots Software Integrity Assurance Case",
    "## 12.4.2.1 The Cots Software Integrity Assurance Case\n\nThe COTS Software Integrity Assurance Case is the vehicle through which the assurance level of the COTS software is assessed to be at least as high as that of software approved by directly satisfying the objectives in sections 4 to 9.",
    "A COTS Software Integrity Assurance Case documents the rationale for demonstrating that, to an acceptable degree, the software meets the requirements imposed upon it by the system within which it operates. A COTS Software Integrity Assurance Case:",
    "a. Embodies the complete rationale for demonstrating that the software meets the stated \nrequirements and objectives. \nb. Receives acceptance from the approval authority.",
    "A COTS Software Integrity Assurance Case is composed in a rigorous manner from claims, arguments, evidence, assumptions, justifications, and strategies. A software integrity assurance argument does not itself replace any software verification process activity. Rather, it provides a means of arguing that the necessary software assurance has been achieved. It also includes an argument for the rationale for using alternative methods to satisfy objectives.",
    "Further details on the content of the COTS Software Integrity Assurance Case are contained in section 12.4.8.1. \n\n## 12.4.3 Cots Software Planning Process",
    "The purpose of the COTS software planning process is to coordinate life cycle processes to acquire and integrate the COTS software including transition criteria, additional considerations, integration, and maintenance. This includes defining the methods and tools necessary for the incorporation of COTS software in CNS/ATM systems. The COTS software planning process is a sub-activity of the CNS/ATM software planning process. The verification of the COTS software planning process is to assure",
    "software planning process. The verification of the COTS software planning process is to assure that all issues regarding the use of COTS software have been addressed.",
    "As part of the approval process, early submittal of the results of the COTS software assessment and selection processes to the appropriate approval authority, is recommended. The objectives and activities defined in this section are additional to the objectives and activities defined in section 4. \n\n## 12.4.3.1 Cots Software Planning Process Objectives\n\n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nThe objectives of the COTS software planning process are:",
    "a. Activities for acquisition and integral processes, including selection criteria, \nintegration, and maintenance, are defined. \nb. COTS software transition criteria for these processes and inter-relationships with \nCNS/ATM life cycle processes, including sequencing, feedback mechanisms, and \ntransition criteria, are defined. \nc. Plans for COTS software processes, including COTS software transition criteria, are \ncoordinated with the CNS/ATM software plans.",
    "## 12.4.3.2 Cots Software Planning Process Activities\n\nThe activities associated with the COTS software planning process are:",
    "a. COTS software planning activities should evaluate the applicability of the COTS \nsoftware product to CNS/ATM requirements and process objectives. The following considerations should be included in the evaluation to determine whether the COTS software item may be suitable for the intended application: \n1. Product availability. \n2. Product requirements. \n3. Availability of life cycle data and compliance with objectives in sections 4 to 9.",
    "3. Availability of life cycle data and compliance with objectives in sections 4 to 9. \n4. Level of integration and extent of additional efforts, such as, additional interface \ncode, architecture mitigation techniques, etc., to allow incorporation of the COTS software into the CNS/ATM system. \n5. Planning for a gap analysis and identification of available assurance activities \napplicable to fill any gaps in meeting the objectives in sections 4 to 9 appropriate to the assurance level.",
    "6. Availability of alternative arguments and evidence to address gaps in the \nassurance that objectives in sections 4 to 9 are satisfied. \n7. Configuration control, including visibility of COTS software supplier's product \nversion control. \n8. Modified COTS software has additional considerations of warranty, authority to \nmodify, continued technical support, etc. The modifications themselves should be \nconsidered a new development. Change impact analysis should be performed to",
    "considered a new development. Change impact analysis should be performed to \ndetermine the extent of the necessary re-verification. \n9. Maintenance issues, for example, patches, retirement, obsolescence, and change \nimpact analysis. \n10. Evidence of SQA activities. 11. Verifiability of the COTS software, including limitations, need for special test \nfacilities, etc. \n12. Information on COTS software in-service problems, both by the current",
    "facilities, etc. \n12. Information on COTS software in-service problems, both by the current  \napplication and by the wider users of the COTS software application, and resolution of those problems. \nb. Relationships between the COTS software planning process, the COTS software \nacquisition process, and the COTS software integral processes should be defined.",
    "acquisition process, and the COTS software integral processes should be defined. \nAdditionally, relationships between COTS software processes and appropriate CNS/ATM life cycle processes should be defined. Every input to a process need not be complete before that process can be initiated, if the transition criteria established \nfor the process are satisfied. \nc. Reviews should be conducted to ensure: \n1. The COTS software planning process and the CNS/ATM planning process are \ncoordinated.",
    "1. The COTS software planning process and the CNS/ATM planning process are \ncoordinated. \n2. COTS software transition criteria are compatible with the CNS/ATM transition \ncriteria. \n3. Transition criteria are verified to assure that the outputs of each process are \nsufficient to begin the next process.",
    "## 12.4.4 Cots Software Acquisition Process",
    "The focus of this section is on the assurance aspects of acquiring COTS software. Business acquisition considerations such as purchasing and intellectual property rights are not included. While the guidance in this section applies to all assurance levels, \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nbusiness considerations using COTS software could prevent an applicant from reaching the highest level that is technically possible for that software. The COTS software acquisition process for assurance is comprised of requirements definition, assessment, and selection.",
    "a. Requirements Definition:  The CNS/ATM software requirements definition process \nidentifies software requirements that COTS software will satisfy. COTS software may contain more capabilities than the requirements needed by the CNS/ATM system. A definition of these capabilities may be available from the COTS software supplier or derived from the COTS software user's manuals, technical materials, \nproduct data, etc. In the model depicted in Figure 12-1\n, the CNS/ATM requirements",
    "product data, etc. In the model depicted in Figure 12-1\n, the CNS/ATM requirements \nsatisfied by COTS software are the intersection of COTS software capabilities and CNS/ATM requirements. The following should be considered: \n1. Due to the use of COTS software, there may be derived requirements that should \nbe added to the CNS/ATM software requirements. Examples include platform \ndependent requirements, interrupt handling, interface handling, resource",
    "dependent requirements, interrupt handling, interface handling, resource \nrequirements, usage constraints, error handling, partitioning. All derived requirements for the COTS software should be provided to the system safety assessment process. \n2. The CNS/ATM Requirements satisfied by the COTS software form the highlevel requirements for the software assurance process. \nb. Assessment:  Candidate COTS software products should be assessed against the",
    "b. Assessment:  Candidate COTS software products should be assessed against the \nselection criteria in the Plan for Software Aspects of Approval. This includes their ability to implement the CNS/ATM requirements, the effect of their respective derived requirements, and their support of the needed assurance level. During the",
    "COTS software assessment process, more than one COTS software candidate product may be examined to determine the extent of intersection of COTS software capabilities with the CNS/ATM requirements as depicted in Figure 12-1\n. Availability and relevance of COTS software life cycle data to support the appropriate assurance level should also be assessed. Additionally, the impact of any unneeded COTS \nsoftware capabilities should be assessed.",
    "Selection\nc. \n: The selection process is an iterative process based on results from the \nassessment process and comparison of COTS software suppliers, including, for \nexample, COTS software supplier's experience in CNS/ATM, the ability of the COTS software supplier to support COTS software version control and maintenance over the expected lifetime of the CNS/ATM system, and COTS software supplier's",
    "commitment to keep the CNS/ATM applicants informed of detected errors. Analyses may be conducted to compare advantages of using COTS software versus developing the software.",
    "## 12.4.4.1 Cots Software Acquisition Process Objectives\n\nThe objectives of the COTS software acquisition process are:",
    "a. The degree to which CNS/ATM software requirements are satisfied by the COTS \nsoftware capabilities is determined. \nb. The adequacy of life cycle data available for assurance purposes is determined. \nc. The derived requirements are identified. Derived requirements consist of: \n1. Requirements imposed on the CNS/ATM system due to the usage of COTS \nsoftware. \n2. Requirements to prevent the unneeded capabilities of the COTS software from \nadversely affecting the CNS/ATM system.",
    "adversely affecting the CNS/ATM system. \nd. The compatibility of COTS software with target hardware and other CNS/ATM \nsoftware is assured. \ne. The strategies to be used to address gaps in the assurance or evidence are determined \nand documented.",
    "## 12.4.4.2 Cots Software Acquisition Process Activities\n\nThe activities of the COTS software acquisition process are:",
    "a. The COTS software capabilities should be examined and an analysis should be \nconducted against the selection criteria identified in the plan. The purpose of this analysis is to determine the criteria satisfied by COTS software and to aid in the comparison of candidate COTS software products. \nb. Assessment of the available COTS software life cycle data should be completed. A",
    "b. Assessment of the available COTS software life cycle data should be completed. A \ngap analysis should be performed against the objectives in sections 4 to 9 for the proposed assurance level. This analysis aids in comparison of candidate COTS software products. This analysis is used to identify the objectives that are fully satisfied and those that are partially or not satisfied, and therefore need to be addressed through alternative strategies.",
    "c. Alternative strategies for providing assurance to fill any gaps identified in the \nassurance evidence available should be determined and agreed with the approval authority. \nd. Analysis should be conducted to identify derived requirements. This analysis should \ninclude all COTS software capabilities, including those not specifically needed to meet CNS/ATM requirements. Derived requirements may be classified as follows:",
    "1. Requirements to prevent adverse effects of any unneeded functions of any COTS \nsoftware. This may result in requirements for isolation, partitioning, wrapper code, coding directives, customization, etc. \n2. Requirements that the selected COTS software may impose on the CNS/ATM",
    "2. Requirements that the selected COTS software may impose on the CNS/ATM \nsystem including those for preventing adverse effects of needed COTS software functions, for example, input formatting, call order, initialization, data conversion, resources, and range checking. This may result in requirements for \ninterface code, coding directives, architecture considerations, resource sizing, etc. \ne. Assure that the selected COTS software is shown to be compatible with the target",
    "e. Assure that the selected COTS software is shown to be compatible with the target \ncomputer(s) and interfacing systems. \nf. Sections 12.4.4.2.a to 12.4.4.2.e should be integrated into the structure of, or \nreferenced in, the COTS Software Integrity Assurance Case. \ng. Provide all CNS/ATM requirements satisfied by COTS software, and the resulting \nderived requirements, to the system safety assessment process.",
    "## 12.4.5 Cots Software Verification Process",
    "The COTS software verification process is an extension of the verification process discussed in section 6. In particular, the COTS software acquisition process frequently identifies verification objectives that cannot be satisfied using traditional means. For those verification objectives found during the gap analysis process where compliance cannot be demonstrated by the available COTS software data (for example, design or requirements) additional activities and approaches are used to verify",
    "data (for example, design or requirements) additional activities and approaches are used to verify the adequacy of the software as agreed with the approval authority. In some cases, these may not directly satisfy the defined objective in sections 4 to 9, but the underlying intent of the objective. The objectives and activities defined in this section are additional to the objectives and activities defined in section 6.",
    "## 12.4.5.1 Cots Software Verification Process Objectives\n\nThe objectives of the COTS software verification process are:",
    "The objectives of the COTS software verification process are: \n\na. Provide evidence of satisfying the objectives in sections 4 to 9 appropriate to the \nassurance level to be achieved. \nb. For any objectives appropriate to the assurance level where there is not sufficient \nevidence available to demonstrate satisfaction, provide a set of claims, supported by arguments and evidence that the COTS software has the same integrity as would be the case had the objectives been met.",
    "## 12.4.5.2 Cots Software Verification Process Activities\n\nThe verification activities defined in section 6 are applicable to COTS software; however, there are some additional activities that typically include:",
    "a. Software reviews and analyses of CNS/ATM requirements satisfied by COTS \nsoftware. \nb. Requirements-based testing (RBT) of CNS/ATM requirements satisfied by COTS \nsoftware. \nc. Verification of development of any supplemental software due to COTS software, for \nexample, partitioning or wrappers. The supplemental software should be at the same \nassurance level as the highest assurance level associated with the functions that use the supplemental software.",
    "d. Verification of complete and correct integration of COTS software into the \nCNS/ATM system. \ne. Documenting a set of claims, supported by arguments and evidence that the COTS \nsoftware has the same integrity, as would be the case had all the objectives in sections 4 to 9 relevant to the assurance level had been met. This evidence is recorded in the COTS Software Integrity Assurance Case.",
    "## 12.4.6 Cots Software Configuration Management Process",
    "This section describes the configuration management process for a system using COTS software. The configuration management system of the COTS software supplier may not be under the control of CNS/ATM configuration management system; therefore, the CNS/ATM configuration management system should include control of the COTS software versions. The objectives and activities defined in this section are additional to the objectives and activities defined in section 7.",
    "## 12.4.6.1 Cots Software Configuration Management Process Objectives\n\nThe objectives of the COTS software configuration management process are:",
    "a. The COTS software specific configuration and data items (for example, software, \ndocumentation, or adaptation data) are uniquely identified in the CNS/ATM software configuration management system. \nb. The CNS/ATM problem reporting includes the management of problems found in \nCOTS software. \nc. The CNS/ATM change control process ensures that the incorporation of COTS \nsoftware releases is controlled. \nd. COTS software specific configuration and data items are included in the CNS/ATM",
    "d. COTS software specific configuration and data items are included in the CNS/ATM \narchive, retrieval, and release.",
    "## 12.4.6.2 Cots Software Configuration Management Process Activities\n\nThe activities associated with configuration management of COTS software are:",
    "a. An identification method should be established to ensure that the COTS software \nconfiguration and data items are uniquely identified. \nNote\n: The identification method may be based on identification from the COTS \nsoftware supplier and any additional data such as release or delivery date. \nb. The CNS/ATM problem reporting should include management of problems found in \nCOTS software. Additionally, bi-directional problem reporting mechanism with the",
    "COTS software. Additionally, bi-directional problem reporting mechanism with the \nCOTS software supplier should be established. \nc. The CNS/ATM change control process for the incorporation of updated COTS \nsoftware versions should be established. \nAn impact analysis of changes to the COTS software baseline should be performed prior to incorporation of new releases of COTS software.",
    "Note\n: The list of changes, including problem fixes and new, changed, or deleted \nfunctions, implemented in each new release may be available from the COTS software supplier. \nThe CNS/ATM archival, retrieval, and release should include COTS software specific configuration and data items. \n\nNote\n: Consideration may be given to technology obsolescence issues for accessing \narchived data. \n\n## 12.4.7 Cots Software Quality Assurance Process",
    "## 12.4.7 Cots Software Quality Assurance Process\n\nThe CNS/ATM quality assurance process (see 8) should also assess the COTS software processes and data outputs to obtain assurance that the objectives associated with COTS software are satisfied.",
    "Note\n: It is recommended that the COTS software supplier quality assurance is \ncoordinated with the CNS/ATM quality assurance process where feasible. \nSoftware conformity reviews may need to be tailored with respect to the approach used for COTS software approval. Data may also be combined from more than one method to gain assurance data that the objectives are satisfied. \n\n## 12.4.8 Software Life Cycle Data",
    "## 12.4.8 Software Life Cycle Data\n\nThis section discusses the content of new or modified life cycle data items introduced for CNS/ATM systems by the guidance for COTS software. COTS software specific life cycle data items are:",
    "a. Planning data as input to CNS/ATM planning data. In the PSAA (see 11.1), the \napplicant should propose a set of activities and outputs that encompass the COTS \nsoftware acquisition and verification objectives and activities defined in this section. \nb. Acquisition process data, including requirements definition, assessment, gap analysis, \ncriteria, and rationale for selection. In the Software Configuration Management Plan",
    "criteria, and rationale for selection. In the Software Configuration Management Plan \n(see 11.4), the applicant should propose a set of activities and outputs that encompass the COTS SCM process defined in section 12.4.6. \nc. Verification data, including a COTS Software Integrity Assurance Case.",
    "## 12.4.8.1 Cots Software Integrity Assurance Case\n\nA COTS Software Integrity Assurance Case should include:",
    "a. The claim(s) made about the integrity of the software. The claim(s) should clearly \nstate which requirements, or objectives are being addressed by the case. \nb. The environment within which the software is expected to operate and for which the \nargument was prepared. \nc. The software requirements that are satisfied by the COTS software capabilities. \nd. The identification, impact assessment, and mitigation of any unneeded COTS \nsoftware capabilities.",
    "software capabilities. \ne. The extent to which the objectives in sections 4 to 9 appropriate to the assurance \nlevel have been demonstrated to have been achieved using evidence of compliance to objectives, for example, those objectives that are satisfied in the traditional manner \nand hence do not need to be justified further. \nf. For those objectives in sections 4 to 9 where life cycle data is not available to fully",
    "f. For those objectives in sections 4 to 9 where life cycle data is not available to fully \nmeet the objective, the arguments and evidence to support the claim that an equivalent level of confidence has been achieved by other means. \ng. Identification of the strategies and arguments in a format agreed with the approval \nauthority with a reference that describes the notation that was used. \nh. Identification of the evidence used in the supplied Software Integrity Assurance",
    "h. Identification of the evidence used in the supplied Software Integrity Assurance \nCase. The evidence is to include all software life cycle data created during COTS software acquisition and verification to the extent that such data constitute evidence. \ni. \nA list of the assumptions and justifications upon which the software integrity \nassurance argument depends. \nj.",
    "assurance argument depends. \nj. \nA description of the process used to verify that the Software Integrity Assurance Case addresses all of the objectives identified in the gap analysis.",
    "## 12.4.9 Changes To Cots From An Earlier Baseline\n\nThe process to incorporate the changed COTS software baseline should follow the same process as incorporating changes in any software product per the objectives in sections 4 to 9. This includes, but is not limited to:",
    "- \nImpact of COTS software requirements changes to system requirements. \n- \nImpact of the change on the system safety analysis. \n- \nImpact of the change on the previously accepted approval data package to determine \nthe scope of the change and to decide if the change should or can be accomplished. \n- \nImpact of changes to any documentation. \n- \nImpact on reverification efforts. \n- \nProcess effort to implement and verify the COTS software upgrade into the system.",
    "The decision to change the baseline version of COTS software should be based on the same considerations used in the original selection of the COTS software. A new accept/reject decision should be made before going forward with the change. It is important to gain access to the COTS supplier's Problem Reports that were incorporated into the change, as well as the revised software requirements, for example, to incorporate new features, as a roadmap to incorporation and reverification of the new",
    "example, to incorporate new features, as a roadmap to incorporation and reverification of the new version of the system software. It is important to capture any information related to problems fixed in between versions.",
    "Note\n: The revised software requirements statement may be available through a list of new features or an updated user's manual, for example, requirements statement. \n\nConfiguration considerations for COTS software should ensure baseline and subsequent updates are controlled by the applicant. \n\n## 12.4.10 Additional Process Objectives And Outputs By Assurance Level For Cots Software",
    "## 12.4.10 Additional Process Objectives And Outputs By Assurance Level For Cots Software\n\nThis section provides guidance for the additional objectives and outputs that COTS software should satisfy in addition to the objectives contained in sections 4 to 9. These tables reference the objectives and outputs of the software life cycle processes previously described in section 12.4 of this document. Table 12-2, Table 12-3, Table 12-4, and Table 12-5 include guidance for:",
    "a. Process objectives applicable for each assurance level. b. Independence by assurance level of the software life cycle process activities \napplicable to satisfy that process's objectives. \nc. Control category by assurance level for the software life cycle data produced by the \nsoftware life cycle process activities (see 7.3).",
    "software life cycle process activities (see 7.3). \nThese tables should not be used as a checklist. In order to fully understand the guidance, the full body of this document should be considered. The following legend applies to \"Applicability by Assurance Level\" and \"Control Category by Assurance Level\" for all tables: \nLEGEND: \n \nThe objective should be satisfied with independence.",
    " \nThe objective should be satisfied. \n \nBlank \nSatisfaction of objective is at applicant's discretion. \n \n \nData satisfies the objectives of Control Category 1 (CC1). \n \n \nData satisfies the objectives of Control Category 2 (CC2).",
    "| Objective           | Output     |\n|---------------------|------------|\n|                     |            |\n|                     |            |\n| Applicability by    |            |\n| Assurance Level     |            |\n| Control Category by |            |\n| Assurance Level     |            |\n| Activity            |            |\n|                     |            |\n| Description         | Ref        |\n| AL                  |            |\n| 1                   |            |",
    "| AL                  |            |\n| 1                   |            |\n| AL                  |            |\n| 2                   |            |\n| AL                  |            |\n| 3                   |            |\n| AL                  |            |\n| 4                   |            |\n| AL                  |            |\n| 5                   |            |\n| Data Item           | Ref        |\n| AL                  |            |\n| 1                   |            |",
    "| AL                  |            |\n| 1                   |            |\n| AL                  |            |\n| 2                   |            |\n| AL                  |            |\n| 3                   |            |\n| AL                  |            |\n| 4                   |            |\n| AL                  |            |\n| 5                   |            |\n| PSAA                | 11.1       |\n|                    |            |\n|                     |            |",
    "|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n| 1                   |            |\n| 12.4.3.1.a          |            |\n|                    |            |",
    "| 12.4.3.1.a          |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n| Acquisition and     |            |\n| integral            |            |",
    "| Acquisition and     |            |\n| integral            |            |\n| process plans       |            |\n| are defined.        |            |\n| 12.4.8.1            |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |",
    "|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n| COTS                |            |\n| Software            |            |\n| Integrity           |            |\n| Assurance           |            |\n| Case                |            |\n| 2                   |            |\n| 12.4.3.1.b          | 12.4.3.2.b |\n|                    |            |\n|                     |            |",
    "|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                     |            |\n| PSAA                | 11.1       |\n|                    |            |\n|                     |            |\n|                    |            |",
    "|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                     |            |\n| Transition          |            |\n| criteria are        |            |\n| defined, inter-     |            |\n| relationships       |            |\n| and                 |            |",
    "| relationships       |            |\n| and                 |            |\n| sequencing          |            |\n| among               |            |\n| processes are       |            |\n| defined.            |            |\n| 3                   |            |\n| 12.4.3.1.c          |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |",
    "|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                     |            |\n| SQA Records         | 11.19      |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |",
    "|                    |            |\n|                     |            |\n|                    |            |\n|                     |            |\n|                     |            |\n| COTS                |            |\n| software plans      |            |\n| are                 |            |\n| coordinated         |            |\n| with CNS/ATM        |            |\n| plans.              |            |\n|                     |            |\n  \nObjective \nApplicability by \nAssurance Level",
    "|                     |            |\n  \nObjective \nApplicability by \nAssurance Level \nOutput \nControl Category by \nAssurance Level \nActivity \n  \nDescription \nRef \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \n11.14 \n \n \n \n \n \nSoftware Verification Results \n1 \n12.4.4.1.a 12.4.4.2.a \n     \n12.4.8.1 \n     \nCNS/ATM requirements satisfied by the COTS software is determined. \nCOTS Software Integrity Assurance Case PSAA \n11.1 \n \n \n \n \n \n2",
    "COTS Software Integrity Assurance Case PSAA \n11.1 \n \n \n \n \n \n2 \n12.4.4.1.b 12.4.4.2.a \n     \nAdequacy of life cycle data is determined. \n12.4.8.1 \n     \nCOTS Software Integrity Assurance Case \n11.1 \n \n \n \n \n \nSoftware Requirements Data \n3 \n12.4.4.1.c 12.4.4.2.d \n     \nDerived requirements are defined. \n12.4.8.1 \n     \nCOTS Software Integrity Assurance Case \n11.14 \n \n \n \n \n \nSoftware Verification Results \n4 \n12.4.4.1.d 12.4.4.2.e \n     \n12.4.8.1",
    "11.14 \n \n \n \n \n \nSoftware Verification Results \n4 \n12.4.4.1.d 12.4.4.2.e \n     \n12.4.8.1 \n     \nCompatibility of COTS software with target hardware and other CNS/ATM software is assured. \nCOTS Software Integrity Assurance Case \n5 \n12.4.8.1 \n \n \n \n \n \n12.4.4.1.e \n12.4.4.2.c \n12.4.4.2.f \n     \nCOTS Software Integrity Assurance Case \nThe strategies to be used to \naddress gaps in the assurance or evidence are determined and documented. \n  \nObjective \nApplicability by",
    "Objective \nApplicability by \nAssurance Level \nOutput \nControl Category by \nAssurance Level \nActivity \n  \nDescription \nRef \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \n12.4.5.1.a \n1 \n11.14 \n \n \n \n \n \n     \nSoftware Verification Results \n12.4.5.2.a \n12.4.5.2.b \n12.4.5.2.c \n12.4.5.2.d \nSatisfy objectives appropriate to the assurance level which can be achieved. SAS \n11.20 \n \n \n \n \n \n \n2 \n \n12.4.5.1.b 12.4.4.2.g \n12.4.5.2.e \n    ",
    "11.20 \n \n \n \n \n \n \n2 \n \n12.4.5.1.b 12.4.4.2.g \n12.4.5.2.e \n     \n12.4.8.1 \n     \nProvide assurance that the COTS software has \nthe same integrity as would be the case had the Sections 4 to 9 objectives been met. \nCOTS Software Integrity Assurance Case \n  \nObjective \nApplicability by \nAssurance Level \nOutput \nControl Category by \nAssurance Level \nActivity \n  \nDescription \nRef \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \n1 \n11.16 \n \n \n \n \n",
    "1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \n1 \n11.16 \n \n \n \n \n \n12.4.6.1.a 12.4.6.2.a \n \n \n \n \n \nSoftware \nConfiguration Index \nCOTS software \nconfiguration and data items are identified. \n2 \n12.4.6.1.b 12.4.6.2.b \n12.4.6.2.c \n \n \n \n \n Problem \nReports \n11.17 \n \n \n \n \n \nCOTS software problem reporting is established. \n3 \n12.4.6.1.c \n12.4.6.2.c \n12.4.6.2.d \n \n \n \n \n SCM Records \n11.18 \n \n \n \n \n",
    "3 \n12.4.6.1.c \n12.4.6.2.c \n12.4.6.2.d \n \n \n \n \n SCM Records \n11.18 \n \n \n \n \n \nIncorporation of COTS software release is controlled. \n4 \n12.4.6.1.d 12.4.6.2.e \n \n \n \n \n SCM Records \n11.18 \n \n \n \n \n \nCOTS software configuration and data items are archived.",
    "## 12.4.11 Alternative Methods For Providing Assurance Of Cots Software 12.4.11.1 Definitions For Alternative Methods Of Cots Software Assurance Strategies",
    "The use of COTS software often requires alternative methods to be used to gain assurance that the appropriate objectives are satisfied. These methods may be used as guidance with agreement from the appropriate approval authority. These methods can be used alone, or in combination as means of providing assurance that the same integrity has been achieved as would be the case had the objectives in sections 4 to 9 been met in the normal manner. It is recognized that this material is not exhaustive",
    "4 to 9 been met in the normal manner. It is recognized that this material is not exhaustive and other methods may also be used with the agreement of the approval authority. The COTS Software Integrity Assurance Case will record the claims, arguments, and evidence that these methods ensure the COTS software meets its requirements to the same level of confidence as would be the case had the objectives in sections 4 to 9 been met. The categories of available methods include, but are not restricted",
    "in sections 4 to 9 been met. The categories of available methods include, but are not restricted to, the following:",
    "a. Service experience. b. Additional testing. \nc. Restriction of functionality. \nd. Monitoring and recovery. \ne. Design knowledge. \nf. Audits and inspections. \ng. Prior product approval. \nDefinitions of each of these methods are contained in the following sections. \n\n## 12.4.11.1.1 Service Experience\n\nThe use of service experience for COTS software is the same as the use of service experience as a general means of compliance (see 12.3.4). \n\n## 12.4.11.1.2 Additional Testing",
    "## 12.4.11.1.2 Additional Testing\n\nIn some cases, assurance for COTS software can be obtained by completing additional testing of the software. This is always useful and may be sufficient for lower assurance levels or when the functionality is simple. \n\nSpecific methods are:",
    "Specific methods are: \n\na. Exhaustive input testing\n:  In some cases, software is so simple and isolated that the \nset of inputs and outputs can be bounded. In these cases, software functionality may be exhaustively testable. Refer to section 12.3.1 for guidance on this method. \nb. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nRobustness testing:  Testing software with a focus on off-nominal or erroneous inputs to determine its behavior under those conditions.",
    "c. System-level testing\n:  Additional testing performed without regard to internal code \nstructure. \nLong-term soak testing\nd. \n:  Running the system for a period of time while exposing it to \na range of inputs that simulate the normal expected range of inputs, followed by a \nfunctional test without resetting the system. \nUse of system for training\ne. \n:  Using a system for user training, exposing it to an",
    "Use of system for training\ne. \n:  Using a system for user training, exposing it to an \nindependent set of user inputs that has the potential to expose previously unnoticed errors in the system.",
    "## 12.4.11.1.3 Restriction Of Functionality\n\nThe concept \"restriction of functionality\" involves restricting the use of COTS software to a subset of its functionality, by techniques such as, but not limited to, run-time checks, build-time restrictions, or design practices, for example, company design and code standards. The restricted set of functional requirements may then become the basis for use of the COTS software. Specific techniques include the use of:",
    "a. Operational constraints\n:  This is a technique to restrict, through operational \nprocedures, the elements of the COTS software that are used or to limit the range of inputs presented to the COTS software. The operational constraints may be \npermanent or temporary. If the operational constraints are temporary, this technique should be used together with others that ensure correct operation after the constraints \nare removed. \nWrappers\nb.",
    "are removed. \nWrappers\nb. \n:  These are a layer of controlling software written around the COTS \nsoftware. This is a technique that can be used to verify the I/O and/or to constrain the \nusage of the COTS software. \nRemoval of features\nc. \n:  This is a technique to remove or disable components of the \nCOTS software to ensure that they are not used or do not interfere with the operational software.",
    "COTS software to ensure that they are not used or do not interfere with the operational software. \nThis \"restriction of functionality\" method may make it feasible to show compliance to objectives in sections 4 to 9, particularly by reducing verification and auditing activities to the restricted subset of functional requirements.",
    "The definition of subset functionality should be determined and documented, for example, software requirements. Additionally, the functionality of the COTS software should be known. The description of the restriction mechanisms should be documented and verified. The mechanisms by which the restrictions are enforced may need additional verification. It may be difficult to ensure the COTS software has an architecture that prevents execution of undesired, that is, deactivated code.",
    "## 12.4.11.1.4 Monitoring And Recovery",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "For some COTS systems where uncertainty remains about expected operational performance, monitoring can be used to reduce system risk. An example of this is where a system is used in a new context and there is uncertainty about the impact of the change in environment on the system. Monitors are used to detect potential failures, for example due to performance issues. Operational procedures are then put in place to deal with these unanticipated failures before they can have safety-related",
    "are then put in place to deal with these unanticipated failures before they can have safety-related impacts. This specific method assumes that monitoring continues for a predetermined amount of time and then the system, without the monitor, becomes fully operational. Any use of monitoring and recovery should ensure that system risk will still be acceptable once the monitor has been removed. Other related methods include:",
    "a. Failure retection and recovery:  This is a method to detect failures of the COTS \nsoftware and to apply suitable recovery actions for the system. This method is similar \nto \"monitoring and recovery\", but the failure detection and recovery mechanisms are \nbuilt into the system architecture and are intended to remain in the final product. \nb. Fallback systems:  This is a variant of failure detection and recovery where the \nrecovery occurs through a separate fallback system.",
    "## 12.4.11.1.5 Design Knowledge\n\nIn some cases, there may be some evidence available on the design of the COTS software. This method uses knowledge of the COTS software requirements or the system design to provide assurance about the COTS software. The knowledge of the COTS software may be provided by the supplier of the software or obtained through analysis of the software.",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nA related method is reverse engineering, the process of generating software data from existing software data, such as Source Code from object code or Executable Object Code. Reverse engineering may produce software life cycle data that can be reviewed or analyzed to satisfy some objectives in sections 4 to 9, such as design structure, Source Code, or calling trees.",
    "The use of reverse engineering is subject to interpretation on many points, however, and may not be relied upon to always reproduce the original data. As an alternative method, it is unlikely to provide a complete mapping to the objectives in sections 4 to 9. \n\n## 12.4.11.1.6 Audits And Inspections",
    "Audits and inspections are a means by which one can determine if a process has been adequately performed. They can provide additional assurance data via examination of existing data, such as process records, development folders, and any other development data. Reviews of COTS software data in conjunction with SQA records substantiating the process applied to these software data may help satisfy these objectives. In some cases, a traditional inspection of the data itself may be needed. In other",
    "objectives. In some cases, a traditional inspection of the data itself may be needed. In other cases, the alternative method of reviewing other evidence of the process activities may help satisfy these objectives. This method may also be used in conjunction with other alternative methods.",
    "Inputs needed include software data items in support of the audit or inspection. Audits and inspections are limited, since evidence can only be provided when necessary documentation is available. \n\n## 12.4.11.1.7 Prior Product Approval",
    "Prior product approval may have occurred where the COTS software was approved as a part of a previously approved, certified or qualified system application. Examples of product approvals that may be considered include other CNS/ATM applications, aircraft applications, security features, medical devices, military applications, and nuclear applications. Successful use of prior product approval evidence may result in the acceptance of that software development process and data as being compliant",
    "may result in the acceptance of that software development process and data as being compliant to objectives in sections 4 to 9. This may result in a reduction of effort to show compliance of the COTS software to the desired assurance level. The needed inputs are data items that support the objectives of the product's COTS software prior approval and the basis of that approval. A mapping of objectives in section 4 to 9 to the prior product approval objectives may show compliance to the",
    "of objectives in section 4 to 9 to the prior product approval objectives may show compliance to the objectives and thereby provide a direct means of demonstrating compliance to objectives in sections",
    "4 to 9. The prior product approval method will not necessarily provide a complete mapping of objectives in sections 4 to 9. Consequently, objectives that are not addressed need to be shown through other means.",
    "## 12.4.11.2 Applicability Of Alternative Methods To Objectives\n\nThis section provides guidance on how the methods in section 12.4.11.1 can provide assurance for the objectives in sections 4 to 9. \n\n## 12.4.11.2.1 Software Planning Process Objectives",
    "The purpose of the software planning process is to define the means of producing software which will satisfy the system requirements and provide the level of confidence which is consistent with requirements. No current alternative method or item evidence exists that may be substituted for the PSAA. The PSAA should define the framework for mapping the evidence from the COTS software to the satisfaction of each objective in Annex A and Tables 12-2 through 12-5. Within this framework, the PSAA",
    "of each objective in Annex A and Tables 12-2 through 12-5. Within this framework, the PSAA should include credit sought, either full or partial, for the COTS software; analyses and assumptions made; means of compliance; safety aspects impacted by the use of this COTS software or its lack of development data; and any activities remaining to be accomplished during the project in order to satisfy each objective for the appropriate assurance level. These alternative methods may either satisfy or",
    "each objective for the appropriate assurance level. These alternative methods may either satisfy or partially satisfy other software planning objectives:",
    "a. Prior product approval\n:  May provide reuse of plans and standards developed for a \nprevious development. The existence of suitable plans and standards from the previous program may preclude the need to duplicate the same plans and standards for the current program. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "b. \nAudits and inspections:  Reviews of COTS software data, such as Software \nVerification Results, applied development standards, and other life cycle data, in \nconjunction with SQA records substantiating the process applied to the development of these software data items may help satisfy these objectives. \n\n## 12.4.11.2.2 Software Development Processes",
    "## 12.4.11.2.2 Software Development Processes\n\nThe software requirements process uses the outputs of the system life cycle processes to develop the high-level requirements. These high-level requirements include functional, performance, interface, and safety-related requirements. In the software coding process, the Source Code is implemented from the software architecture and the low-level requirements.",
    "The target computer, and the Source Code from the software coding process are used with the compiling, linking, and loading data in the integration process to develop the integrated system or equipment. \n\nTraceability is established to:",
    "- \nEnable verification of the complete implementation of the system requirements and give visibility to the derived requirements. \n- \nGive visibility to the derived requirements and the architectural design decisions made during the software design process, and allow verification of the complete implementation of the high-level requirements. \n- \nEnable verification of the absence of extraneous Source Code and verification of the \ncomplete implementation of the low-level requirements.",
    "complete implementation of the low-level requirements. \nThese alternative methods may either satisfy or partially satisfy the software development process objectives, noting that the detail may need to increase as the assurance level increases:",
    "a. Prior product approval\n:  COTS software data items obtained from previous approvals \nmay be applicable in satisfying the objectives in sections 4 to 9. \nDesign knowledge\nb. \n:  Elements of the development process such as requirements, \narchitecture, design description, or even the Source Code may be produced from \nother available elements of the implementation. \nRestriction of functionality\nc. \n:  A restriction of COTS software functionality may",
    "Restriction of functionality\nc. \n:  A restriction of COTS software functionality may \nprovide a reduction in the amount of applicable COTS software functionality subject to software development process objectives. \nd. \nService experience:  Adequate and relevant service experience may mitigate the need",
    "for providing low-level requirements. Considerations should include the intended operational environment impact on the COTS software, the role of the COTS software in the safety model, and the mapping of COTS software functionality to the application's needs. For example, if COTS software without adequate development data was used unchanged in a similar application within a system on the same operational platform, then a degree of relevance may be achieved by the service experience of that COTS",
    "platform, then a degree of relevance may be achieved by the service experience of that COTS software. Service experience may also enable an applicant to receive credit for development of Source Code. If Source Code is not present, the applicant should show that the COTS software being used has a clearly-defined",
    "interface and that the usage of that interface during the service experience is similar to the future usage in the new software (see 12.3.4).",
    "e. Audits and inspections\n:  Post-development reviews of COTS software data items \nsuch as a Product Description, Equipment Specification, and other elements of the",
    "such as a Product Description, Equipment Specification, and other elements of the \ndevelopment process in conjunction with SQA records substantiating the process applied to these data items may help satisfy these objectives. In some cases, a traditional inspection of the data itself may be needed. In other cases, the alternative method of reviewing other evidence of the process activities may help satisfy these objectives. \nThird-party \ninspections \nmay \nhelp \nalleviate \nproprietary \ndata",
    "Third-party \ninspections \nmay \nhelp \nalleviate \nproprietary \ndata \nconsiderations.",
    "## 12.4.11.2.3 Verification Of Outputs Of Software Requirements Process\n\nThe objective of these reviews and analyses is to detect and report requirements errors that may have been introduced during the software requirements process. \n\nThese alternative methods may either satisfy or partially satisfy the verification of outputs of software requirements process:",
    "a. Prior product approval\n:  COTS software data items obtained from previous approvals \nmay be applicable in satisfying the objectives in sections 4 to 9. For example, a previous approval may provide a traceability analysis showing the flow from system requirements through software requirements. The applicant's ability to reuse the COTS software effectively is related to its previous assurance level. \nDesign knowledge\nb. \n:  Data items of the previous development process may be used to",
    "Design knowledge\nb. \n:  Data items of the previous development process may be used to \ngenerate missing or incomplete data items such as using test procedures to generate a software requirements data. \nService Experience\nc. \n:  Similarity of operating environments may provide empirical \nevidence that supports the compatibility of high-level requirements to their target environment. \nAudits and inspections\nd. \n:  Reviews of COTS software data items, such as requirements",
    "Audits and inspections\nd. \n:  Reviews of COTS software data items, such as requirements \nand safety analyses, may provide evidence of accurate and consistent requirements. The COTS software requirements specification can be audited against documented \nstandards. In some cases, the COTS supplier may be able to provide relevant test, review, and analysis data for the stand-alone COTS software. Such data may be used \nto supplement tests on the target computer in the target environment.",
    "## 12.4.11.2.4 Verification Of Outputs Of Software Design Process\n\nThe purpose of the software verification process is to detect and report errors that may have been introduced during the software development processes. It ensures that:",
    "- \nThe software architecture does not conflict with the high-level requirements, \nespecially functions that ensure system integrity, for example, partitioning schemes. \n- \nA correct relationship exists between the components of the software architecture. \n- \nNo \nconflicts \nexist \n(especially \nfor \ninitialization, \nasynchronous \noperation, \nsynchronization and interrupts) between the software architecture and the hardware/software features of the target computer.",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "- \nThe software architecture can be verified, for example, that there are no unbounded \nrecursive algorithms. \n- \nThe Software Design Standards were followed during the software design process \nand that deviations to the standards are justified, especially complexity restrictions \nand design constructs that would not comply with the system safety objectives. \n- \nPartitioning breaches are prevented or isolated.",
    "- \nPartitioning breaches are prevented or isolated. \nThese alternative methods may either satisfy or partially satisfy the verification of outputs of low-level requirements process or design process:",
    "a. Prior product approval\n:  COTS software data items obtained from previous approvals \nmay be applicable in satisfying the objectives in sections 4 to 9. For example, evidence of traceability and verification from prior approval may be used for \ndemonstrating that the low-level requirements comply with the high-level \nrequirements, \nDesign knowledge\nb. \n:  Data items of the previous development process may be used to",
    "Design knowledge\nb. \n:  Data items of the previous development process may be used to \ngenerate missing or incomplete data items, such as deriving the design architecture from the Source Code. \nRestriction of functionality\nc. \n:  A restriction of COTS software functionality may \nprovide a reduction in the amount of applicable COTS software functionality subject to software design process objectives. \nService experience\nd. \n:  If service experience is deemed adequate to mitigate the need for",
    "Service experience\nd. \n:  If service experience is deemed adequate to mitigate the need for \nproviding low-level requirements, then it also mitigates the need to satisfy objectives pertaining to the quality of low-level requirements. Similarity of operating",
    "environments may provide empirical evidence that supports the accuracy of algorithms. Adequacy of the software architecture may be shown using service experience if the data needed to assess the architecture's compatibility with the target computer is collected. Examples include central processing unit (CPU) utilization, memory utilization, throughput analysis and timing, and I/O bandwidth. Relevant service experience combined with restriction of functionality may be used to show that the",
    "Relevant service experience combined with restriction of functionality may be used to show that the restrictions appropriately enforce partitioning integrity.",
    "e. \nAudits and inspections: Reviews of COTS software data items, such as requirements",
    "and safety analyses, may provide evidence of accurate and consistent requirements. The COTS software requirements specification can be audited against documented standards. In some cases, the COTS software supplier may be able to provide relevant test, review, and analysis data for the stand-alone COTS software. Such data may be used to supplement tests on the target computer in the target environment. Examples include CPU utilization, memory utilization, throughput analysis and timing, and I/O",
    "Examples include CPU utilization, memory utilization, throughput analysis and timing, and I/O bandwidth. It may be possible to show that the partitioning integrity",
    "of the COTS software is maintained by analysis of the partitioning design implementation.",
    "## 12.4.11.2.5 Verification Of Outputs Of Software Coding & Integration Processes\n\nThe objective is to ensure that the results of the coding and integration processes are complete and correct. This could be performed by a detailed examination of the linking and loading data and memory map. Typical examples of potential errors include:",
    "- \nIncorrect hardware addresses. \n- \nMemory overlaps. \n- \nMissing software components. \nThese alternative methods may either satisfy or partially satisfy the verification of outputs of software coding and integration processes:",
    "a. Prior product approval\n:  COTS software data items obtained from previous approvals \nmay be applicable in satisfying the objectives in sections 4 to 9. For example, \nevidence of traceability and verification from prior approval may be used for demonstrating Source Code to low-level requirements compliance and Source Code \nverifiability through test, demonstration, inspection, or analysis. \nDesign knowledge\nb. \n:  Elements of the process such as available code walkthroughs",
    "Design knowledge\nb. \n:  Elements of the process such as available code walkthroughs \nfrom the COTS software originator may be used to show conformance to standards, for example, to identify trends showing adherence to standards. It may be possible to show that the coding process has not introduced any errors. This may be accomplished by review of the COTS software Source Code, by verification that",
    "algorithms used satisfy the intended functionality, or by review of COTS software Source Code expected behavior under normal and abnormal conditions. \nRestriction of functionality\nc. \n:  A restriction of COTS software functionality may \nprovide a reduction in the amount of applicable COTS software functionality subject to software coding and integration processes objectives. \nService experience\nd. \n:  In products where service experience is used to show",
    "Service experience\nd. \n:  In products where service experience is used to show \ncompliance, Source Code may not be available. If the Source Code is not available,",
    "service experience may be used to partially meet these objectives. The service experience should be combined with other alternative means for the objectives to be completely met; for instance, it might be combined with additional robustness testing. The amount of service experience needed, and the extent to which other alternative methods should be used to complement it, increase with assurance level. The methods used to address these objectives and rationale for why they address those",
    "level. The methods used to address these objectives and rationale for why they address those objectives to the requisite level should be agreed to in advance with the approval authority and specified in the Plan for Software Aspects of Approval.",
    "Audits and inspections\ne. \n:  Reviews of COTS software data items such as code \nwalkthrough results, complexity metrics, past test and integration results, and  \ntraceability evidence can help satisfy objectives of this process.",
    "## 12.4.11.2.6 Verification Of Outputs Of Integration Process\n\nThe objective is to ensure that the results of the integration process are complete and correct. The process demonstrates that the software satisfies its requirements and also that errors which could lead to unacceptable failure conditions, as determined by the system safety assessment process, have been mitigated.",
    "Alternative methods are not generally applicable for this process. However, service experience can show compliance to the objective of compatibility of Executable Object Code with the target computer, and if low-level requirements do not exist for the COTS software, then service experience may be used in lieu of low-level requirements testing. For testing of outputs of the integration process, all of the normal data are available and the COTS software is fully integrated into the system.",
    "all of the normal data are available and the COTS software is fully integrated into the system. Considerations include COTS testing for performance, functionality, robustness, limitations, restrictions, and normal and robustness input testing. Also, system testing addresses system performance, functionality, software/software integration, and hardware/software integration.",
    "## 12.4.11.2.7 Verification Of Verification Process Results Objectives\n\nThe objectives of the verification process results are to:",
    "- \nEnsure that the testing of the code was developed and performed accurately and \ncompletely. \n- \nDetermine how well the requirements-based testing verified the implementation of \nthe software requirements. \n- \nDetermine which code structure was not exercised by the requirements-based test procedures.",
    "- \nDetermine which code structure was not exercised by the requirements-based test procedures. \nThe higher the level of coverage needed by the assurance level, the stronger the evidence supporting the assurance should be. The specific evidence to be used and specific credit for these objectives to be obtained using that evidence should be stated in the PSAA and agreed to in advance with the approval authority.",
    "Techniques that may contribute to satisfying the verification of verification process objectives above for COTS software are:",
    "a. Additional testing\n:  Exhaustive input testing may help satisfy the structural coverage \nobjectives. \nPrior product approval\nb. \n:  COTS software data items obtained from previous approvals \nor approvals may be applicable in satisfying the objectives in sections 4 to 9.  \nDesign knowledge\nc. \n:  Although generally not applicable for these objectives, it can \nsometimes be used to augment test coverage analysis. \nRestriction of functionality\nd.",
    "sometimes be used to augment test coverage analysis. \nRestriction of functionality\nd. \n:  The effort to satisfy these objectives may be reduced by \nthese methods. \nService experience and additional testing\ne. \n:  Analysis of this data may be used to \nsupport these objectives. For example, service experience of a duration that is commensurate with the assurance level might be combined with soak testing and robustness testing to ensure coverage of off-nominal inputs. \nf.",
    "f. \nMonitoring and recovery:  This is generally used only to enable sufficient service \nexperience data to be gathered to create a service experience argument. Any reduction in effort claimed for this technique should be clearly accompanied by an explanation of why the level of confidence is not reduced when monitoring is discontinued. Failure detection and recovery or fallback systems may also assist in \nthe same manner. \ng. Audits and inspections",
    "the same manner. \ng. Audits and inspections\n:  Use of previous reviews and audits may be used to satisfy \nthe objectives of the current project. Evidence may be obtained to show that test procedures used were correct and traceable to requirements, test results were verified, and any discrepancies have been identified and explained.",
    "## 12.4.11.2.8 Software Configuration Management Process\n\nThe SCM process includes the activities of configuration identification, change control, baseline establishment, and archiving of the software product, including the related software life cycle data. The SCM processes will:",
    "a. Label unambiguously each configuration item, including its successive versions, so \nthat a basis is established for the control and reference of configuration items. \nb. Define a basis for further software life cycle process activity and allow reference to, \ncontrol of, and traceability between configuration items. Guidance includes: \n1. Record process non-compliance with software plans and standards. \n2. Record deficiencies of outputs of software life cycle processes.",
    "2. Record deficiencies of outputs of software life cycle processes. \n3. Record anomalous behavior of software products and ensure resolution of these \nproblems. \n4. Provide for recording, evaluation, resolution, and approval of changes throughout \nthe software life cycle. \nc. Ensure problems and changes are assessed, approved or disapproved, approved \nchanges are implemented, and feedback is provided to affected processes through",
    "changes are implemented, and feedback is provided to affected processes through \nProblem Reports and change control methods defined during the software planning process. \nd. Provide data for the configuration management of software life cycle processes with \nrespect to configuration identification, baselines, Problem Reports, and change \ncontrol. \ne. Ensure that the software life cycle data associated with the software product can be",
    "control. \ne. Ensure that the software life cycle data associated with the software product can be \nretrieved in case of a need to duplicate, regenerate, retest, or modify the software product. \nf. Ensure that only authorized software is used, especially for software manufacturing, \nin addition to being archived and retrievable. \ng. Ensure that the Executable Object Code is loaded into the system or equipment with",
    "g. Ensure that the Executable Object Code is loaded into the system or equipment with \nappropriate safeguards. Software load control refers to the process by which programmed instructions and data are transferred from a master memory device into the CNS/ATM system or equipment. \nh. Ensure that the tools used to produce the software are identified, controlled, and \nretrievable.",
    "The SCM process does not stop when the software product is accepted, but continues throughout the service life of the system or equipment. Section 12.4.6 defines the additional COTS configuration management process objectives that are to be satisfied for COTS. Satisfaction of these objectives may provide some evidence that also satisfies the software configuration management process objectives in section 7.1. Alternative methods are not generally applicable for the SCM process. However, the use",
    "section 7.1. Alternative methods are not generally applicable for the SCM process. However, the use of COTS software may add the consideration of protection of proprietary data rights. For protection of proprietary data rights, the objectives may be met by an independent data holding arrangement between the applicant and the COTS software supplier. The mechanism to control the configuration of the COTS software should be documented and is a function of the system using the COTS software.",
    "## 12.4.11.2.9 Software Quality Assurance Process",
    "The SQA process assesses the software life cycle processes and their outputs to obtain assurance that the objectives are satisfied; that deficiencies are detected, evaluated, tracked and resolved; and that the software product and software life cycle data conform to approval requirements. The process provides confidence that the software life cycle processes produce software that conforms to its requirements by assuring that these processes are performed in compliance with the approved plans",
    "requirements by assuring that these processes are performed in compliance with the approved plans and standards.",
    "Assurance aspects may be met by the results of audits and inspections of the COTS software development process and its outputs or by evidence from prior product approval. \n\n## 12.4.11.2.10 Approval Liaison Process",
    "The approval liaison process establishes communication and understanding between the applicant and the approval authority throughout the software life cycle to assist the approval process. Alternative methods are not generally applicable for this process. The PSAA should define the framework for mapping the evidence from the COTS software to the intent of each objective in Annex A and Section 12.4.10, and the SAS",
    "should define the details of the mapping to each objective. The details should include the COTS software approval credit sought either full or partial, assumptions, evidence of compliance, and any activities accomplished during the project in order to satisfy each objective.",
    "The COTS Software Integrity Assurance Case documents the rationale that, to an acceptable degree, the software meets requirements imposed upon it by the system within which it operates. Compliance substantiation is a function of the system and is not unique to the COTS software. \n\n## Annex A Process Objectives And Outputs By Assurance Level\n\nReferences in these tables point to those sections in the text that define the particular objectives, related activities and outputs.",
    "The tables include guidance for:",
    "a. The process objectives applicable for each assurance level. For level AL6 software, \nsee section 2.3.3. \nb. The independence by assurance level of the software life cycle process activities \napplicable to satisfy that process's objectives. \nc. The control category by assurance level for the software life cycle data produced by \nthe software life cycle process activities (see 7.3).",
    "the software life cycle process activities (see 7.3). \nThese tables should not be used as a checklist. These tables do not reflect all aspects of compliance to this document. In order to fully understand the guidance, the full body of this document should be considered. The following legend applies to \"Applicability by Assurance Level\" and \"Control Category by Assurance Level\" for all tables:",
    "LEGEND: \n \nThe objective should be satisfied with independence. \n \n \nThe objective should be satisfied. \n \nBlank \nSatisfaction of objective is at applicant's discretion. \n \n Data satisfies the objectives of Control Category 1 (CC1). \n \n Data satisfies the objectives of Control Category 2 (CC2). \n\n## Software Planning Process",
    "Objective \nApplicability by \nAssurance Level \nOutput \nControl Category by \nAssurance Level \nActivity \n  \nDescription \nRef \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \n4.1.a \n1 \n     \nThe activities of the software life cycle \nprocesses are defined. \n4.2.a 4.2.c 4.2.d 4.2.e 4.2.g 4.2.i 4.2.l 4.3.c \n2 \n4.1.b \n4.2.i 4.3.b \n      \nThe software life cycle(s), including the \ninter-relationships \nbetween the processes, their sequencing, feedback mechanisms, and transition criteria, is defined. \n4.1.c",
    "4.1.c \n3 \n      \nSoftware life cycle environment is selected and defined. \n4.4.1 4.4.2.a 4.4.2.b 4.4.2.c \n4.4.3 \n4.1.d \n4 \n     \nAdditional considerations are addressed. \n4.2.f 4.2.h 4.2.i 4.2.j 4.2.k \n5 \nSoftware development standards are defined. \n4.1.e \n      \n4.2.b 4.2.g 4.5 \n6 \nSoftware plans comply with this document. \n4.1.f \n4.3.a 4.6 \n      \n7 \n4.1.g \n4.2.g 4.6 \n      \nDevelopment and revision of software plans are coordinated. \nAL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL",
    "Development and revision of software plans are coordinated. \nAL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \nPSAA \n11.1 \n     \nSDP \n11.2 \n     \nSVP \n11.3 \n     \nSCM Plan \n11.4 \n     \nSQA Plan \n11.5 \n     \nPSAA \n11.1 \n    \n \nSDP \n11.2 \n    \n \nSVP \n11.3 \n    \n \nSCM Plan \n11.4 \n    \n \nSQA Plan \n11.5 \n    \n \nPSAA \n11.1 \n    \n \nSDP \n11.2 \n    \n \nSVP \n11.3 \n    \n \nSCM Plan \n11.4 \n    \n \nSQA Plan \n11.5 \n    \n \nPSAA \n11.1 \n    ",
    "SVP \n11.3 \n    \n \nSCM Plan \n11.4 \n    \n \nSQA Plan \n11.5 \n    \n \nPSAA \n11.1 \n     \nSDP \n11.2 \n     \nSVP \n11.3 \n     \nSCM Plan \n11.4 \n     \nSQA Plan \n11.5 \n     \nSW Requirements Standards \n11.6 \n      \nSW Design Standards \n11.7 \n     \nSW Code Standards \n11.8 \n     \nSoftware \nVerification Results 11.14 \n      \nSoftware \nVerification Results 11.14 \n   ",
    "## Software Development Processes",
    "Objective \nApplicability by \nAssurance Level \nOutput \nControl Category by \nAssurance Level \nActivity \nAL\nAL\n  \nDescription \nRef \nRef \nAL\n1 \n2 \n     \n1 \nHigh-level requirements are developed. \n5.1.1.a \n5.1.2.a 5.1.2.b 5.1.2.c 5.1.2.d 5.1.2.e \n5.1.2.f 5.1.2.g \n5.1.2.j 5.5.a \n2 \n5.1.1.b 5.1.2.h\n5.1.2.i \n     \nDerived high-level requirements are defined and \nprovided to the system processes, including the system safety assessment process. \n3 \nSoftware architecture is developed.",
    "3 \nSoftware architecture is developed. \n5.2.1.a \n5.2.2.a 5.2.2.d \n     Design \nDescription \n11.10 \n     \n4 \nLow-level requirements are \ndeveloped. \n5.2.1.a \n       \n5.2.2.a 5.2.2.e \n5.2.2.f 5.2.2.g \n5.2.3.a 5.2.3.b 5.2.4.a 5.2.4.b 5.2.4.c 5.5.b \n5 \n5.2.1.b 5.2.2.b \n5.2.2.c \n       \nDerived low-level \nrequirements are defined and provided to the system processes, including the system safety assessment process. \n6 \nSource Code is developed. \n5.3.1.a \n  ",
    "6 \nSource Code is developed. \n5.3.1.a \n       \n5.3.2.a 5.3.2.b 5.3.2.c 5.3.2.d 5.5.c \n5.4.1.a \n7 \n     \nExecutable Object Code and Adaptation Data Item Files, if any, are  produced and loaded in the target computer. \n5.4.2.a 5.4.2.b 5.4.2.c 5.4.2.d 5.4.2.e \n5.4.2.f \n| AL             | AL    | AL   |   AL |   AL |\n|----------------|-------|------|------|------|\n| 3              | 4     | 5    |      |      |\n| Data Item      | Ref   |      |      |      |",
    "| 3              | 4     | 5    |      |      |\n| Data Item      | Ref   |      |      |      |\n| AL             |       |      |      |      |\n| 1              | 2     | 3    |   4  |   5  |\n| 11.9           |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |",
    "|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n| Software       |       |      |      |      |\n| Requirements   |       |      |      |      |\n| Data           |       |      |      |      |",
    "| Requirements   |       |      |      |      |\n| Data           |       |      |      |      |\n| Trace Data     | 11.21 |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |",
    "|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n| 11.9           |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|                |       |      |      |      |",
    "|                |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |",
    "|               |       |      |      |      |\n|                |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|                |       |      |      |      |\n| Software       |       |      |      |      |\n| Requirements   |       |      |      |      |\n| Data           |       |      |      |      |\n|                |       |      |      |      |",
    "| Data           |       |      |      |      |\n|                |       |      |      |      |\n| Design         |       |      |      |      |\n| Description    |       |      |      |      |\n| 11.10          |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |",
    "|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|                |       |      |      |      |\n| Trace Data     | 11.21 |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |",
    "|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|                |       |      |      |      |\n| 11.10          |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |",
    "|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|                |       |      |      |      |\n|                |       |      |      |      |\n| Design         |       |      |      |      |\n| Description    |       |      |      |      |",
    "| Design         |       |      |      |      |\n| Description    |       |      |      |      |\n|                |       |      |      |      |\n| Source Code    | 11.11 |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |",
    "|               |       |      |      |      |\n|                |       |      |      |      |\n|                |       |      |      |      |\n| Trace Data     | 11.21 |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |",
    "|               |       |      |      |      |\n|                |       |      |      |      |\n|                |       |      |      |      |\n| Executable     |       |      |      |      |\n| Object Code    |       |      |      |      |\n| 11.12          |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |",
    "|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n| Adaptation     |       |      |      |      |\n| Data Item File |       |      |      |      |",
    "| Adaptation     |       |      |      |      |\n| Data Item File |       |      |      |      |\n| 11.22          |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |",
    "|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |\n|               |       |      |      |      |\n|                |       |      |      |      |",
    "## Verification Of Outputs Of Software Requirements Process",
    "Objective \nApplicability by \nAssurance Level \nOutput \nControl Category by \nAssurance Level \n \nActivity \n  \nDescription \nRef \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \n1 \n11.14 \n     \n6.3.1.a \n6.3.1 \n     \nSoftware Verification Results \nHigh-level requirements comply with system requirements. \n11.14 \n     \n2 \nHigh-level requirements are accurate and consistent. \n6.3.1.b \n6.3.1 \n     \nSoftware Verification Results \n3 \n11.14 \n ",
    "6.3.1.b \n6.3.1 \n     \nSoftware Verification Results \n3 \n11.14 \n  \n  \n  \n  \n6.3.1.c \n6.3.1 \n        \nHigh-level requirements are compatible with target computer. \nSoftware Verification Results \n11.14 \n    \n  \n4 \nHigh-level requirements are verifiable. \n6.3.1.d \n6.3.1 \n      \nSoftware Verification Results \n11.14 \n    \n  \n5 \nHigh-level requirements conform to standards. \n6.3.1.e \n6.3.1 \n      \nSoftware Verification Results \n6 \n11.14 \n     \n6.3.1.f \n6.3.1 \n    ",
    "6.3.1 \n      \nSoftware Verification Results \n6 \n11.14 \n     \n6.3.1.f \n6.3.1 \n     \nSoftware Verification Results \nHigh-level requirements are traceable to system requirements. \n11.14 \n    \n  \n7 \nAlgorithms are accurate. \n6.3.1.g \n6.3.1 \n      \nSoftware Verification Results",
    "## Verification Of Outputs Of Software Design Process",
    "Objective \nApplicability by \nAssurance Level \nOutput \nControl Category by \nAssurance Level \nActivity \n  \nDescription \nRef \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \n1 \n6.3.2.a \n6.3.2 \n \n \n   \n  \nLow-level \nrequirements comply with high-level \nrequirements. \n2 \n6.3.2.b \n6.3.2 \n \n \n   \n  \nLow-level \nrequirements are accurate and consistent. \n3 \n6.3.2.c \n6.3.2 \n \n   \n  \n  \nLow-level \nrequirements are \ncompatible with target computer. \n4 \n6.3.2.d \n6.3.2 \n \n   \n  \n  \nLow-level",
    "requirements are \ncompatible with target computer. \n4 \n6.3.2.d \n6.3.2 \n \n   \n  \n  \nLow-level \nrequirements are verifiable. \n5 \n6.3.2.e \n6.3.2 \n \n \n   \n  \nLow-level \nrequirements conform to standards. \n6 \n6.3.2.f \n6.3.2 \n \n \n   \n  \nLow-level \nrequirements are traceable to highlevel requirements. \n7 \nAlgorithms are accurate. \n6.3.2.g \n6.3.2 \n \n \n \n   \n8 \n6.3.3.a \n6.3.3 -  \n \n   \nSoftware architecture is compatible with high-level requirements. \n9 \n6.3.3.b \n6.3.3 \n \n \n \n",
    "9 \n6.3.3.b \n6.3.3 \n \n \n \n \n  \nSoftware architecture is consistent. \n10 \n6.3.3.c \n6.3.3 \n \n  \n \n  \nSoftware \narchitecture is compatible with target computer. \n11 \n6.3.3.d \n6.3.3 \n \n \n  \n  \n  \nSoftware architecture is verifiable. \n12 \n6.3.3.e \n6.3.3 \n \n \n \n \n  \nSoftware architecture conforms to standards. \n13 \n6.3.3.f \n6.3.3 \n \n \n \n \n \nSoftware partitioning integrity is confirmed. \nAL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \n11.14 \n \n \n",
    "AL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \n11.14 \n \n \n \n  \n  \nSoftware Verification Results \n11.14 \n \n \n \n  \n  \nSoftware Verification Results \n11.14 \n \n \n  \n  \n  \nSoftware Verification \nResults \n11.14 \n \n \n  \n  \n  \nSoftware Verification Results \n11.14 \n \n \n \n  \n  \nSoftware Verification Results \n11.14 \n \n \n \n  \n  \nSoftware Verification Results \n11.14 \n \n \n \n \n  \nSoftware Verification Results \n11.14 \n \n \n \n \n  \nSoftware Verification Results \n11.14 \n \n \n \n",
    "11.14 \n \n \n \n \n  \nSoftware Verification Results \n11.14 \n \n \n \n \n  \nSoftware Verification Results \n11.14 \n \n",
    "Software \nVerification Results \n11.14 \n \n \n  \n  \n  \nSoftware Verification Results \n11.14 \n \n \n \n \n  \nSoftware Verification Results \n11.14 \n \n \n \n \n \nSoftware Verification Results \n  \nObjective \nApplicability by \nAssurance Level \nOutput \nControl Category by \nAssurance Level \nActivity \n  \nDescription \nRef \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \n1 \n11.14 \n \n \n \n  \n  \n6.3.4.a \n6.3.4 \n \n \n   \n  \nSoftware Verification Results",
    "AL\n4 \nAL\n5 \n1 \n11.14 \n \n \n \n  \n  \n6.3.4.a \n6.3.4 \n \n \n   \n  \nSoftware Verification Results \nSource Code complies with lowlevel requirements. \n2 \n11.14 \n \n \n \n  \n  \n6.3.4.b \n6.3.4 \n \n \n   \n  \nSoftware Verification Results \nSource Code complies with software architecture. \n11.14 \n \n \n  \n  \n  \n3 Source Code is \nverifiable. \n6.3.4.c \n6.3.4 \n \n     \n  \nSoftware Verification Results \n4 \n11.14 \n \n \n \n  \n  \n6.3.4.d \n6.3.4 \n \n \n   \n  \nSoftware Verification Results",
    "4 \n11.14 \n \n \n \n  \n  \n6.3.4.d \n6.3.4 \n \n \n   \n  \nSoftware Verification Results \nSource Code conforms to standards. \n5 \n11.14 \n \n \n \n  \n  \n6.3.4.e \n6.3.4 \n \n \n   \n  \nSoftware Verification Results \nSource Code is traceable to low-level requirements. \n6 \n11.14 \n \n \n \n  \n  \n6.3.4.f \n6.3.4 \n \n \n   \n  \nSoftware Verification Results \nSource Code is accurate and consistent. \n7 \n11.14 \n \n \n \n \n  \n6.3.5.a \n6.3.5 \n \n \n \n",
    "Source Code is accurate and consistent. \n7 \n11.14 \n \n \n \n \n  \n6.3.5.a \n6.3.5 \n \n \n \n   \nOutput of software integration process is complete and correct. \nSoftware Verification Results \n11.13 \n \n \n \n \n \nSoftware Verification Cases and Procedures \n8 \n6.6.a \n6.6 \n \n \n \n \n \nAdaptation Data Item File is correct and complete. \n11.14 \n     \nSoftware Verification Results \n9 \n11.14 \n \n \n \n \n  \n6.6.b \n6.6 \n \n \n \n \n  \nVerification of Adaptation Data Item \nFile is achieved",
    " \n \n \n \n  \n6.6.b \n6.6 \n \n \n \n \n  \nVerification of Adaptation Data Item \nFile is achieved \nSoftware Verification \nResults",
    "## Testing Of Outputs Of Integration Process Table A-6",
    "Objective \nApplicability by \nAssurance Level \nOutput \nControl Category by \nAssurance Level \nActivity \n  \nDescription \nRef \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \n1 \n6.4.a \n \n \n \n \n \nExecutable Object Code complies with high-level requirements. \n6.4.2 6.4.2.1 6.4.3 6.5 \n2 \n6.4.b \n \n \n \n \n \nExecutable Object Code is robust with high-level requirements. \n6.4.2 6.4.2.2 6.4.3 6.5 \n3 \n6.4.c \n \n \n   \n  \nExecutable Object Code complies with low-level requirements. \n6.4.2 6.4.2.1 6.4.3 6.5 \n4",
    "   \n  \nExecutable Object Code complies with low-level requirements. \n6.4.2 6.4.2.1 6.4.3 6.5 \n4 \n6.4.d \n \n \n   \n  \nExecutable Object \nCode is robust with low-level requirements. \n6.4.2 \n6.4.2.2 6.4.3 6.5 \n5 \n6.4.e 6.4.1.a\n6.4.3.a \n \n \n \n \n \nExecutable Object Code is compatible with target computer. \nAL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \n11.13 \n \n \n \n \n \nSoftware Verification Cases and Procedures \n11.14 \n     \nSoftware Verification Results Trace Data \n11.21",
    "11.14 \n     \nSoftware Verification Results Trace Data \n11.21 \n     \n11.13 \n \n \n \n \n \nSoftware Verification Cases and Procedures \n11.14 \n     \nSoftware Verification Results Trace Data \n11.21 \n     \n11.13 \n \n \n \n  \n  \nSoftware Verification Cases and Procedures \n11.14 \n   \n \n \nSoftware Verification Results Trace Data \n11.21 \n   \n \n \n11.13 \n \n \n \n  \n  \nSoftware Verification Cases and Procedures \n11.14 \n   \n \n \nSoftware Verification Results Trace Data \n11.21",
    "11.14 \n   \n \n \nSoftware Verification Results Trace Data \n11.21 \n   \n \n \n11.13 \n \n \n      \nSoftware Verification Cases and Procedures \n11.14 \n     \nSoftware Verification Results",
    "## Verification Of Verification Process Results",
    "Objective \nApplicability by \nAssurance Level \nOutput \nControl Category by \nAssurance Level \nActivity \n  \nDescription \nRef \nRef \nAL\n1 \nAL\n2 \nAL\n3 \n1 \nTest procedures are correct. \n6.4.5.b \n6.4.5 \n      \n2 \n6.4.5.c \n6.4.5 \n      \nTest results are correct and discrepancies explained. \n3 \n6.4.4.a \n6.4.4.1 \n     \nTest coverage of high-level requirements is \nachieved. \n4 \n6.4.4.b \n6.4.4.1 \n       \nTest coverage of lowlevel requirements is achieved. \n5 \n6.4.4.c \n",
    "6.4.4.1 \n       \nTest coverage of lowlevel requirements is achieved. \n5 \n6.4.4.c \n   \n  \n  \n  \n6.4.4.2.a 6.4.4.2.b 6.4.4.2.d 6.4.4.3 \nTest coverage of software structure (modified condition/decision) is achieved. \n6 \n6.4.4.c \n    \n  \n  \nTest coverage of software structure (decision coverage) is achieved. \n6.4.4.2.a 6.4.4.2.b 6.4.4.2.d 6.4.4.3 \n7 \n6.4.4.c \n       \nTest coverage of software structure (statement coverage) is achieved. \n6.4.4.2.a 6.4.4.2.b 6.4.4.2.d 6.4.4.3 \n8 \n6.4.4.d",
    "6.4.4.2.a 6.4.4.2.b 6.4.4.2.d 6.4.4.3 \n8 \n6.4.4.d \n      \n6.4.4.2.c 6.4.4.2.d 6.4.4.3 \nTest coverage of software structure (data coupling and control coupling) is achieved. \n9 \n6.4.4.c \n6.4.4.2.b \n \n  \n  \n  \n  \nVerification of additional code, that cannot be traced to Source Code, is achieved. \nAL\n4 \nAL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \n11.14 \n \n \n \n \n  \nSoftware Verification Results \n11.14 \n \n \n \n \n  \nSoftware Verification Results \n11.14 \n \n \n \n \n",
    "11.14 \n \n \n \n \n  \nSoftware Verification Results \n11.14 \n \n \n \n \n \nSoftware Verification \nResults \n11.14 \n \n \n \n  \n  \nSoftware Verification Results \n11.14 \n \n  \n  \n  \n  \nSoftware Verification Results \n11.14 \n \n \n  \n  \n  \nSoftware Verification Results \n11.14 \n \n \n \n  \n  \nSoftware Verification Results \n11.14 \n \n \n \n \n  \nSoftware Verification Results \n11.14 \n \n  \n  \n  \n  \nSoftware Verification Results \n  \nObjective \nApplicability by \nAssurance Level \nOutput",
    "Software Verification Results \n  \nObjective \nApplicability by \nAssurance Level \nOutput \nControl Category by \nAssurance Level \nActivity \n  \nDescription \nRef \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \n1 \nConfiguration items are identified. \n7.1.a \n7.2.1 \n \n \n \n \n SCM Records \n11.18 \n \n \n \n \n \n11.16 \n \n \n \n \n \nSoftware Configuration Index \n2 \n7.1.b \n7.2.2 \n \n \n \n \n \nBaselines and traceability are established. \nSCM Records",
    "2 \n7.1.b \n7.2.2 \n \n \n \n \n \nBaselines and traceability are established. \nSCM Records \n11.18 \n     \nProblem \nReports \n11.17 \n \n \n \n \n \n3 \n \n \n \n \n \n7.1.c \n7.1.d 7.1.e \n7.1.f \n7.2.3 7.2.4 7.2.5 7.2.6 \nSCM Records \n11.18 \n     \nProblem reporting, \nchange control, change review, and configuration status accounting are established. \n4 \n7.1.g \n7.2.7 \n \n \n \n \n SCM Records \n11.18 \n \n \n \n \n \nArchive, retrieval, and release are established. \n5",
    " \n \n \n SCM Records \n11.18 \n \n \n \n \n \nArchive, retrieval, and release are established. \n5 \nSoftware load control is established. \n7.1.h \n7.4 \n \n \n \n \n SCM Records \n11.18 \n \n \n \n \n \n11.15 \n \n \n \n \n \n6 \n7.1.i \n7.5 \n \n \n \n \n \nSoftware Life Cycle Environment Configuration Index \nSoftware life cycle environment control is established. \nSCM Records \n11.18 \n    ",
    "## Software Quality Assurance Process Table A-9",
    "Objective \nApplicability by \nAssurance Level \nOutput \nControl Category by \nAssurance Level \nActivity \n  \nDescription \nRef \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \n1 \n8.1.a \n \n \n \n \n \nSQA Records \n11.19 \n \n \n \n \n \n8.2.b 8.2.h \n8.2.i \nAssurance is obtained that software plans and standards are developed and reviewed for compliance to this document and for consistency. \n2 \n8.1.b \n \n \n \n \n SQA Records \n11.19 \n \n \n \n \n \nAssurance is",
    "2 \n8.1.b \n \n \n \n \n SQA Records \n11.19 \n \n \n \n \n \nAssurance is \nobtained that software life cycle \nprocesses comply with approved software plans. \n8.2.a \n8.2.c \n8.2.d \n8.2.f \n8.2.h \n8.2.i \n3 \n8.1.b \n \n \n \n \n \nSQA Records \n11.19 \n \n \n \n \n \nAssurance is obtained that software life cycle \nprocesses comply with approved software standards. \n8.2.a \n8.2.c \n8.2.d \n8.2.f \n8.2.h \n8.2.i \n4 \n8.1.c \n \n \n \n \n \nSQA Records \n11.19 \n \n \n \n \n \n8.2.e 8.2.h \n8.2.i",
    "8.2.f \n8.2.h \n8.2.i \n4 \n8.1.c \n \n \n \n \n \nSQA Records \n11.19 \n \n \n \n \n \n8.2.e 8.2.h \n8.2.i \nAssurance is obtained that transition criteria for the software life cycle processes are satisfied. \n5 \n8.1.d \n \n \n \n \n SQA Records \n11.19 \n \n \n \n \n \n8.2.g 8.2.h \n8.3 \nAssurance is obtained that software conformity review is conducted.",
    "## Software Approval Process",
    "Objective \nApplicability by \nAssurance Level \nOutput \nControl Category by \nAssurance Level \nActivity \n  \nDescription \nRef \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \n1 \n9.a \n9.1.b 9.1.c \n     \nCommunication and understanding between the applicant and the approval authority is established. \n2 \n9.b \n     \n9.1.a 9.1.b \n9.1.c \nThe means of compliance is proposed and agreement with the \nPlan for Software Aspects of Approval is obtained. \n3 \n9.c \n     \nCompliance substantiation is provided.",
    "3 \n9.c \n     \nCompliance substantiation is provided. \n9.2.a 9.2.b 9.2.c \n \nAL\n5 \nData Item \nRef \nAL\n1 \nAL\n2 \nAL\n3 \nAL\n4 \nAL\n5 \n11.1 \n \n \n \n \n \nPlan for Software Aspects of Approval \n11.1 \n \n \n \n \n \nPlan for Software \nAspects of Approval \n11.20 \n \n \n \n \n \nSoftware Accomplishment Summary  \n11.16 \n     \nSoftware Configuration Index",
    "## Annex B Acronyms And Glossary Of Terms",
    "| Acronym    | Meaning                                     |\n|------------|---------------------------------------------|\n| 3-D        | Three-dimensional                           |\n| AC         | Advisory Circular                           |\n| AL         | Assurance Level                             |\n| ATM        | Air Traffic Management                      |\n| ATS        | Air Traffic Services                        |\n| CAST       | Certification Authorities Software Team     |",
    "| CAST       | Certification Authorities Software Team     |\n| CC1        | Control Category 1                          |\n| CC2        | Control Category 2                          |\n| CNS        | Communication, Navigation, and Surveillance |\n| COTS       | Commercial Off-The-Shelf                    |\n| CPU        | Central Processing Unit                     |\n| CRC        | Cyclic Redundancy Check                     |\n| DO         | Document                                    |",
    "| DO         | Document                                    |\n| EASA       | European Aviation Safety Agency             |",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "| DP      | Discussion Paper                                   |\n|---------|----------------------------------------------------|\n| EUROCAE | European Organization for Civil Aviation Equipment |\n| FAA     | Federal Aviation Administration                    |\n| FAQ     | Frequently Asked Question                          |\n| IDAL    | Item Development Assurance Level                   |\n| I/O     | Input/Output                                       |",
    "| I/O     | Input/Output                                       |\n| ISO     | International Organization for Standardization     |\n| JAA     | Joint Aviation Authorities                         |",
    "## Acronym Meaning",
    "| MC/DC    | Modified Condition/Decision Coverage    |\n|----------|-----------------------------------------|\n| PMC      | Program Management Committee            |\n| PSAA     | Plan for Software Aspects of Approval   |\n| RBT      | Requirements-Based Testing              |\n| RTCA     | RTCA, Inc.                              |\n| SAE      | Society of Automotive Engineers         |\n| SAS      | Software Accomplishment Summary         |\n| SC       | Special Committee                       |",
    "| SC       | Special Committee                       |\n| SCI    | Software Configuration Index                        |\n|--------|-----------------------------------------------------|\n| SCM    | Software Configuration Management                   |\n| SCWG   | Special Committee and Working Group                 |\n| SDP    | Software Development Plan                           |\n| SECI   | Software Life Cycle Environment Configuration Index |",
    "| SECI   | Software Life Cycle Environment Configuration Index |\n|        |                                                     |\n| SQA    | Software Quality Assurance                          |\n| SVP    | Software Verification Plan                          |\n| SW     | Software                                            |\n| TGL    | Temporary Guidance Leaflet                          |\n| TOR    | Terms of Reference                                  |",
    "| TOR    | Terms of Reference                                  |\n| TQL    | Tool Qualification Level                            |\n| U.S.A. | United States of America                            |\n| WG     | Working Group                                       |\n|        |                                                     |",
    "## Glossary\n\nThese definitions are provided for the terms used in this document. If a term is not defined in this annex, it is possible that it is defined instead in the body of this document. \n\nActivity - Tasks that provide a means of meeting the objectives. Adaptation data - Data used to customize elements of the air traffic system for its designated purpose.",
    "Adaptation data item - A set of data that, when in the form of an Adaptation Data Item File,  influence the behavior of the software without modifying the Executable Object Code and that is managed as a separate configuration item. Examples include databases and configuration tables.",
    "Adaptation Data Item File - The representation of the adaptation data item that is directly usable by the processing unit of the target computer. An Adaptation Data Item File is an instantiation of the adaptation data item containing defined values for each data element. \n\nAeronautical data - Data used for aeronautical applications such as navigation, flight planning, flight simulators, terrain awareness, and other purposes.",
    "Airborne - A qualifier used to denote software, equipment, or systems onboard an aircraft. \n\nAlgorithm - A finite set of well-defined rules that give a sequence of operations for performing a specific task. \n\nAlternative method - Different approach to satisfy one or more objectives of this document. \n\nAnomalous behavior - Behavior that is inconsistent with specified requirements.",
    "Anomalous behavior - Behavior that is inconsistent with specified requirements. \n\nApplicant - A person or organization seeking approval from the approval authority. Approval\n - A means by which an authorized body gives formal recognition that a product, process, service, or operation conforms to applicable requirements",
    "Note 1: For example, approval is a generic term to refer to certification, \ncommissioning, qualification, initial operational capability, etc. \nNote 2: The term \"approval\" is used slightly different from DO178C to add \nclarification for CNS/ATM systems. \nApproval authority - The relevant body responsible for the approval in accordance with applicable approval requirements.",
    "Approval credit - Acceptance by the approval authority that a process, software product, or demonstration satisfies an applicable requirement. \n\n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nApproval liaison process - The process that establishes communication, understanding, and agreements between the applicant and approval authority.",
    "Approved source - The location of the software life cycle data to be retrieved is identified in the Software Configuration Index. The \"approved source\" could be a software configuration management library, an electronic archive, or an organization other than the developing organization. \n\nAssurance - The planned and systematic actions necessary to provide adequate confidence and evidence that a product or process satisfies given requirements.",
    "Assurance level\n - The designation that is assigned to a software component as determined by the system safety assessment process. The assurance level establishes the rigor necessary to demonstrate compliance with this document.",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nNote: Other industry documents may use a different term for the designation resulting from the system safety assessment process. Examples are \"item development assurance level\" (IDAL) and \"software level\" which are considered synonymous with the term \"assurance level.\" \nAudit - An independent examination of the software life cycle processes and their outputs to confirm required attributes.",
    "Autocode generator - A coding tool that automatically produces Source Code or object code from low-level requirements. \n\nBaseline - The approved, recorded configuration of one or more configuration items, that thereafter serves as the basis for further development, and that is changed only through change control procedures.",
    "Boolean expression - An expression that results in a TRUE or FALSE value. Boolean operator - An operator with Boolean operands that yields a Boolean result. Certification - Legal recognition by the certification authority that a product, service, organization, or person complies with the requirements. Such certification comprises the activity of technically checking the product, service, organization, or person and the formal recognition of compliance with the applicable requirements by issue",
    "or person and the formal recognition of compliance with the applicable requirements by issue of a certificate, license, approval, or other documents as required by national laws and procedures. In particular, certification of a product involves: (a) the process of assessing the design of a product to ensure that it complies with a set of requirements applicable to that type of product so as to demonstrate an acceptable level of safety; (b) the process of assessing an individual product to",
    "to demonstrate an acceptable level of safety; (b) the process of assessing an individual product to ensure that it conforms with the certified type design; (c) the issuance of a certificate required by national laws to declare that compliance or conformity has been found with requirements in accordance with items (a) or (b) above.",
    "Change control",
    "- (1) The process of recording, evaluating, approving or disapproving and coordinating changes to configuration items after formal establishment of their configuration identification or to baselines after their establishment. (2) The systematic evaluation, coordination, approval or disapproval, and implementation of approved changes in the configuration of a configuration item after formal establishment of its configuration identification or to baselines after their establishment.",
    "Note: This term may be called configuration control in other industry standards. \n\nCode - The implementation of particular data or a particular computer program in a symbolic form, such as Source Code, object code, or machine code. \n\nCommercial Off-The-Shelf (COTS) software\n - Software under consideration for use in a CNS/ATM system that may have no or only partial evidence of compliance to this document sections 4 - 9 objectives.",
    "Note: The term \"COTS software\" is used slightly different from DO-178C to add clarification for CNS/ATM systems. \n\nCompacted expressions - Representations of Source Code where many constructs are combined into a single expression. \n\nCompiler - Program that translates Source Code statements of a high level language, such as FORTRAN or Pascal, into object code. \n\nComponent - A self-contained part, combination of parts, subassemblies, or units that performs a distinct function of a system.",
    "Condition - A Boolean expression containing no Boolean operators except for the unary operator (NOT). \n\nConfiguration identification - (1) The process of designating the configuration items in a system and recording their characteristics. (2) The approved documentation that defines a configuration item.",
    "Configuration item - (1) One or more hardware or software components treated as a unit for configuration management purposes. (2) Software life cycle data treated as a unit for configuration management purposes.",
    "Configuration management - (1) The process of (a) identifying and defining the configuration items of a system; (b) controlling the release and change of these items throughout the software life cycle; (c) recording and reporting the status of configuration items and Problem Reports; and (d) verifying the completeness and correctness of configuration items. (2) A discipline applying technical and administrative direction and surveillance to (a) identify and record the functional and physical",
    "administrative direction and surveillance to (a) identify and record the functional and physical characteristics of a configuration item; (b) control changes to those characteristics; and (c) record and report change control processing and implementation status.",
    "Configuration status accounting - The recording and reporting of the information necessary to manage a configuration effectively, including a listing of the approved configuration identification, the status of proposed changes to the configuration, and the implementation status of approved changes.",
    "Control category - Configuration management controls placed on software life cycle data. The two categories, CC1 and CC2, define the software configuration management processes and activities applied to control software life cycle data. \n\nControl coupling - The manner or degree by which one software component influences the execution of another software component. \n\nControl program - A computer program designed to schedule and to supervise the execution of programs in a computer system.",
    "Coverage analysis - The process of determining the degree to which a proposed software verification process activity satisfies its objective. \n\nCutover - A process of switching from one operating version of software to another with limited or no interruption in service. This is also known as hot swapping. \n\nData coupling - The dependence of a software component on data not exclusively under the control of that software component.",
    "Data dictionary - The detailed description of data, parameters, variables, and constants used by the system. \n\nDatabase - A set of data, part or the whole of another set of data, consisting of at least one file that is sufficient for a given purpose or for a given data processing system.",
    "Deactivated code - Executable Object Code (or data) that is traceable to a requirement and by design is either (a) not intended to be executed (code) or used (data), for example, a part of a previously developed software component such as unused legacy code, unused library functions, or future growth code, or (b) is only executed (code) or used (data) in certain configurations of the target computer environment, for example, code that is enabled by a hardware pin selection or software",
    "computer environment, for example, code that is enabled by a hardware pin selection or software programmed options. The following examples are often mistakenly categorized as deactivated code but should be identified as required for implementation of the design/requirements: defensive programming structures inserted for robustness, including compiler-inserted object code for range and array index checks, error or exception handling routines, bounds and reasonableness checking, queuing controls,",
    "checks, error or exception handling routines, bounds and reasonableness checking, queuing controls, and time stamps.",
    "Dead code - Executable Object Code (or data) which exists as a result of a software development error, but cannot be executed (code) or used (data) in any operational configuration of the target computer environment. It is not traceable to a system or software requirement. The following exceptions are often mistakenly categorized as dead code but are necessary for implementation of the requirements/design: embedded identifiers, defensive programming structures to improve robustness, and",
    "embedded identifiers, defensive programming structures to improve robustness, and deactivated code such as unused library functions.",
    "Decision - A Boolean expression composed of conditions and zero or more Boolean operators. If a condition appears more than once in a decision, each occurrence is a distinct condition. \n\nDecision coverage - Every point of entry and exit in the program has been invoked at least once and every decision in the program has taken on all possible outcomes at least once.",
    "Derived requirements - Requirements produced by the software development processes which (a) are not directly traceable to higher level requirements, and/or (b) specify behavior beyond that specified by the system requirements or the higher level software requirements.",
    "Embedded identifier - Identification attributes of the software, for example, creation date, part number, linker integrity verification checksum or cyclic redundancy check (CRC), or version identification, included in the target Executable Object Code. \n\nEmulator - A device, computer program, or system that accepts the same inputs and produces the same output as a given system using the same object code.",
    "End-to-end numerical resolution - Measure of the numerical precision resulting from computations through the integrated system. \n\nEquivalence class - The partition of the input domain of a program such that a test of a representative value of the class is equivalent to a test of other values of the class. \n\nEquivalent safety - Level of safety achieved using an alternative method to satisfy both the objectives of this document and the system safety objectives.",
    "Error - With respect to software, a mistake in requirements, design, or code. Executable Object Code - A form of code that is directly usable by the processing unit of the target computer and is, therefore, a compiled, assembled, and linked binary image that is loaded into the target computing hardware.",
    "Extraneous code - Code (or data) that is not traceable to any system or software requirement. An example of extraneous code is legacy code that was incorrectly retained although its requirements and test cases were removed. Another example of extraneous code is dead code. \n\nFailure - The inability of a system or system component to perform a required function within specified limits. A failure may be produced when a fault is encountered.",
    "Failure condition - The effect on a CNS/ATM system or equipment, both direct and consequential, caused or contributed to by one or more failures which may be attributed to, for example, a latent software error in the executable code or data considering relevant adverse operational and environmental conditions.",
    "Fault - A manifestation of an error in software. A fault, if it occurs, may cause a failure. Fault tolerance - The built-in capability of a system to provide continued correct execution in the presence of a limited number of hardware or software faults. \n\nFormal methods - Descriptive notations and analytical methods used to construct, develop, and reason about mathematical models of system behavior. A formal method is a formal analysis carried out on a formal model.",
    "Gap analysis - An engineering activity designed to identify the differences between a present state and the intended future state. In the context of this document, a gap analysis is conducted to identify what additional or alternate activities should be accomplished to comply with a specified set of objectives. \n\nHardware/software integration - The process of combining the software into the target computer.",
    "Hardware/software integration - The process of combining the software into the target computer. \n\nHigh-level requirements - Software requirements developed from analysis of system requirements, safety-related requirements, and system architecture.",
    "Host computer - The computer on which the software is developed. Hot swapping - See Cutover. Independence - Separation of responsibilities which ensures the accomplishment of objective evaluation. (1) For software verification process activities, independence is achieved when the verification activity is performed by a person(s) other than the developer of the item being verified, and a tool(s) may be used to achieve an equivalence to the human verification activity. (2) For the software",
    "may be used to achieve an equivalence to the human verification activity. (2) For the software quality assurance process, independence also includes the authority to ensure corrective action.",
    "In-service hour - Use of software for one hour in a controlled environment which is functionally equivalent to the target environment. \n\nIntegral process - A process which assists the software development processes and other integral processes and, therefore, remains active throughout the software life cycle. The integral processes are the software verification process, the software quality assurance process, the software configuration management process, and the approval liaison process.",
    "Integrity - An attribute of the system or an item indicating that it can be relied upon to work correctly on demand. \n\nInterrupt - A suspension of a task, such as the execution of a computer program, caused by an event external to that task, and performed in such a way that the task can be resumed.",
    "Low-level requirements - Software requirements developed from high-level requirements, derived requirements, and design constraints from which Source Code can be directly implemented without further information. \n\nMeans of compliance - The intended method(s) to be used by the applicant to satisfy the requirements stated in the approval basis for a CNS/ATM system or equipment.",
    "Examples include statements, drawings, analyses, calculations, testing, simulation, inspection, and environmental qualification. Advisory material issued by the approval authority is used if appropriate. \n\nMedia - Devices or material which act as a means of transferring or storing of software, for example, programmable read-only memory, magnetic tapes or discs, and paper.",
    "Memory device - An article of hardware capable of storing machine-readable computer programs and associated data. Examples include an integrated circuit chip, a circuit card containing integrated circuit chips, a core memory, a disk, or a magnetic tape.",
    "Modified condition/decision coverage - Every point of entry and exit in the program has been invoked at least once, every condition in a decision in the program has taken all possible outcomes at least once, every decision in the program has taken all possible outcomes at least once, and each condition in a decision has been shown to independently affect that decision's outcome. A condition is shown to independently affect a decision's outcome by: (1) varying just that condition while holding",
    "to independently affect a decision's outcome by: (1) varying just that condition while holding fixed all other possible conditions, or (2) varying just that condition while holding fixed all other possible conditions that could affect the outcome.",
    "Monitoring The act of witnessing or inspecting selected instances of test, inspection, or other activity, or records of those activities, to assure that the activity is under control and that the reported results are representative of the expected results. Monitoring is usually associated with activities done over an extended period of time where 100% witnessing is considered impractical or unnecessary. Monitoring permits authentication that the claimed activity was performed as planned.",
    "Multiple-version dissimilar software - Two or more software components that satisfy the same functional requirements, but are intentionally different from one another. Example approaches include the use of separate development organizations or the use of different development techniques. Common mode errors may be minimized by using multipleversion dissimilar software techniques.",
    "Object code - A low-level representation of the computer program not usually in a form directly usable by the target computer but in a form which includes relocation information in addition to the processor instruction information. \n\nObjective - When this document is identified as a means of compliance to the regulations, the objectives are requirements that should be met to demonstrate compliance.",
    "Obsolescence - A condition in which hardware or software becomes out of date and is no longer supportable, causing a need for replacement. \n\nPart number - A set of numbers, letters or other characters used to identify a configuration item. \n\nPartitioning - A technique for providing isolation between software components to contain and/or isolate faults.",
    "Patch - Modification to Executable Object Code in which one or more of the planned steps of re-compiling, re-assembling, or re-linking is bypassed. This does not include embedded identifiers. \n\nPreviously developed software - Software already developed for use. This encompasses a wide range of software, including Commercial Off-The-Shelf (COTS) software through software developed to previous or current software guidance.",
    "Process - A collection of activities performed in the software life cycle to produce a definable output or product. \n\nRelease - The act of formally making available and authorizing the use of a retrievable configuration item. \n\nReverification - The evaluation of the results of a modification process, for example, correction of errors or the introduction of new or additional functionality, to ensure correctness and consistency with respect to the inputs and standards provided to that process.",
    "Reverse engineering - The process of developing higher level software data from existing software data. Examples include developing Source Code from object code or Executable Object Code, or developing high-level requirements from low-level requirements. \n\nRobustness - The extent to which software can continue to operate correctly despite abnormal inputs and conditions.",
    "Safety monitoring - A means of protecting against specific failure conditions by directly monitoring a function for failures that would result in a failure condition.   \nService experience - Intervals of time during which the software is operated within a known relevant and controlled environment, during which successive failures are recorded.",
    "Service experience data - Data obtained on successful and failed operation during the execution of the software over the service experience intervals. \n\nSimulator - A device, computer program or system used during software verification, that accepts the same inputs and produces the same output as a given system, using object code that is derived from the original object code.",
    "Single event upset - Random bit flip in data that can occur in hardware. Software - Computer programs and, possibly, associated documentation and data pertaining to the operation of a computer system. \n\nSoftware architecture - The structure of the software selected to implement the software requirements. \n\nSoftware assurance - The planned and systematic actions necessary to provide confidence and evidence that a software product or process satisfies given requirements.",
    "Software change - A modification in Source Code, object code, Executable Object Code, or its related documentation from its baseline. \n\nSoftware conformity review - A review, typically conducted at the end of a software development project, for the purpose of assuring that the software life cycle processes are complete, software life cycle data is complete, and the Executable Object Code is controlled and can be regenerated.",
    "Software development - A set of activities that result in software products. Software development may include new development, modifications, reuse, reengineering, maintenance, or any other activities that result in software products.",
    "Software development standards Standards which define the rules and constraints for the software development processes. The software development standards include the Software Requirements Standards, the Software Design Standards, and the Software Code Standards.",
    "Software integration - The process of combining code components. Software integrity assurance standard - A standard that provides criteria to evaluate and measure a software product and/or a process to provide assurance that the product and/or process satisfies given requirements and can be relied upon to work correctly in its intended environment. The criteria are a standard set of items dependent upon the assurance level and system failure classification, as determined by the system safety",
    "upon the assurance level and system failure classification, as determined by the system safety assessment process. The standard set of items is criteria to be applied to the software life cycle processes and data to demonstrate compliance to the documented process and correctness of the product. The software integrity assurance standard is the uniform measure of how the software was developed (the process) and a measure of the ability of the product to function as intended (the product).",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nSoftware level\n - The designation that is assigned to an airborne software component as determined by the system safety assessment process. The software level establishes the rigor necessary to demonstrate compliance with DO-178C. \n\n \nNote: The term \"software level\" is used in the airborne software domain and is considered synonymous with the term \"assurance level\" as used in the CNS/ATM domain.",
    "Software library - A controlled repository containing a collection of software and related data and documents designed to aid in software development, use, or modification. Examples include software development library, master library, production library, program library and software repository.",
    "Software life cycle - (1) An ordered collection of processes determined by an organization to be sufficient and adequate to produce a software product. (2) The period of time that begins with the decision to produce or modify a software product and ends when the product is retired from service.",
    "Software partitioning - The process of separating, usually with the express purpose of isolating one or more attributes of the software, to prevent specific interactions and crosscoupling interference. \n\nSoftware product - The set of computer programs, and associated documentation and data, designated for delivery to a user. In the context of this document, this term refers to software intended for use in CNS/ATM system applications and the associated software life cycle data.",
    "Software requirement - A description of what is to be produced by the software given the inputs and constraints. Software requirements include both high-level requirements and low-level requirements. \n\nSoftware tool - A computer program used to help develop, test, analyze, produce, or modify another program or its documentation. Examples are an automated design tool, a compiler, test tools, and modification tools.",
    "Source Code - Code written in source languages, such as assembly language and/or high level language, in a machine-readable form for input to an assembler or a compiler. \n\nStandard - A rule or basis of comparison used to provide both guidance in and assessment of the performance of a given activity or the content of a specified data item. \n\n - Every statement in the program has been invoked at least once. \n\n## Statement Coverage\n\nNote: Statement is as defined by the programming language.",
    "## Statement Coverage\n\nNote: Statement is as defined by the programming language. \n\nStructural coverage analysis - An evaluation of the code structure, including interfaces, exercised during requirements based testing.",
    "Structure - A specified arrangement or interrelation of parts to form a whole. Supplement - Guidance used in conjunction with this document that addresses the unique nature of a specific approach, method, or technique. A supplement adds, deletes or otherwise modifies: objectives, activities, explanatory text, and software life cycle data in this document. \n\nSystem - A collection of hardware and software components organized to accomplish a specific function or set of functions.",
    "System architecture - The structure of the hardware and the software selected to implement the system requirements.",
    "System safety assessment process - An ongoing, systematic, comprehensive evaluation of the proposed system to show that relevant safety-related requirements are satisfied. The major activities within this process include: functional hazard assessment, preliminary system safety assessment, and system safety assessment. The rigor of the activities will depend on the criticality, complexity, novelty, and relevant service experience of the system concerned.",
    "Task - The basic unit of work from the standpoint of a control program. Test case - A set of test inputs, execution conditions, and expected results developed for a particular objective, such as to exercise a particular program path or to verify compliance with a specific requirement. \n\nTest procedure - Detailed instructions for the set-up and execution of a given set of test cases, and instructions for the evaluation of results of executing the test cases.",
    "Testing - The process of exercising a system or system component to verify that it satisfies specified requirements and to detect errors. \n\nTool qualification - The process necessary to obtain approval credit for a software tool within the context of a specific CNS/ATM system.",
    "Trace data - Data providing evidence of traceability of development and verification processes' software life cycle data without implying the production of any particular artifact. Trace data may show linkages, for example, through the use of naming conventions or through the use of references or pointers either embedded in or external to the software life cycle data.",
    "Traceability - An association between items, such as between process outputs, between an output and its originating process, or between a requirement and its implementation. \n\nTransition criteria - The minimum conditions, as defined by the software planning process, to be satisfied to enter a process.",
    "Unbounded recursive algorithm - An algorithm that directly invokes itself (self recursion) or indirectly invokes itself (mutual recursion), and does not have a mechanism to limit the number of times it can do this before completing. \n\nUser-modifiable software - Software intended for modification without review by the approval authority, the CNS/ATM system manufacturer, or the equipment vendor, if within the modification constraints established during the original approval project.",
    "Validation - The process of determining that the requirements are the correct requirements and that they are complete. The system life cycle processes may use software requirements and derived requirements in system validation. \n\nVerification - The evaluation of the outputs of a process to ensure correctness and consistency with respect to the inputs and standards provided to that process.",
    "Wrapper code\n - Software used to encapsulate some functionality of a system for the purpose of isolating it from the remaining functionality of the system. \n\n## Appendix A Background Of Do-278/Ed-109 Document 1. Prior Document Version History A. Software Considerations In Airborne Systems And Equipment Certification",
    "DO-178B/ED-12B and its predecessors provide industry-accepted guidance on how airworthiness requirements can be satisfied when using software in airborne systems and equipment. DO-178B/ED-12B is formally recognized as a means of evaluating software for the purposes of complying with applicable regulations by the Federal Aviation Administration (FAA) in Advisory Circular (AC) 20-115B and by the Joint Aviation Authorities (JAA) in Temporary Guidance Leaflet (TGL) Number 4. Other approval",
    "the Joint Aviation Authorities (JAA) in Temporary Guidance Leaflet (TGL) Number 4. Other approval authorities have similar means of recognizing DO-178B/ED-12B as a means of showing compliance to the regulations.",
    "The International Organization for Standardization (ISO) recognized DO-178B/ED-12B \nas a de facto international standard in 1997. \n\n## B. The Structure Of Do-178B/Ed-12B\n\nDO-178B/ED-12B provides detailed guidelines for the production of software for airborne systems that perform intended functions with a level of confidence in safety and which comply with airworthiness requirements. This is achieved by:",
    "- \nDefining objectives for the software life cycle processes. \n- \nIdentifying activities and considerations which represent a means of satisfying these objectives. \n- \nProviding a description of the evidence indicating the objectives have been satisfied.",
    "- \nProviding a description of the evidence indicating the objectives have been satisfied. \nDO-178B/ED-12B varies the rigor of the software development process according to the system failure condition classifications. The failure condition classes used are those adopted from FAA AC 25.1309-1A and/or the JAA Advisory Material - Joint (AMJ)",
    "25.1309, as amended. The failure condition classifications are defined as catastrophic, hazardous/severe-major, major, minor, and no effect. DO-178B/ED-12B assumes that a system safety assessment process is applied to identify the appropriate failure condition classification and software level, from A to E, with Level A software having the highest integrity. The main body of DO-178B/ED-12B identifies the objectives to be satisfied for Level A",
    "software. Annex A of the document identifies the objectives to be satisfied for each level. The general approach is that fewer objectives are needed to be satisfied as the software level is reduced.",
    "## C. The Need For Do-278/Ed-109\n\nSince publication, the aviation community has gained experience using DO-178B/ED-\n12B and raised a number of questions regarding the document's content and application.",
    "In order to address these questions, EUROCAE Working Group 52 (WG-52) was formed February 1996 and, together with RTCA Special Committee 190 (SC-190), became a joint committee in September 1996. In 1999, this group issued DO-248/ED-94, First Annual Report for Clarification of DO-178B/ED-12B \"Software Considerations in Airborne Systems and Equipment Certification.\" A second annual report was issued in 2000 and a final report in 2001. The final report, DO-248B/ED-94B, contains all of the errata,",
    "in 2000 and a final report in 2001. The final report, DO-248B/ED-94B, contains all of the errata, Frequently Asked Questions (FAQs), and Discussion Papers (DP's) approved by the committee. Around the same time that SC-190 and WG-52 was established, another joint RTCA/EUROCAE group (SC-189/WG-53) was established. One of their tasks was to produce a safety assessment process that would identify the safety significance of software in CNS/ATM data communication systems. Assurance for software in",
    "safety significance of software in CNS/ATM data communication systems. Assurance for software in the airborne systems was already addressed by DO-178B/ED-12B. However, guidance did not exist for software in CNS/ATM systems not located on the aircraft.",
    "Some questions had also been provided to SC-190/WG-52 about how to address software in non-airborne systems that could affect the safety of aircraft directly or indirectly. As a result of these questions and the work being performed by SC-189/WG-53, SC-190/WG-",
    "52 established a team to: \"Develop guidance material based on DO-178B/ED-12B for Communication, Navigation and Surveillance (CNS) and Air Traffic Management (ATM) software shown by system safety assessment to affect the safety of aircraft occupants or airframe in its operational environment. Relevant software integrity assurance standards, and approval processes should be taken into account.\" After reviewing the applicability of DO-178B/ED-12B to CNS/ATM systems, it was decided to develop a new",
    "reviewing the applicability of DO-178B/ED-12B to CNS/ATM systems, it was decided to develop a new document containing guidance material. This became DO- 278/ED-109. Some principles were established to guide the production of the guidance. These principles indicate the original intent of the guidance in this document:",
    "- \nThere should be no changes to the guidance to achieve certification of airborne systems. \n- \nThere should be consensus from the airborne and CNS/ATM community that the new guidance provides the same level of software integrity assurance for CNS/ATM nonairborne software of a given level as that obtained in airborne systems. \n- \nThere should be consensus from the non-airborne CNS/ATM community that the new guidance is practicable, feasible and relevant to non-airborne CNS/ATM \nsoftware. \n-",
    "software. \n- \nRepresentatives from ATM service providers and equipment providers should be included on the working group. \n- \nThe regulators and certification authorities should find the guidance material being produced by the CNS/ATM team acceptable. \n- \nDifferences in regulatory and certification requirements for airborne and nonairborne systems should be considered during the preparation of the guidance material. \n-",
    "- \nThere is no intent to change approval nor certification mechanisms as part of this activity. \n- \nThe new guidance should be reviewed against what is performed at present for \nCNS/ATM systems. \n- \nThe products produced by the CNS/ATM Team should be consistent with the \nproducts of SC-189/WG-53",
    "## D. Features Of Do-278/Ed-109\n\nThe SC-190/WG-52 CNS/ATM team who developed the guidance included representatives from CNS/ATM service providers, equipment suppliers and regulators. This was intended to ensure that the guidance would be applicable to the industry that would have to apply it. The team reviewed guidance developed for the airborne segment of the civil aviation community (DO-178B/ED-12B). In some cases, the guidance was updated for application to CNS/ATM systems.",
    "In preparing the guidance, consideration was given to:",
    "1. Different regulatory environments for airborne and non-airborne systems. The result \nwas that all references to 'certification' were removed from the document. \n2. Different characteristics of airborne and non-airborne systems including the \nenvironments in which they are used.",
    "CNS/ATM systems operate continuously which demands high availability constraints. These systems are also often large-scale, interconnected systems that make use of commercial services, for example, telecommunications, as well as commercial software. In such systems, integration is the focus rather than development and maintenance personnel usually support these systems at all times. This resulted in additional guidance being included for COTS software and adaptation data.",
    "3. Lack of a common and accepted definition of failure conditions for non-airborne \nCNS/ATM systems.",
    "CNS/ATM systems. \nThe result was that six software levels were established. These are referred to as assurance levels 1 to 6 in contrast to software levels A to E in DO-178B/ED-12B. The allocation of objectives to software levels was achieved by using the domain knowledge of the CNS/ATM team. It is assumed that anyone using the guidance material has their own processes to establish the appropriate software level.",
    "## 2. Rtca/Eurocae Committee Activities In The Production Of This Document",
    "Since 2002, the aviation industry and approval authorities around the world have used the considerations in DO-278/ED-109 as an acceptable means of compliance for the approval of software in CNS/ATM systems. As experience was gained in the use of DO-278/ED- 109, questions arose regarding the document's content and application. Some of these questions were addressed in DO-248B/ED-94B. However, DO-248B/ED-94B did not contain additional guidance for approval of software, only clarifications.",
    "DO-248B/ED-94B did not contain additional guidance for approval of software, only clarifications. Additionally, advances in hardware and software technology resulted in software development",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nmethodologies and issues which were not adequately addressed in DO-278/ED-109 or DO-248B/ED-94B. In 2004, the FAA and aviation industry representatives initiated a discussion with RTCA",
    "concerning the advances in software technology since 2002, when DO-278/ED-109 was published. RTCA then requested a Software Ad Hoc committee evaluate the issues and determine the need for improved guidance in light of these advancements in technology. The Software Ad Hoc committee, which included European participants, recommended to RTCA that a special committee be formed to address these issues. In December 2004, RTCA and EUROCAE approved the sponsorship of such a joint special",
    "these issues. In December 2004, RTCA and EUROCAE approved the sponsorship of such a joint special committee/working group, Special Committee 205/Working Group 71, SC-205/WG-71.",
    "a. The Terms of Reference (TORs) provided to the Special Committee and Working \nGroup  (SCWG) were: \n1. Modify DO-178B/ED-12B to become DO-178C/ED-12C, or other document \nnumber. \n2. Modify DO-248B/ED-94B to become DO-248C/ED-94C, or other document \nnumber. \n3. Consolidate Software Development Guidance. \n4. Consolidate Software Development Guidelines. \n5. Develop and document technology-specific or method-specific guidance and \nguidelines.",
    "5. Develop and document technology-specific or method-specific guidance and \nguidelines. \n6. Determine, document and report the effects of DO-178C/ED-12C or other \nmodified documents to DO-278/ED-109 and recommend direction to ensure \nconsistency. \n7. Develop and document rationale for each DO-178B/ED-12B objective. 8. Evaluate the issues in the 'Software Issues List.xls' spreadsheet produced by the",
    "Software Ad Hoc committee and other identified issues. Determine \"if'\", \"where\" and \"how\" each issue should be addressed. \n9. Coordinate SCWG products with software certification authorities via \nCertification Authorities Software Team (CAST) or other appropriate groups. \n10. Coordinate with other groups and existing organizations, for example, SAE S18, \nWG-63, SC-200/WG-60, as appropriate. \n11. Report to the SCWG's governing body the direction being taken by the",
    "11. Report to the SCWG's governing body the direction being taken by the \ncommittee within 6-9 months after the first SCWG meeting. \n12. Work with RTCA and EUROCAE to explore and implement ways of expanding \nthe usability of the deliverables (for example, hypertext electronic versions). \n13. Modify DO-278/ED-109 to become DO-278A/ED-109A, or other document \nnumber. \n14. Submit to RTCA and EUROCAE a DO-178C/ED-12C and DO-278A/ED-109A \ncommonality analysis when documents are finalized.",
    "commonality analysis when documents are finalized. \nb. The RTCA Program Management Committee (PMC) directed the SCWG to maintain \nor adhere to the following while accomplishing the TORs: \n1. Maintain the current objective-based approach for software assurance. \n2. Maintain the technology independent nature of the DO-178B/ED-12B and \nDO-278/ED-109 objectives. \n3. Evaluate issues as brought forth to the SCWG. For any candidate guidance",
    "3. Evaluate issues as brought forth to the SCWG. For any candidate guidance \nmodifications determine if the issue can be satisfied first in guideline related documents. \n4. Modifications to DO-178B/ED-12B and DO-278/ED-109 should: \ni. \nIn the context of maintaining backward compatibility with DO-178B/ED-",
    "i. \nIn the context of maintaining backward compatibility with DO-178B/ED-\n12B, make those changes to the existing text that are needed to adequately address the current states of the art and practice in software development in support of system safety, to address emerging trends, and to allow change with technology. \nii. Consider the economic impact relative to system certification or approval \nwithout compromising system safety.",
    "without compromising system safety. \niii. Address clear errors or inconsistencies in DO-178B/ED-12B and DO-\n278/ED-109. \niv. Fill any clear gaps in DO-178B/ED-12B and DO-278/ED-109. \nv. Meet a documented need to a defined assurance benefit. \nvi. Report any proposed changes to the number of software levels or mapping of \nlevels to hazard categories to the SCWG's governing body and provide a documented substantiated need, at the earliest feasible opportunity.",
    "Communicate back to the SCWG at large, any concerns of the governing body. \nvii. Ensure that all deliverables produced by the committee contain consistent \nand complete usability mechanisms (for example, indexes, glossaries). \nc. Seven joint RTCA/EUROCAE working groups were formed to address the TORs: \n1. Documentation Integration. \n2. Issues and Rationale. \n3. Tool Qualification. 4. Model-Based Development and Verification. \n5. Object-Oriented Technology. \n6. Formal Methods.",
    "5. Object-Oriented Technology. \n6. Formal Methods. \n7. Special Considerations and CNS/ATM. \nd. The cooperative efforts of SC-205 and WG-71 culminated in the publication of \nRTCA document DO-278A and EUROCAE document ED-109A.",
    "## 3. Summary Of Differences Between Do-278 And Do-278A\n\nDO-278A is an update to DO-278. The DO-278A updates fall into a variety of categories:",
    "a. Creation of a standalone document\n: DO-278A can be used without referring to DO-\n178C.  When DO-278 was created, it was intended to be used with DO-178B. For example, often caused one to purchase both documents and have them open \"side-byside\" in order to effectively and efficiently use them for software approvals. \nErrors and inconsistencies\nb. \n:  DO-278A addressed DO-178B's and DO-278's known",
    "Errors and inconsistencies\nb. \n:  DO-278A addressed DO-178B's and DO-278's known \nerrors and inconsistencies. For example, DO-278A has addressed the errata of DO- 178B and has removed inconsistencies between the different tables of DO-178B Annex A and DO-278 Annex A. \nConsistent terminology\nc. \n:  DO-278A addressed issues regarding the use of specific \nterms such as \"guidance\", \"guidelines\", \"purpose\", \"goal\", \"objective\", and",
    "terms such as \"guidance\", \"guidelines\", \"purpose\", \"goal\", \"objective\", and \n\"activity\" by changing the text so that the use of those terms is consistent throughout \nthe document. \nWording improvements\nd. \n:  DO-278A made wording improvements throughout the \ndocument. All such changes were made simply to make the document more precise; they were not meant to change the original intent of DO-278. These changes included",
    "bringing the use of the word \"applicant\" in line with DO-178C and using the term \n\"supplier\" as a sub-tier developer of software or COTS.",
    "## Objectives And Activities",
    "e. \n:  DO-278A reinforced the point that, in order to fully \nunderstand the recommendations, the full body of this document should be considered. For example, Annex A now includes references to each activity as well as to each objective; and section 1.4, titled \"How to Use This Document\" reinforces \nthe point that activities are a major part of the overall guidance. \nSupplements\nf. \n:  DO-278A recognized that new software development techniques may",
    "Supplements\nf. \n:  DO-278A recognized that new software development techniques may \nresult in new issues. Rather than expanding text to account for all the current software development techniques and being revised yet again to account for future \ntechniques, DO-278A acknowledged that one or more supplements may be used in conjunction with DO-278A to modify the guidance for specific techniques. \nTool qualification (Section 12.2)\ng. \n:  The terms \"development tool\" and \"verification",
    "Tool qualification (Section 12.2)\ng. \n:  The terms \"development tool\" and \"verification \ntool\" are replaced by three tool qualification criteria that determine the applicable tool qualification level (TQL) in regard of the software level. The guidance to qualify a tool is removed in DO-178C, but provided in a domain independent, external document, referenced in section 12.2. \nh. \nCoordinated system/software aspects:  DO-278A updated Section 2, which provides",
    "h. \nCoordinated system/software aspects:  DO-278A updated Section 2, which provides \nsystem aspects relating to software development, to reflect current system practices. The updates were based upon coordination with the committees which were updating \nARP 4754 for system-level guidance at the same time SC-205/WG-71 was updating DO-278 for software-level guidance.",
    "i. \nDO-278 \"hidden\" objectives\n:  DO-278A added the so-called \"hidden objectives\" to \nAnnex A: \n1. A means for detecting additional code that is not directly traceable to the Source \nCode and a means to ensure its verification coverage are defined (see Objective 9 of \nTable A-7\n). \n). \n2. Assurance is obtained that software plans and standards are developed and \nreviewed for consistency (see Objective 1 of \nTable A-9",
    "reviewed for consistency (see Objective 1 of \nTable A-9\nAlso, Trace Data was identified as software life cycle data (see 11.21, Table A-2, and \n).",
    "Table A-6\n\nGeneral topics\nj. \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n\n:  DO-278A addressed some general topics that resulted in changes to \nseveral sections of the document. The topics included a variety of subjects such as applicant's oversight of suppliers, adaptation data items, and traceability. In \naddressing these topics, three additional objectives were added to Annex A: \n\n## ). Table A-2",
    "1. Adaptation data and related processes are defined (when applicable) (see \nObjective 8 of \nTable A-\n2. Adaptation Data Item File is correct and complete (see Objective 8 of \n). \n5\nTable \n3. Verification of Adaptation Data Item File is achieved (see Objective 9 of \n). \nA-5\nDO-278 gaps and clarifications\nk. \n:  DO-278A addressed several specific issues that",
    "). \nA-5\nDO-278 gaps and clarifications\nk. \n:  DO-278A addressed several specific issues that \nresulted in change to only one or two paragraphs. Each such change may have an impact upon the applicant as these changes either addressed clear gaps in DO-178B or clarified guidance that was subject to differing interpretations. \nExamples of gaps addressed include:",
    "1. The \"Modified Condition/Decision Coverage\" (MC/DC) definition changed. \nMasking MC/DC and Short Circuit, as well as DO-178B's interpretation of MC/DC (often termed \"Unique-Cause\" MC/DC), are now allowed (see Glossary). \n2. Derived requirements should now be provided to the system processes, including \nthe system safety assessment process, rather than just provided to the system safety assessment process (see 5.1.1.b and 5.2.1.b). \nExamples of clarifications include:",
    "1. Clarified that the structural coverage analysis of data and control coupling \nbetween code components should be achieved by assessing the results of the requirements-based tests (see 6.4.4.2.c). \n2. Clarified that all tests added to achieve structural coverage are based on \nrequirements (see 6.4.4.2.d). \nl. \nCOTS software\n:  DO-278A was updated to provide additional information, methods",
    "l. \nCOTS software\n:  DO-278A was updated to provide additional information, methods \nand improved objectives tables to assist software approvals when using Commercial Off-The-Shelf software. \nService experience\nm. \n:  DO-278A was expanded to include extra techniques, approaches \nand methodologies for using service experience data when seeking software approvals. \n \n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "## Appendix B Committee Membership Executive Committee Members\n\n| Jim Krodel, Pratt & Whitney            |     |     |             |\n|----------------------------------------|-----|-----|-------------|\n| Grard Ladier, Airbus/Aerospace Valley |     |     | WG-71 Chair |\n\nMike DeWalt, Certification Services, Inc./FAA  \nSC-205 Secretary (until March 2008)",
    "| Leslie Alford, Boeing Company             |     |                 | SC-205 Secretary (from March 2008)    |\n|-------------------------------------------|-----|-----------------|---------------------------------------|\n| Ross Hannan, Sigma Associates (Aerospace) |     | WG-71 Secretary |                                       |\n| Barbara Lingberg, FAA                     |     |                 |                                       |",
    "| Jean-Luc Delamaide, EASA                  |     |                 |                                       |\n| John Coleman, Dawson Consulting           |     |                 | Sub-group Liaison                     |",
    "Matt Jaffe, Embry-Riddle Aeronautical University Web Site Liaison Todd R. White, L-3 Communications/Qualtech  \nCollaborative Technology Software Liaison \n\n## Sub-Group Leadership Sg-1 - Document Integration\n\n| Ron Ashpole, SILVER ATENA     |     |     | SG-1 Co-chair    |\n|-------------------------------|-----|-----|------------------|\n\n--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n\n| Tom Ferrell, Ferrell and Associates Consulting                      | SG-1 Co-chair (until March 2008)    |\n|---------------------------------------------------------------------|-------------------------------------|\n| Marty Gasiorowski, Worldwide Certification Services                 | SG-1 Co-chair (from March 2008)     |\n| Tom Roth, Airborne Software Certification Consulting SG-1 Secretary |                                     |",
    "## Sg-2 - Issues And Rationale",
    "| Ross Hannan, Sigma Associates (Aerospace)     |                                  | SG-2 Co-chair    |\n|-----------------------------------------------|----------------------------------|------------------|\n| Mike DeWalt, Certification Services, Inc./FAA | SG-2 Co-chair (until March 2008) |                  |\n| Will Struck, FAA                              |                                  |                  |",
    "| Fred Moyer, Rockwell Collins                  |                                  |                  |\n| John Angermayer, Mitre                        |                                  |                  |",
    "## Sg-3 - Tool Qualification",
    "| Frdric Pothon, ACG Solutions            |     |               |                   | SG-3 Co-chair                      |\n|-------------------------------------------|-----|---------------|-------------------|------------------------------------|\n| Leanna Rierson, Digital Safety Consulting |     | SG-3 Co-chair |                   |                                    |",
    "| Bernard Dion, Esterel Technologies        |     |               | SG-3 Co-secretary |                                    |\n| Gene Kelly, CertTech                      |     |               |                   | SG-3 Co-secretary (until May 2009) |\n| Mo Piper, Boeing Company                  |     |               |                   | SG-3 Co-secretary (from May 2009)  |",
    "## Sg-4 - Model-Based Development And Verification\n\n| Pierre Lionne, EADS APSYS         |     |     |                | SG-4 Co-chair    |\n|-----------------------------------|-----|-----|----------------|------------------|\n| Mark Lillis, Goodrich GPECS       |     |     | SG-4 Co-chair  |                  |\n| Herv Delseny, Airbus             |     |     |                | SG-4 Co-chair    |\n| Martha Blankenberger, Rolls-Royce |     |     | SG-4 Secretary |                  |",
    "## Sg-5 - Object-Oriented Technology",
    "|                                 | Peter Heller, Airbus Operations GmbH     |     | SG-5 Co-chair (until February 2009)     |\n|---------------------------------|------------------------------------------|-----|-----------------------------------------|\n| Jan-Hendrik Boelens, Eurocopter |                                          |     | SG-5 Co-chair (Feb 2009 to August 2010) |",
    "| James Hunt, aicas               |                                          |     |                                         |\n| Jim Chelini, Verocel            |                                          |     |                                         |\n| Greg Millican, Honeywell        |                                          |     |                                         |",
    "| Jim Chelini, Verocel            |                                          |     |                                         |",
    "## Sg-6 - Formal Methods\n\n|                      |    |    | Duncan Brown, Aero Engine Controls (Rolls-Royce)    | SG-6 Co-chair    |\n|----------------------|----|----|-----------------------------------------------------|------------------|\n| Kelly Hayhurst, NASA |    |    |                                                     | SG-6 Co-chair    |\n\n## Sg-7 - Special Considerations And Cns/Atm",
    "|                               | David Hawken, NATS     |     |                                   |                                  | SG-7 Co-chair (until June 2010)    |\n|-------------------------------|------------------------|-----|-----------------------------------|----------------------------------|------------------------------------|",
    "| Jim Stewart, NATS             |                        |     |                                   |                                  | SG-7 Co-chair (from June 2010)     |\n| Don Heck, Boeing Company      |                        |     |                                   | SG-7 Co-chair                    |                                    |",
    "| Leslie Alford, Boeing Company |                        |     | SG-7 Secretary (until March 2008) |                                  |                                    |\n| Marguerite Baier, Honeywell   |                        |     |                                   | SG-7 Secretary (from March 2008) |                                    |",
    "## Rtca Representative:",
    "| Rudy Ruana    |     |     |     |     |                               | RTCA Inc. (until September 2009)    |\n|---------------|-----|-----|-----|-----|-------------------------------|-------------------------------------|\n| Ray Glennon   |     |     |     |     |                               | RTCA Inc. (until March 2010)        |\n| Hal Moses     |     |     |     |     |                               | RTCA Inc. (until August 2010)       |",
    "| Cyndy Brown   |     |     |     |     | RTCA Inc. (until August 2011) |                                     |\n| Hal Moses     |     |     |     |     |                               | RTCA Inc. (from August 2011)        |",
    "## Eurocae Representative:\n\n| Gilbert Amato     |     |     |     |     | EUROCAE (until September 2009)    |\n|-------------------|-----|-----|-----|-----|-----------------------------------|\n| Roland Mallwitz   |     |     |     |     | EUROCAE (from October 2009)       |\n\n## Editorial Committee",
    "|                                        | Leanna Rierson, Digital Safety Consulting     |                     | Editorial Committee Chair    |\n|----------------------------------------|-----------------------------------------------|---------------------|------------------------------|\n| Ron Ashpole, SILVER ATENA              |                                               |                     | Editorial Committee          |",
    "| Alex Ayzenberg, Boeing Company         |                                               |                     | Editorial Committee          |\n| Patty (Bartels) Bath, Esterline AVISTA |                                               | Editorial Committee |                              |\n| Dewi Daniels, Verocel                  |                                               |                     |                              |",
    "| Herv Delseny, Airbus                  |                                               |                     |                              |\n| Andrew Elliott, Design Assurance       |                                               |                     | Editorial Committee          |\n| Kelly Hayhurst, NASA                   |                                               |                     |                              |",
    "| Barbara Lingberg, FAA                  |                                               |                     |                              |\n| Steven C. Martz, Garmin                |                                               |                     |                              |\n| Steve Morton, TBV Associates           |                                               |                     | Editorial Committee          |",
    "| Marge Sonnek, Honeywell                |                                               |                     |                              |\n|                                        |                                               |                     |                              |",
    "## Committee Membership\n\nName \nOrganization",
    "Kyle Achenbach Rolls-Royce Dana E. Adkins Kidde Aerospace Leslie Alford Boeing Company Carlo Amalfitano Certon Software, Inc Gilbert Amato EUROCAE \nPeter Amey Praxis High Integrity Systems Allan Gilmour Anderson Embraer Hkan Anderwall Saab AB \nJoseph Angelo NovAtel Inc, Canada John Charles Angermayer Mitre Corp Robert Annis GE Aviation Ron Ashpole SILVER ATENA",
    "Alex Ayzenberg Boeing Company Marguerite Baier Honeywell Fred Barber Avidyne Clay Barber Garmin International Gerald F. Barofsky L-3 Communications Patty (Bartels) Bath Esterline AVISTA \nBrigitte Bauer  \nThales Phillipe Baufreton SAGEM DS   Safran Group Connie Beane ENEA Embedded Technology Inc Bernard Beaudouin EADS APSYS",
    "Germain Beaulieu Independent Consultant Martin Beeby Seaweed Systems Scott Beecher Pratt & Whitney Haik Biglari Fairchild Controls Peter Billing Aviya Technologies Inc Denise Black Embedded Plus Engineering Brad Blackhurst Independent Consultant Craig Bladow Woodward  \nMartha Blankenberger Rolls-Royce Holger Blasum SYSGO \nThomas Bleichner Rohde & Schwarz  \nDon Bockenfeld CMC Electronics Jan-Hendrik Boelens Eurocopter Eric Bonnafous CommunicationSys Jean-Christophe Bonnet CEAT",
    "Hugues Bonnin Cap Gemini Matteo Bordin AdaCore Feliks Bortkiewicz Boeing Julien Bourdeau DND (Canada) \nPaul Bousquet Volpe National Transportation Systems Center David Bowen EUROCAE \nElizabeth Brandli FAA \nAndrew Bridge EASA \nPaul Brook Thales Daryl Brooke Universal Avionics Systems Corporation Cyndy Brown RTCA, Inc.",
    "Duncan Brown Aero Engine Controls (Rolls-Royce) \n\nName \nOrganization",
    "Thomas Buchberger Siemens AG \nBrett Burgeles Consultant Bernard Buscail Airbus Bob Busser Systems and Software Consortium Christopher Caines QinetiQ \nCristiano Campos Almeida De Freitas Embraer Jean-Louis Camus Esterel Technologies Richard Canis EASA \nYann Carlier DGAC \nLuc Casagrande EADS Apsys Mark Chapman Hamilton Sundstrand Scott Chapman FAA \nJim Chelini Verocel Daniel Chevallier",
    "Jim Chelini Verocel Daniel Chevallier  \nThales John Chilenski Boeing Company Subbiah Chockalingam HCL Technologies Chris Clark Sysgo Darren Cofer Rockwell Collins Keith Coffman Goodrich John Coleman Dawson Consulting Cyrille Comar AdaCore Ray Conrad Lockheed Martin Mirko Conrad The MathWorks, Inc.",
    "Nathalie Corbovianu DGAC \nAna Costanti Embraer Dewi Daniels Verocel Eric Danielson Rockwell Collins Henri De La Valle Poussin SABCA \nMichael Deitz Gentex Corporation Jean-Luc Delamaide EASA \nHerv Delseny Airbus Patrick Desbiens Transport Canada Mike DeWalt Certification Services, Inc./FAA \nMansur Dewshi Ultra Electronics Controls Bernard Dion Esterel Technologies Antonio Jose Vitorio Domiciano Embraer Kurt Doppelbauer TTTech Cheryl Dorsey Digital Flight Rick Dorsey",
    "Digital Flight John Doughty Garmin International Vincent Dovydaitis III \nFoliage Software Systems, Inc.",
    "Georges Duchein DGA \nBranimir Dulic Transport Canada Gilles Dulon SAGEM DS   Safran Group Paul Dunn Northrop Grumman Corporation Andrew Eaton UK CAA \nBrian Eckmann Universal Avionics Systems Corporation Vladimir Eliseev Sukhoi Civil Aircraft Company (SCAC) \nAndrew Elliott Design Assurance Mike Elliott Boeing Company Joao Esteves Critical Software \n\n## Name Organization",
    "Rowland Evans Pratt & Whitney Canada Louis Fabre Eurocopter Martin Fassl Siemens AG \nMichael Fee Aero Engine Controls (Rolls-Royce) \n \nTom Ferrell Ferrell and Associates Consulting Uma Ferrell Ferrell and Associates Consulting Lou Fisk GE Aviation Ade Fountain Penny and Giles Claude Fournier Liebherr Pierre Francine Thales Timothy Frey Honeywell  \nStephen J. Fridrick GE Aviation Leonard Fulcher TTTech",
    "Stephen J. Fridrick GE Aviation Leonard Fulcher TTTech  \nRandall Fulton Seaweed Systems Francoise Gachet Dassault-Aviation Victor Galushkin GosNIIAS \nMarty Gasiorowski Worldwide Certification Services Stephanie Gaudan Thales Jean-Louis Gebel Airbus Dries Geldof BARCO \nDimitri Gianesini Airbus Jim Gibbons Boeing Company Dara Gibson FAA \nGreg Gicca AdaCore Steven Gitelis Lumina Engineering Ian Glazebrook WS Atkins Santiago Golmayo GMV SA",
    "Ben Gorry British Aerospace Systems Florian Gouleau DGA Techniques Aronautiques Olivier Graff Intertechnique - Zodiac  \nRussell DeLoy Graham Garmin International Robert Green BAE Systems Mark Grindle Systems Enginuity Peter Grossinger Pilatus Aircraft Mark Gulick Solers, Inc.",
    "Pierre Guyot Dassault Aviation Ibrahim Habli University of York Ross Hannan Sigma Associates (Aerospace) Limited Christopher H. Hansen Rockwell Collins Wue Hao Wen Civil Aviation Administration of China (CAAC) \nKeith Harrison HVR Consulting Services Ltd Bjorn Hasselqvist Saab AB \nKevin Hathaway Aero Engine Controls (Goodrich)  \nDavid Hawken NATS \nKelly Hayhurst NASA \nPeter Heath  \nSecuraplane Technologies Myron Hecht Aerospace Corporation Don Heck Boeing Company",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nPeter Heller Airbus Operations GmbH \nBarry Hendrix Lockheed Martin Michael Hennell LDRA \n\nName \nOrganization \n\nMichael Herring Rockwell Collins Ruth Hirt FAA \nKent Hollinger Mitre Corp C. Michael Holloway NASA \nIan Hopkins Aero Engine Controls (Rolls-Royce) \n \nGary Horan FAA \nChris Hote PolySpace Inc.",
    "Susan Houston FAA \nJames Hummell Embedded Plus Dr. James J. Hunt aicas Rebecca L. Hunt Boeing Company Stuart Hutchesson Aero Engine Controls (Rolls-Royce) \n \nRex Hyde Moog Inc. Aircraft Group Mario Iacobelli Mannarino Systems Melissa Isaacs FAA \nVladimir Istomin Sukhoi Civil Aircraft Company (SCAC) \nStephen A. Jacklin NASA  \nMatt Jaffe Embry-Riddle Aeronautical University Marek Jaglarz Pilatus Aircraft Myles Jalalian FAA  \nMerlin James Garmin International Tomas Jansson Saab AB",
    "Merlin James Garmin International Tomas Jansson Saab AB \nEric Jenn Thales Lars Johannknecht EADS  \nRikard Johansson Saab AB \nJohn Jorgensen Universal Avionics Systems Jeffrey Joyce Critical Systems Labs Chris Karis Ensco Gene Kelly CertTech Anne-Ccile Kerbrat Aeroconseil Randy Key FAA \nCharles W. Kilgore II \nFAA \nWayne King Honeywell Daniel Kinney Boeing Company Judith Klein Lockheed Martin Joachim Klichert Diehl Avionik Systeme Jeff Knickerbocker Sunrise Certification & Consulting, Inc.",
    "John Knight  \nUniversity of Virginia Rainer Kollner Verocel Andrew Kornecki Embry-Riddle Aeronautical University Igor Koverninskiy Gos NIIAS \nJim Krodel Pratt & Whitney Paramesh Kunda Pratt & Whitney Canada Sylvie Lacabanne AIRBUS \nGrard Ladier Airbus/Aerospace Valley Ron Lambalot Boeing Company Boris Langer Diehl Aerospace Susanne Lanzerstorfer APAC GesmbH \nGilles Laplane SAGEM DS   Safran Group Jeanne Larsen Hamilton Sundstrand Emmanuel Ledinot Dassault Aviation",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nName Organization Stephane Leriche Thales Hong Leung Bell Helicopter Textron John Lewis FAA \nJohn Li Thales Mark Lillis Goodrich GPECS \n \nBarbara Lingberg FAA \nPierre Lionne EADS APSYS \nHoyt Lougee Foliage Software Systems Howard Lowe GE Aviation Hauke Luethje NewTec GmbH \nJonathan Lynch Honeywell Franoise Magliozzi Atos Origin Veronique Magnier EASA \nKristine Maine Aerospace Corporation Didier Malescot DSNA/DTI",
    "Kristine Maine Aerospace Corporation Didier Malescot DSNA/DTI \nVarun Malik Hamilton Sundstrand Patrick Mana EUROCONTROL \nJoseph Mangan Coanda Aerospace Software Ghilaine Martinez DGA Techniques Aronautiques Steven C. Martz Garmin International Peter Matthews Independent Consultant Frank McCormick Certification Services Inc Scott McCoy Harris Corporation Thomas McHugh FAA \nWilliam McMinn Lockheed Martin Josh McNeil US Army AMCOM SED",
    "William McMinn Lockheed Martin Josh McNeil US Army AMCOM SED \nKevin Meier Cessna Aircraft Company Amanda Melles Bombardier Marc Meltzer Belcan Engineering Steven Miller Rockwell Collins Gregory Millican Honeywell  \nJohn Minihan Resource Group Martin Momberg Cassidian Air Systems Pippa Moore UK CAA \nEmilio Mora-Castro EASA \nEndrich Moritz Technical University Robert Morris CDL Systems Ltd.",
    "Allan Terry Morris NASA \nSteve Morton TBV Associates  \nHarold Moses RTCA, Inc. \n\nNadir Mostefat Mannarino Systems Fred B. Moyer Rockwell Collins Robert D. Mumme Embedded Plus Engineering Arun Murthi AERO&SPACE USA \nArmen Nahapetian Teledyne Controls Gerry Ngu EASA \nElisabeth Nguyen Aerospace Corporation Robert Noel Mitre Corp Sven Nordhoff SQS AG \nPaula Obeid Embedded Plus Engineering Eric Oberle Becker Avionics \n\nName \nOrganization",
    "Brenda Ocker FAA \nTorsten Ostermeier Bundeswehr Frederic Painchaud Defence Research and Development Canada Sean Parkinson Resource Group Dennis Patrick Penza AVISTA \nJean-Phillipe Perrot Turbomeca Robin Perry GE Aviation David Petesch Hamilton Sundstrand John Philbin Northrop Grumman Integrated Systems Christophe Piala Thales Avionics Cyril Picard EADS APSYS \nFrancine Pierre Thales Avionics Patrick Pierre Thales Avionics Gerald Pilj FAA \nBenoit Pinta Intertechnique - Zodiac",
    "Benoit Pinta Intertechnique - Zodiac  \nMo Piper Boeing Company Andreas Pistek ITK Engineering AG",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\nLaurent Plateaux DGA \nLaurent Pomies Independent Consultant Jennifer Popovich Jeppesen Inc.",
    "Clifford Porter Aircell LLC \nFrdric Pothon ACG Solutions Bill Potter The MathWorks Inc Sunil Prasad HCL Technologies, Chennai, India Paul J. Prisaznuk ARINC-AEEC \nNaim Rahmani  \nTransport Canada Angela Rapaccini ENAC \nLucas Redding Silver-Atena David Redman Aerospace Vehicle Systems Institute (AVSI) \nTammy Reeve Patmos Engineering Services, Inc.  \nGuy Renault SAGEM DS   Safran Group Leanna Rierson Digital Safety Consulting  \nGeorge Romanski Verocel Cyrille Rosay EASA \nEdward Rosenbloom",
    "George Romanski Verocel Cyrille Rosay EASA \nEdward Rosenbloom  \nKollsman, Inc  \nTom Roth Airborne Software Certification Consulting Jamel Rouahi CEAT \nMarielle Roux  \nRockwell Collins France Benedito Massayuki Sakugawa ANAC Brazil Almudena Sanchez GMV SA \nVdot Santhanam Boeing Company Laurence Scales Thales Deidre Schilling Hamilton Sundstrand Ernst Schmidt Bundeswehr  \nPeter Schmitt Universitt Karlsruhe Dr. Achim Schoenhoff EADS Military Aircraft Martin Schwarz TT Technologies",
    "Gabriel Scolan SAGEM DS   Safran Group Christel Seguin ONERA \nBeatrice Sereno Teuchos SAFRAN \nPhillip L. Shaffer GE Aviation",
    "## Name Organization",
    "Jagdish Shah Parker Vadim Shapiro TetraTech/AMT \nJean Franois Sicard DGA Techniques Aronautiques Marten Sjoestedt Saab AB \nPeter Skaves FAA \nGreg Slater Rockwell Collins Claudine Sokoloff Atos Origin Marge Sonnek Honeywell  \nGuillaume Soudain EASA \nRoger Souter FAA \nRobin L. Sova FAA \nRichard Spencer FAA \nThomas Sperling The Mathworks William StClair LDRA \nRoland Stalford Galileo Industries Spa  \nJerry Stamatopoulous Aircell LLC \nTom Starnes Cessna Aircraft Company Jim Stewart NATS",
    "Jerry Stamatopoulous Aircell LLC \nTom Starnes Cessna Aircraft Company Jim Stewart NATS \nTim Stockton Certon Victor Strachan Northrop-Grumman John Strasburger FAA \nMargarita Strelnikova Sukhoi Civil Aircraft Company (SCAC) \nRonald Stroup FAA \nWill Struck FAA \nWladimir Terzic SAGEM DS   Safran Group Wolfgang Theurer C-S SI \nJoel Thornton Tier5 Inc Mikael Thorvaldsson KnowIT Technowledge  \nBozena Brygida Thrower Hamilton Sundstrand Christophe Travers Dassault Aviation Fay Trowbridge Honeywell",
    "Nick Tudor Tudor Associates Silpa Uppalapati FAA \nMarie-Line Valentin Airbus Jozef Van Baal Civil Aviation Authorities Netherlands John Van Leeuwen Sikorsky Aircraft Aulis Viik NAV Canada Bertrand Voisin Dassault Aviation Katherine Volk L-3 Communications  \nDennis Wallace FAA \nAndy Wallington Bell Helicopter Yunming Wang Esterel Technologies Don Ward AVSI \nSteve Ward Rockwell Collins Patricia Warner Software Engineering Michael Warren Rockwell Collins Rob Weaver NATS",
    "Yu Wei CAA China Terri Weinstein Parker Hannifin Marcus Weiskirchner EADS Military Aircraft Daniel Weisz Sandel Avionics, Inc.",
    "--``,,,,,,,,,``````,`,,``,,,,`,,-`-`,,`,,`,`,,`---\n\n| Name           | Organization    |\n|----------------|-----------------|\n| Rich Wendlandt | Quantum3D       |\n\nMichael Whalen Rockwell Collins Paul Whiston High Integrity Solutions Ltd Todd R. White L-3 Communications/Qualtech Virginie Wiels ONERA \nElRoy Wiens Cessna Aircraft Company Terrance Williamson Jeppesen Inc.",
    "Graham Wisdom BAE Systems Patricia Wojnarowski Boeing Commercial Airplanes Joerg Wolfrum Diehl Aerospace Kurt Woodham NASA \nCai Yong CAAC (Civil Aviation Administration of China) \nEdward Yoon Curtiss-Wright Controls, Inc Robert Young Rolls-Royce William Yu CAAC China Erhan Yuceer Savunma Teknolojileri Muhendislik ve Ticaret Uli Zanker Liebherr"
  ]
}